=== NIPS-2017-attention-is-all-you-need-Paper.pdf ===
Attention Is All You Need
Ashish Vaswani‚àó
Google Brain
avaswani@google.com
Noam Shazeer‚àó
Google Brain
noam@google.com
Niki Parmar‚àó
Google Research
nikip@google.com
Jakob Uszkoreit‚àó
Google Research
usz@google.com
Llion Jones‚àó
Google Research
llion@google.com
Aidan N. Gomez‚àó‚Ä†
University of Toronto
aidan@cs.toronto.edu
≈Åukasz Kaiser‚àó
Google Brain
lukaszkaiser@google.com
Illia Polosukhin‚àó‚Ä°
illia.polosukhin@gmail.com
Abstract
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.
1 Introduction
Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks
in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [31, 21, 13].
‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efÔ¨Åcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.
‚Ä†Work performed while at Google Brain.
‚Ä°Work performed while at Google Research.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
statesht, as a function of the previous hidden stateht‚àí1 and the input for positiont. This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [18] and conditional
computation [26], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.
Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
tion models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms
are used in conjunction with a recurrent network.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
2 Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difÔ¨Åcult to learn dependencies between distant positions [ 11]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [28].
To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [14, 15] and [8].
3 Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].
Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence
of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output
sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive
[9], consuming the previously generated symbols as additional input when generating the next.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1 Encoder and Decoder Stacks
Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
sub-layers. The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position-
2
Figure 1: The Transformer - model architecture.
wise fully connected feed-forward network. We employ a residual connection [10] around each of
the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is
LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimensiondmodel = 512.
Decoder: The decoder is also composed of a stack ofN = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for positioni can depend only on the known outputs at positions less thani.
3.2 Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
3.2.1 Scaled Dot-Product Attention
We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of
queries and keys of dimensiondk, and values of dimensiondv. We compute the dot products of the
3
Scaled Dot-Product Attention
 Multi-Head Attention
Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.
query with all keys, divide each by‚àödk, and apply a softmax function to obtain the weights on the
values.
In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrixQ. The keys and values are also packed together into matricesK andV . We compute
the matrix of outputs as:
Attention(Q,K,V ) = softmax(QK T
‚àödk
)V (1)
The two most commonly used attention functions are additive attention [2], and dot-product (multi-
plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of 1‚àödk
. Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized
matrix multiplication code.
While for small values ofdk the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values ofdk [3]. We suspect that for large values of
dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients 4. To counteract this effect, we scale the dot products by 1‚àödk
.
3.2.2 Multi-Head Attention
Instead of performing a single attention function withdmodel-dimensional keys, values and queries,
we found it beneÔ¨Åcial to linearly project the queries, keys and valuesh times with different, learned
linear projections todk,dk anddv dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yieldingdv-dimensional
output values. These are concatenated and once again projected, resulting in the Ô¨Ånal values, as
depicted in Figure 2.
Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.
4To illustrate why the dot products get large, assume that the components of q and k are independent random
variables with mean 0 and variance 1. Then their dot product, q ¬∑ k = ‚àëdk
i=1 qiki, has mean 0 and variance dk.
4
MultiHead(Q,K,V ) = Concat(head 1,..., headh)W O
where headi = Attention(QW Q
i ,KW K
i ,VW V
i )
Where the projections are parameter matricesW Q
i ‚àà Rdmodel√ódk,W K
i ‚àà Rdmodel√ódk,W V
i ‚àà Rdmodel√ódv
andW O‚àà Rhdv√ódmodel.
In this work we employ h = 8 parallel attention layers, or heads. For each of these we use
dk =dv =dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.
3.2.3 Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:
‚Ä¢ In "encoder-decoder attention" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[31, 2, 8].
‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.
‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information Ô¨Çow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to‚àí‚àû) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.
3.3 Position-wise Feed-Forward Networks
In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.
FFN(x) = max(0,xW 1 +b1)W2 +b2 (2)
While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality
df f = 2048.
3.4 Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimensiondmodel. We also use the usual learned linear transfor-
mation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [24]. In the embedding layers, we multiply those weights by‚àödmodel.
3.5 Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the
5
Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types.n is the sequence length,d is the representation dimension,k is the kernel
size of convolutions andr the size of the neighborhood in restricted self-attention.
Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention O(n2¬∑d) O(1) O(1)
Recurrent O(n¬∑d2) O(n) O(n)
Convolutional O(k¬∑n¬∑d2) O(1) O(logk(n))
Self-Attention (restricted) O(r¬∑n¬∑d) O(1) O(n/r)
bottoms of the encoder and decoder stacks. The positional encodings have the same dimensiondmodel
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and Ô¨Åxed [8].
In this work, we use sine and cosine functions of different frequencies:
PE (pos,2i) =sin(pos/100002i/dmodel)
PE (pos,2i+1) =cos(pos/100002i/dmodel)
wherepos is the position andi is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000¬∑ 2œÄ. We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any Ô¨Åxed offsetk,PE pos+k can be represented as a linear function of
PE pos.
We also experimented with using learned positional embeddings [8] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.
4 Why Self-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tional layers commonly used for mapping one variable-length sequence of symbol representations
(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,z i‚àà Rd, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.
One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.
The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.
As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence
length n is smaller than the representation dimensionality d, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece
[31] and byte-pair [25] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of sizer in
6
the input sequence centered around the respective output position. This would increase the maximum
path length toO(n/r). We plan to investigate this approach further in future work.
A single convolutional layer with kernel widthk<n does not connect all pairs of input and output
positions. Doing so requires a stack ofO(n/k) convolutional layers in the case of contiguous kernels,
orO(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity
considerably, toO(k¬∑n¬∑d +n¬∑d2). Even with k = n, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.
As side beneÔ¨Åt, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.
5 Training
This section describes the training regime for our models.
5.1 Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-
target vocabulary of about 37000 tokens. For English-French, we used the signiÔ¨Åcantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.
5.2 Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).
5.3 Optimizer
We used the Adam optimizer [17] withŒ≤1 = 0.9,Œ≤2 = 0.98 andœµ = 10‚àí9. We varied the learning
rate over the course of training, according to the formula:
lrate =d‚àí0.5
model¬∑ min(step_num‚àí0.5,step _num¬∑warmup_steps‚àí1.5) (3)
This corresponds to increasing the learning rate linearly for the Ô¨Årstwarmup_steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
warmup_steps = 4000.
5.4 Regularization
We employ three types of regularization during training:
Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
Pdrop = 0.1.
7
Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
Model
BLEU Training Cost (FLOPs)
EN-DE EN-FR EN-DE EN-FR
ByteNet [15] 23.75
Deep-Att + PosUnk [32] 39.2 1.0¬∑ 1020
GNMT + RL [31] 24.6 39.92 2.3¬∑ 1019 1.4¬∑ 1020
ConvS2S [8] 25.16 40.46 9.6¬∑ 1018 1.5¬∑ 1020
MoE [26] 26.03 40.56 2.0¬∑ 1019 1.2¬∑ 1020
Deep-Att + PosUnk Ensemble [32] 40.4 8.0¬∑ 1020
GNMT + RL Ensemble [31] 26.30 41.16 1.8¬∑ 1020 1.1¬∑ 1021
ConvS2S Ensemble [8] 26.36 41.29 7.7¬∑ 1019 1.2¬∑ 1021
Transformer (base model) 27.3 38.1 3.3 ¬∑ 1018
Transformer (big) 28.4 41.0 2.3¬∑ 1019
Label Smoothing During training, we employed label smoothing of value œµls = 0.1 [30]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
6 Results
6.1 Machine Translation
On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conÔ¨Åguration of this model is
listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.
On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,
outperforming all of the previously published single models, at less than 1/4 the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout ratePdrop = 0.1, instead of 0.3.
For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4 and length penaltyŒ± = 0.6 [31]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [31].
Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of Ô¨Çoating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision Ô¨Çoating-point capacity of each GPU 5.
6.2 Model Variations
To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the
development set, newstest2013. We used beam search as described in the previous section, but no
checkpoint averaging. We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.
5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.
8
Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.
N d model dff h d k dv Pdrop œµls
train PPL BLEU params
steps (dev) (dev) √ó106
base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
(A)
1 512 512 5.29 24.9
4 128 128 5.00 25.5
16 32 32 4.91 25.8
32 16 16 5.01 25.4
(B) 16 5.16 25.1 58
32 5.01 25.4 60
(C)
2 6.11 23.7 36
4 5.19 25.3 50
8 4.88 25.5 80
256 32 32 5.75 24.5 28
1024 128 128 4.66 26.0 168
1024 5.12 25.4 53
4096 4.75 26.2 90
(D)
0.0 5.77 24.6
0.2 4.95 25.5
0.0 4.67 25.3
0.2 5.47 25.7
(E) positional embedding instead of sinusoids 4.92 25.7
big 6 1024 4096 16 0.3 300K 4.33 26.4 213
In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneÔ¨Åcial. We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting. In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical
results to the base model.
7 Conclusion
In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.
For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art. In the former task our best
model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor.
Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.
9
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014.
[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural
machine translation architectures. CoRR, abs/1703.03906, 2017.
[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733, 2016.
[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. CoRR, abs/1406.1078, 2014.
[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv
preprint arXiv:1610.02357, 2016.
[7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.
[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-
tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.
[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770‚Äì778, 2016.
[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient Ô¨Çow in
recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.
[12] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735‚Äì1780, 1997.
[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
[14] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference
on Learning Representations (ICLR), 2016.
[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-
ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,
2017.
[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.
In International Conference on Learning Representations , 2017.
[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint
arXiv:1703.10722, 2017.
[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130, 2017.
[20] Samy Bengio ≈Åukasz Kaiser. Can active memory replace attention? In Advances in Neural
Information Processing Systems, (NIPS), 2016.
10
[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model. In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. arXiv preprint arXiv:1705.04304, 2017.
[24] OÔ¨År Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859, 2016.
[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. arXiv preprint arXiv:1508.07909, 2015.
[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538, 2017.
[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: a simple way to prevent neural networks from overÔ¨Åtting. Journal of Machine
Learning Research, 15(1):1929‚Äì1958, 2014.
[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory
networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448. Curran Associates,
Inc., 2015.
[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.
[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine
translation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with
fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.
11


=== Chinchilla-paper.pdf ===
Training Compute-Optimal Large Language Models
Jordan HoÔ¨Ämann‚òÖ, Sebastian Borgeaud‚òÖ, Arthur Mensch‚òÖ, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,
Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre‚òÖ
‚òÖEqual contributions
We investigate the optimal model size and number of tokens for training a transformer language model
under a given compute budget. We Ô¨Ånd that current large language models are signiÔ¨Åcantly under-
trained, a consequence of the recent focus on scaling language models whilst keeping the amount of
trainingdataconstant. Bytrainingover400languagemodelsrangingfrom70milliontoover16billion
parameters on 5 to 500 billion tokens, we Ô¨Ånd that for compute-optimal training, the model size and
the number of training tokens should be scaled equally: for every doubling of model size the number
of training tokens should also be doubled. We test this hypothesis by training a predicted compute-
optimal model,Chinchilla, that uses the same compute budget asGopher but with 70B parameters and
4 more more data.Chinchilla uniformly and signiÔ¨Åcantly outperformsGopher (280B), GPT-3 (175B),
Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.
This also means thatChinchilla uses substantially less compute for Ô¨Åne-tuning and inference, greatly
facilitating downstream usage. As a highlight,Chinchilla reaches a state-of-the-art average accuracy of
67.5% on the MMLU benchmark, greater than a 7% improvement overGopher.
1. Introduction
Recently a series ofLarge Language Models(LLMs) have been introduced (Brown et al., 2020; Lieber
et al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense
language models now having over 500 billion parameters. These large autoregressive transformers
(Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of
evaluation protocols such as zero-shot, few-shot, and Ô¨Åne-tuning.
The compute and energy cost for training large language models is substantial (Rae et al., 2021;
Thoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training
compute budget is often known in advance: how many accelerators are available and for how long
we want to use them. Since it is typically only feasible to train these large models once, accurately
estimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021).
Kaplan et al. (2020) showed that there is a power law relationship between the number of
parameters in an autoregressive language model (LM) and its performance. As a result, the Ô¨Åeld has
beentraininglargerandlargermodels,expectingperformanceimprovements. Onenotableconclusion
in Kaplan et al. (2020) is that large models should not be trained to their lowest possible loss to be
compute optimal. Whilst we reach the same conclusion, we estimate that large models should be
trained for many more training tokens than recommended by the authors. SpeciÔ¨Åcally, given a10
increase computational budget, they suggests that the size of the model should increase5¬ì5 while
the number of training tokens should only increase 1.8. Instead, we Ô¨Ånd that model size and the
number of training tokens should be scaled in equal proportions.
Following Kaplan et al. (2020) and the training setup of GPT-3 (Brown et al., 2020), many of the
recently trained large models have been trained for approximately 300 billion tokens (Table 1), in
line with the approach of predominantly increasing model size when increasing compute.
Corresponding authors: {jordanhoÔ¨Ämann|sborgeaud|amensch|sifre}@deepmind.com
¬© 2023 DeepMind. All rights reserved
arXiv:2203.15556v1  [cs.CL]  29 Mar 2022
1017 1019 1021 1023 1025
FLOPs
10M
100M
1.0B
10B
100B
1T
Parameters
Approach 1
Approach 2
Approach 3
Kaplan et al (2020)
 
Chinchilla (70B)
Gopher (280B)
GPT-3 (175B)
Megatron-Turing NLG (530B)
Figure 1j Overlaid predictions.We overlay the predictions from our three diÔ¨Äerent approaches,
along with projections from Kaplan et al. (2020). We Ô¨Ånd that all three methods predict that current
large models should be substantially smaller and therefore trained much longer than is currently
done. In Figure A3, we show the results with the predicted optimal tokens plotted against the optimal
number of parameters for Ô¨Åxed FLOP budgets.Chinchillaoutperforms Gopherand the other large
models (see Section 4.2).
In this work, we revisit the question:Given a Ô¨Åxed FLOPs budget,1 how should one trade-oÔ¨Ä model
size and the number of training tokens?To answer this question, we model the Ô¨Ånal pre-training loss2
ùêø¬πùëÅ¬î ùê∑¬∫ as a function of the number of model parametersùëÅ, and the number of training tokens,ùê∑.
Since the computational budgetùê∂ is a deterministic functionFLOPs¬πùëÅ¬î ùê∑¬∫ of the number of seen
training tokens and model parameters, we are interested in minimizingùêø under the constraint
FLOPs¬πùëÅ¬î ùê∑¬∫ = ùê∂:
ùëÅùëúùëùùë°¬πùê∂¬∫¬î ùê∑ùëúùëùùë°¬πùê∂¬∫ = argmin
ùëÅ¬îùê∑ s.t. FLOPs¬πùëÅ¬îùê∑¬∫=ùê∂
ùêø¬πùëÅ¬î ùê∑¬∫¬ì (1)
The functionsùëÅùëúùëùùë°¬πùê∂¬∫, andùê∑ùëúùëùùë°¬πùê∂¬∫ describe the optimal allocation of a computational budgetùê∂. We
empirically estimate these functions based on the losses of over 400 models, ranging from under70M
to over16B parameters, and trained on5B to over400B tokens ‚Äì with each model conÔ¨Åguration
trained for several diÔ¨Äerent training horizons. Our approach leads to considerably diÔ¨Äerent results
than that of Kaplan et al. (2020). We highlight our results in Figure 1 and how our approaches diÔ¨Äer
in Section 2.
Based on our estimated compute-optimal frontier, we predict that for the compute budget used
to trainGopher, an optimal model should be 4 times smaller, while being training on 4 times more
tokens. We verify this by training a morecompute-optimal70B model, calledChinchilla, on 1.4 trillion
tokens. Not only doesChinchilla outperform its much larger counterpart,Gopher, but its reduced
model size reduces inference cost considerably and greatly facilitates downstream uses on smaller
hardware. The energy cost of a large language model is amortized through its usage for inference an
Ô¨Åne-tuning. The beneÔ¨Åts of a more optimally trained smaller model, therefore, extend beyond the
immediate beneÔ¨Åts of its improved performance.
1For example, knowing the number of accelerators and a target training duration.
2For simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the test loss, as
we are in the inÔ¨Ånite data regime (the number of training tokens is less than the number of tokens in the entire corpus).
2
Table 1j Current LLMs. We show Ô¨Åve of the current largest dense transformer models, their size,
and the number of training tokens. Other than LaMDA (Thoppilan et al., 2022), most models are
trained for approximately 300 billion tokens. We introduceChinchilla, a substantially smaller model,
trained for much longer than 300B tokens.
Model Size ( # Parameters) Training Tokens
LaMDA (Thoppilan et al., 2022) 137 Billion 168 Billion
GPT-3 (Brown et al., 2020) 175 Billion 300 Billion
Jurassic (Lieber et al., 2021) 178 Billion 300 Billion
Gopher (Rae et al., 2021) 280 Billion 300 Billion
MT-NLG 530B (Smith et al., 2022) 530 Billion 270 Billion
Chinchilla 70 Billion 1.4 Trillion
2. Related Work
Large language models. A variety of large language models have been introduced in the last few
years. These include both dense transformer models (Brown et al., 2020; Lieber et al., 2021; Rae
et al., 2021; Smith et al., 2022; Thoppilan et al., 2022) and mixture-of-expert (MoE) models (Du
et al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500
billion parameters (Smith et al., 2022). The drive to train larger and larger models is clear‚Äîso far
increasing the size of language models has been responsible for improving the state-of-the-art in many
language modelling tasks. Nonetheless, large language models face several challenges, including
their overwhelming computational requirements (the cost of training and inference increase with
model size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality
training data. In fact, in this work we Ô¨Ånd that larger, high quality datasets will play a key role in any
further scaling of language models.
Modelling the scaling behavior. Understanding the scaling behaviour of language models and
their transfer properties has been important in the development of recent large models (Hernandez
et al., 2021; Kaplan et al., 2020). Kaplan et al. (2020) Ô¨Årst showed a predictable relationship between
model size and loss over many orders of magnitude. The authors investigate the question of choosing
the optimal model size to train for a given compute budget. Similar to us, they address this question
by training various models. Our work diÔ¨Äers from Kaplan et al. (2020) in several important ways.
First, the authors use a Ô¨Åxed number of training tokens and learning rate schedule for all models; this
prevents them from modelling the impact of these hyperparameters on the loss. In contrast, we Ô¨Ånd
that setting the learning rate schedule to approximately match the number of training tokens results
in the best Ô¨Ånal loss regardless of model size‚Äîsee Figure A1. For a Ô¨Åxed learning rate cosine schedule
to 130B tokens, the intermediate loss estimates (forùê∑0 ¬ù¬ù 130B) are therefore overestimates of the
loss of a model trained with a schedule length matchingùê∑0. Using these intermediate losses results in
underestimating the eÔ¨Äectiveness of training models on less data than 130B tokens, and eventually
contributes to the conclusion that model size should increase faster than training data size as compute
budget increases. In contrast, our analysis predicts that both quantities should scale at roughly the
same rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight
curvature in the FLOP-loss frontier (see Appendix E)‚Äîin fact, the majority of the models used in
our analysis have more than 500 million parameters, in contrast the majority of runs in Kaplan et al.
(2020) are signiÔ¨Åcantly smaller‚Äîmany being less than 100M parameters.
Recently, Clark et al. (2022) speciÔ¨Åcally looked in to the scaling properties of Mixture of Expert
3
language models, showing that the scaling with number of experts diminishes as the model size
increases‚Äîtheir approach models the loss as a function of two variables: the model size and the
number of experts. However, the analysis is done with a Ô¨Åxed number of training tokens, as in Kaplan
et al. (2020), potentially underestimating the improvements of branching.
Estimatinghyperparametersforlargemodels. Themodelsizeandthenumberoftrainingtokens
are not the only two parameters to chose when selecting a language model and a procedure to train
it. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and
width-to-depth ratio. In this work, we focus on model size and the number of training steps, and
we rely on existing work and provided experimental heuristics to determine the other necessary
hyperparameters. Yang et al. (2021) investigates how to choose a variety of these parameters for
training an autoregressive transformer, including the learning rate and batch size. McCandlish et al.
(2018) Ô¨Ånds only a weak dependence between optimal batch size and model size. Shallue et al.
(2018); Zhang et al. (2019) suggest that using larger batch-sizes than those we use is possible. Levine
et al. (2020) investigates the optimal depth-to-width ratio for a variety of standard model sizes. We
use slightly less deep models than proposed as this translates to better wall-clock performance on our
hardware.
Improved model architectures. Recently, various promising alternatives to traditional dense trans-
formers have been proposed. For example, through the use of conditional computation large MoE
models like the 1.7 trillion parameter Switch transformer (Fedus et al., 2021), the 1.2 Trillion pa-
rameter GLaM model (Du et al., 2021), and others (Artetxe et al., 2021; Zoph et al., 2022) are able
to provide a large eÔ¨Äective model size despite using relatively fewer training and inference FLOPs.
However, for very large models the computational beneÔ¨Åts of routed models seems to diminish (Clark
et al., 2022). An orthogonal approach to improving language models is to augment transformers
with explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al.
(2020). This approach eÔ¨Äectively increases the number of data tokens seen during training (by a
factor of 10 in Borgeaud et al. (2021)). This suggests that the performance of language models
may be more dependant on the size of the training data than previously thought.
3. Estimating the optimal parameter/training tokens allocation
We present three diÔ¨Äerent approaches to answer the question driving our research:Given a Ô¨Åxed
FLOPs budget, how should one trade-oÔ¨Ä model size and the number of training tokens?In all three
cases we start by training a range of models varying both model size and the number of training
tokens and use the resulting training curves to Ô¨Åt an empirical estimator of how they should scale.
We assume a power-law relationship between compute and model size as done in Clark et al. (2022);
Kaplan et al. (2020), though future work may want to include potential curvature in this relationship
for large model sizes. The resulting predictions are similar for all three methods and suggest that
parameter count and number of training tokens should be increased equally with more compute3‚Äî
with proportions reported in Table 2. This is in clear contrast to previous work on this topic and
warrants further investigation.
3We compute FLOPs as described in Appendix F.
4
1017 1018 1019 1020 1021 1022
FLOPS
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0Training loss
75M
250M
500M
1B
2.5B
5B
10B
1017 1019 1021 1023 1025
FLOPs
109
1010
1011
1012
Tokens
1.5T
1017 1019 1021 1023 1025
FLOPs
100M
1.0B
10B
100B
1T
Parameters
67B
Figure 2j Training curve envelope.On theleft we show all of our diÔ¨Äerent runs. We launched a
range of model sizes going from 70M to 10B, each for four diÔ¨Äerent cosine cycle lengths. From these
curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the
optimal model size (center) for a given compute budget and the optimal number of training tokens
(right). In green, we show projections of optimal model size and training token count based on the
number of FLOPs used to trainGopher (5¬ì76 1023).
3.1. Approach 1: Fix model sizes and vary number of training tokens
In our Ô¨Årst approach we vary the number of training steps for a Ô¨Åxed family of models (ranging from
70M to over 10B parameters), training each model for 4 diÔ¨Äerent number of training sequences.
From these runs, we are able to directly extract an estimate of the minimum loss achieved for a given
number of training FLOPs. Training details for this approach can be found in Appendix D.
For each parameter countùëÅ we train 4 diÔ¨Äerent models, decaying the learning rate by a factor of
10 over a horizon (measured in number of training tokens) that ranges by a factor of16. Then, for
each run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous
mapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine
which run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP
count ùê∂, to the most eÔ¨Écient choice of model sizeùëÅ and number of training tokensùê∑ such that
FLOPs¬πùëÅ¬î ùê∑¬∫ = ùê∂.4 At1500logarithmicallyspacedFLOPvalues, weÔ¨Åndwhichmodelsizeachievesthe
lowest loss of all models along with the required number of training tokens. Finally, we Ô¨Åt power laws
to estimate the optimal model size and number of training tokens for any given amount of compute
(see the center and right panels of Figure 2), obtaining a relationshipùëÅùëúùëùùë°/ ùê∂ùëé and ùê∑ùëúùëùùë°/ ùê∂ùëè. We
Ô¨Ånd that ùëé = 0¬ì50 and ùëè = 0¬ì50‚Äîas summarized in Table 2. In Section D.4, we show a head-to-head
comparison at1021 FLOPs, using the model size recommended by our analysis and by the analysis of
Kaplan et al. (2020)‚Äîusing the model size we predict has a clear advantage.
3.2. Approach 2: IsoFLOP proÔ¨Åles
In our second approach we vary the model size5 for a Ô¨Åxed set of 9 diÔ¨Äerent training FLOP counts6
(ranging from6 1018 to 3 1021 FLOPs), and consider the Ô¨Ånal training loss for each point7. in
contrast with Approach 1 that considered points¬πùëÅ¬î ùê∑¬î ùêø¬∫ along the entire training runs. This allows
us to directly answer the question: For a given FLOP budget, what is the optimal parameter count?
4Notethatallselectedpointsarewithinthelast15%oftraining. Thissuggeststhatwhentrainingamodelover ùê∑tokens,
we should pick a cosine cycle length that decays10 over approximatelyùê∑ tokens‚Äîsee further details in Appendix B.
5In approach 2, model size varies up to 16B as opposed to approach 1 where we only used models up to 10B.
6The number of training tokens is determined by the model size and training FLOPs.
7We set the cosine schedule length to match the number of tokens, which is optimal according to the analysis presented
in Appendix B.
5
100M 300M 1B 3B 6B 30B
Parameters
2.0
2.2
2.4
2.6
2.8
3.0
3.2Training Loss
6e18
1e19
3e19
6e19
1e20
3e20
6e20
1e21
3e21
1017 1019 1021 1023 1025
FLOPs
100M
1B
10B
100B
1T
Parameters
63B
1017 1019 1021 1023 1025
FLOPs
100M
1B
10B
100B
1T
10T
Tokens
1.4T
Figure 3j IsoFLOP curves.For various model sizes, we choose the number of training tokens such
that the Ô¨Ånal FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We
Ô¨Ånd a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train
(left). Using the location of these valleys, we project optimal model size and number of tokens for
larger models (centerand right). In green, we show the estimated number of parameters and tokens
for anoptimalmodel trained with the compute budget ofGopher.
For each FLOP budget, we plot the Ô¨Ånal loss (after smoothing) against the parameter count in
Figure 3 (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see
a clear minimum in the loss. We Ô¨Åt a parabola to each IsoFLOPs curve to directly estimate at what
model size the minimum loss is achieved (Figure 3 (left)). As with the previous approach, we then Ô¨Åt
a power law between FLOPs and loss-optimal model size and number of training tokens, shown in
Figure 3 (center, right). Again, we Ô¨Åt exponents of the formùëÅùëúùëùùë°/ ùê∂ùëé and ùê∑ùëúùëùùë°/ ùê∂ùëè and we Ô¨Ånd that
ùëé = 0¬ì49 and ùëè = 0¬ì51‚Äîas summarized in Table 2.
3.3. Approach 3: Fitting a parametric loss function
Lastly, we model all Ô¨Ånal losses from experiments in Approach 1 & 2 as a parametric function of
model parameter count and the number of seen tokens. Following a classical risk decomposition (see
Section D.2), we propose the following functional form
ÀÜùêø¬πùëÅ¬î ùê∑¬∫ , ùê∏¬∏ ùê¥
ùëÅ ùõº¬∏ ùêµ
ùê∑ùõΩ ¬ì (2)
The Ô¨Årst term captures the loss for an ideal generative process on the data distribution, and should
correspond to the entropy of natural text. The second term captures the fact that a perfectly trained
transformer withùëÅ parameters underperforms the ideal generative process. The Ô¨Ånal term captures
the fact that the transformer is not trained to convergence, as we only make a Ô¨Ånite number of
optimisation steps, on a sample of the dataset distribution.
Model Ô¨Åtting. To estimate¬π ùê¥¬î ùêµ¬î ùê∏¬î ùõº¬î ùõΩ¬∫, we minimize the Huber loss (Huber, 1964) between the
predicted and observed log loss using the L-BFGS algorithm (Nocedal, 1980):
min
ùê¥¬îùêµ¬îùê∏¬îùõº¬îùõΩ
‚àëÔ∏Å
Runs ùëñ
Huberùõø

log ÀÜùêø¬πùëÅùëñ¬î ùê∑ùëñ¬∫  log ùêøùëñ

(3)
We account for possible local minima by selecting the best Ô¨Åt from a grid of initialisations. The Huber
loss (ùõø = 10 3) is robust to outliers, which we Ô¨Ånd important for good predictive performance over
held-out data points. Section D.2 details the Ô¨Åtting procedure and the loss decomposition.
6
1018 1019 1020 1021 1022 1023 Gopher
budget
Training FLOPs
100M
1B
10B
40B
100BModel size
IsoLoss contours
Efficient frontier
Empirical data
IsoFLOPs slice
2.00
3.00
4.00
5.00Loss
100M 1B 10B 40B
Model size
IsoFLOPs slices
Train. FLOPs
6e+18
1e+19
3e+19
6e+19
1e+20
3e+20
6e+20
1e+21
3e+21
Gopher
Figure 4j Parametric Ô¨Åt.We Ô¨Åt a parametric modelling of the lossÀÜùêø¬πùëÅ¬î ùê∑¬∫ and display contour (left)
and isoFLOP slices (right). For each isoFLOP slice, we include a corresponding dashed line in the left
plot. In the left plot, we show the eÔ¨Écient frontier in blue, which is a line in log-log space. SpeciÔ¨Åcally,
the curve goes through each iso-loss contour at the point with the fewest FLOPs. We project the
optimal model size given theGopher FLOP budget to be 40B parameters.
EÔ¨Écient frontier. We can approximate the functionsùëÅùëúùëùùë° and ùê∑ùëúùëùùë° by minimizing the parametric
loss ÀÜùêø under the constraintFLOPs¬πùëÅ¬î ùê∑¬∫ 6ùëÅ ùê∑ (Kaplan et al., 2020). The resultingùëÅùëúùëùùë° and ùê∑ùëúùëùùë°
balance the two terms in Equation(3) that depend on model size and data. By construction, they
have a power-law form:
ùëÅùëúùëùùë°¬πùê∂¬∫ = ùê∫
 ùê∂
6
 ùëé
¬î ùê∑ ùëúùëùùë°¬πùê∂¬∫ = ùê∫ 1
 ùê∂
6
 ùëè
¬î where ùê∫ =
 ùõºùê¥
ùõΩ ùêµ
 1
ùõº¬∏ùõΩ
¬î ùëé = ùõΩ
ùõº¬∏ ùõΩ ¬î and ùëè = ùõº
ùõº¬∏ ùõΩ ¬ì (4)
WeshowcontoursoftheÔ¨Åttedfunction ÀÜùêøinFigure4(left),andtheclosed-formeÔ¨Écientcomputational
frontier in blue. From this approach, we Ô¨Ånd thatùëé = 0¬ì46 and ùëè = 0¬ì54‚Äîas summarized in Table 2.
3.4. Optimal model scaling
We Ô¨Ånd that the three approaches, despite using diÔ¨Äerent Ô¨Åtting methodologies and diÔ¨Äerent trained
models, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs
(shown in Table 2). All three approaches suggest that as compute budget increases, model size and
the amount of training data should be increased in approximately equal proportions. The Ô¨Årst and
second approaches yield very similar predictions for optimal model sizes, as shown in Figure 1 and
Figure A3. The third approach predicts even smaller models being optimal at larger compute budgets.
We note that the observed points¬πùêø¬î ùëÅ¬î ùê∑¬∫ for low training FLOPs (ùê∂ 6 1ùëí21) have larger residuals
kùêø  ÀÜùêø¬πùëÅ¬î ùê∑¬∫k
2
2 than points with higher computational budgets. The Ô¨Åtted model places increased
weight on the points with more FLOPs‚Äîautomatically considering the low-computational budget
points as outliers due to the Huber loss. As a consequence of the empirically observed negative
curvature in the frontierùê∂! ùëÅùëúùëùùë° (see Appendix E), this results in predicting a lowerùëÅùëúùëùùë° than the
two other approaches.
In Table 3 we show the estimated number of FLOPs and tokens that would ensure that a model of
a given size lies on the compute-optimal frontier. Our Ô¨Åndings suggests that the current generation of
7
Table 2j Estimated parameter and data scaling with increased training compute.The listed
values are the exponents,ùëé and ùëè, on the relationshipùëÅùëúùëùùë°/ ùê∂ùëé and ùê∑ùëúùëùùë°/ ùê∂ùëè. Our analysis suggests
a near equal scaling in parameters and data with increasing compute which is in clear contrast
to previous work on the scaling of large models. The 10th and 90th percentiles are estimated via
bootstrapping data (80% of the dataset is sampled 100 times) and are shown in parenthesis.
Approach CoeÔ¨Ä. ùëé where ùëÅùëúùëùùë°/ ùê∂ùëé CoeÔ¨Ä. ùëè where ùê∑ùëúùëùùë°/ ùê∂ùëè
1. Minimum over training curves 0¬ì50¬π0¬ì488¬î 0¬ì502¬∫ 0¬ì50¬π0¬ì501¬î 0¬ì512¬∫
2. IsoFLOP proÔ¨Åles 0¬ì49¬π0¬ì462¬î 0¬ì534¬∫ 0¬ì51¬π0¬ì483¬î 0¬ì529¬∫
3. Parametric modelling of the loss 0¬ì46¬π0¬ì454¬î 0¬ì455¬∫ 0¬ì54¬π0¬ì542¬î 0¬ì543¬∫
Kaplan et al. (2020) 0.73 0.27
Table 3j Estimated optimal training FLOPs and training tokens for various model sizes.For
various model sizes, we show the projections from Approach 1 of how many FLOPs and training
tokens would be needed to train compute-optimal models. The estimates for Approach 2 & 3 are
similar (shown in Section D.3)
.
Parameters FLOPs FLOPs (in Gopher unit) Tokens
400 Million 1.92e+19 1¬ù29¬î 968 8.0 Billion
1 Billion 1.21e+20 1¬ù4¬î 761 20.2 Billion
10 Billion 1.23e+22 1¬ù46 205.1 Billion
67 Billion 5.76e+23 1 1.5 Trillion
175 Billion 3.85e+24 6¬ì7 3.7 Trillion
280 Billion 9.90e+24 17¬ì2 5.9 Trillion
520 Billion 3.43e+25 59¬ì5 11.0 Trillion
1 Trillion 1.27e+26 221¬ì3 21.2 Trillion
10 Trillion 1.30e+28 22515¬ì9 216.2 Trillion
large language models are considerably over-sized, given their respective compute budgets, as shown
inFigure1. Forexample,weÔ¨Åndthata175billionparametermodelshouldbetrainedwithacompute
budget of4¬ì41 1024 FLOPs and on over 4.2 trillion tokens. A 280 billionGopher-like model is the
optimal model to train given a compute budget of approximately1025 FLOPs and should be trained on
6.8 trillion tokens. Unless one has a compute budget of1026 FLOPs (over 250 the compute used to
train Gopher), a 1 trillion parameter model is unlikely to be the optimal model to train. Furthermore,
the amount of training data that is projected to be needed is far beyond what is currently used to
train large models, and underscores the importance of dataset collection in addition to engineering
improvements that allow for model scale. While there is signiÔ¨Åcant uncertainty extrapolating out
many orders of magnitude, our analysis clearly suggests that given the training compute budget for
many current LLMs, smaller models should have been trained on more tokens to achieve the most
performant model.
In Appendix C, we reproduce the IsoFLOP analysis on two additional datasets: C4 (RaÔ¨Äel et al.,
2020a) and GitHub code (Rae et al., 2021). In both cases we reach the similar conclusion that model
size and number of training tokens should be scaled in equal proportions.
8
4. Chinchilla
BasedonouranalysisinSection3,theoptimalmodelsizeforthe Gophercomputebudgetissomewhere
between 40 and 70 billion parameters. We test this hypothesis by training a model on the larger end
of this range‚Äî70B parameters‚Äîfor 1.4T tokens, due to both dataset and computational eÔ¨Éciency
considerations. In this section we compare this model, which we callChinchilla, toGopher and other
LLMs. BothChinchilla and Gopher have been trained for the same number of FLOPs but diÔ¨Äer in the
size of the model and the number of training tokens.
While pre-training a large language model has a considerable compute cost, downstream Ô¨Åne-
tuning and inference also make up substantial compute usage (Rae et al., 2021). Due to being4
smaller thanGopher, both the memory footprint and inference cost ofChinchilla are also smaller.
4.1. Model and training details
The full set of hyperparameters used to trainChinchilla are given in Table 4.Chinchilla uses the same
model architecture and training setup asGopher with the exception of the diÔ¨Äerences listed below.
‚Ä¢ We trainChinchilla on MassiveText(the same dataset asGopher) but use a slightly diÔ¨Äerent
subset distribution (shown in Table A1) to account for the increased number of training tokens.
‚Ä¢ We use AdamW (Loshchilov and Hutter, 2019) forChinchilla rather than Adam (Kingma and
Ba, 2014) as this improves the language modelling loss and the downstream task performance
after Ô¨Ånetuning.8
‚Ä¢ We trainChinchilla with a slightly modiÔ¨Åed SentencePiece (Kudo and Richardson, 2018)
tokenizer that does not apply NFKC normalisation. The vocabulary is very similar‚Äì 94.15% of
tokens are the same as those used for trainingGopher. We Ô¨Ånd that this particularly helps with
the representation of mathematics and chemistry, for example.
‚Ä¢ Whilst the forward and backward pass are computed inbfloat16, we store afloat32 copy
of the weights in the distributed optimiser state (Rajbhandari et al., 2020). SeeLessons Learned
from Rae et al. (2021) for additional details.
In Appendix G we show the impact of the various optimiser related changes betweenChinchilla
and Gopher. All models in this analysis have been trained on TPUv3/TPUv4 (Jouppi et al., 2017) with
JAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020). We include aChinchilla model card
(Mitchell et al., 2019) in Table A8.
Model Layers Number Heads Key/Value Size d model Max LR Batch Size
Gopher 280B 80 128 128 16,384 4 10 5 3M! 6M
Chinchilla 70B 80 64 128 8,192 1 10 4 1.5M! 3M
Table 4j Chinchilla architecture details. We list the number of layers, the key/value size, the
bottleneck activation size dmodel, the maximum learning rate, and the training batch size (# tokens).
The feed-forward size is always set to4dmodel. Note that we double the batch size midway through
training for bothChinchilla and Gopher.
8Interestingly, a model trained with AdamW only passes the training performance of a model trained with Adam around
80% of the way through the cosine cycle, though the ending performance is notably better‚Äì see Figure A7
9
# Tasks Examples
Language Modelling 20 WikiText-103, The Pile: PG-19, arXiv, FreeLaw,¬ì ¬ì ¬ì
Reading Comprehension 3 RACE-m, RACE-h, LAMBADA
Question Answering 3 Natural Questions, TriviaQA, TruthfulQA
Common Sense 5 HellaSwag, Winogrande, PIQA, SIQA, BoolQ
MMLU 57 High School Chemistry, Astronomy, Clinical Knowledge,¬ì ¬ì ¬ì
BIG-bench 62 Causal Judgement, Epistemic Reasoning, Temporal Sequences,¬ì ¬ì ¬ì
Table 5j All evaluation tasks.We evaluateChinchilla on a collection of language modelling along
with downstream tasks. We evaluate on largely the same tasks as in Rae et al. (2021), to allow for
direct comparison.
4.2. Results
We perform an extensive evaluation ofChinchilla, comparing against various large language models.
We evaluate on a large subset of the tasks presented in Rae et al. (2021), shown in Table 5. As
the focus of this work is on optimal model scaling, we included a large representative subset, and
introduce a few new evaluations to allow for better comparison to other existing large models. The
evaluation details for all tasks are the same as described in Rae et al. (2021).
4.2.1. Language modelling
pubmed_abstracts
nih_exporter
uspto_backgrounds
pubmed_central
pile_cc
bookcorpus2
stackexchange
opensubtitles
openwebtext2
hackernews
dm_mathematics
arxiv
freelaw
books3
philpapers
github
ubuntu_irc
europarl
gutenberg_pg_19
0.00
0.02
0.04
0.06
0.08
0.10
Decrease in bpb 
 compared to Gopher
Figure 5j Pile Evaluation.For the diÔ¨Äerent evaluation sets in The Pile (Gao et al., 2020), we show
the bits-per-byte (bpb) improvement (decrease) ofChinchilla compared toGopher. On all subsets,
Chinchilla outperformsGopher.
Chinchilla signiÔ¨Åcantly outperformsGopher on all evaluation subsets of The Pile (Gao et al.,
2020), as shown in Figure 5. Compared to Jurassic-1 (178B) Lieber et al. (2021),Chinchilla is more
performant on all but two subsets‚Äìdm_mathematics and ubuntu_irc‚Äì see Table A5 for a raw
bits-per-byte comparison. On Wikitext103 (Merity et al., 2017),Chinchilla achieves a perplexity of
7.16 compared to 7.75 forGopher. Some caution is needed when comparingChinchilla with Gopher
on these language modelling benchmarks asChinchilla is trained on 4 more data thanGopher and
thus train/test set leakage may artiÔ¨Åcially enhance the results. We thus place more emphasis on other
10
Random 25.0%
Average human rater 34.5%
GPT-3 5-shot 43.9%
Gopher 5-shot 60.0%
Chinchilla 5-shot 67.6%
Average human expert performance 89.8%
June 2022 Forecast 57.1%
June 2023 Forecast 63.4%
Table 6j Massive Multitask Language Understanding (MMLU).We report the average 5-shot
accuracy over 57 tasks with model and human accuracy comparisons taken from Hendrycks et al.
(2020). We also include the average prediction for state of the art accuracy in June 2022/2023 made
by 73 competitive human forecasters in Steinhardt (2021).
tasks for which leakage is less of a concern, such as MMLU (Hendrycks et al., 2020) and BIG-bench
(BIG-bench collaboration, 2021) along with various closed-book question answering and common
sense analyses.
4.2.2. MMLU
TheMassiveMultitaskLanguageUnderstanding(MMLU)benchmark(Hendrycksetal.,2020)consists
of a range of exam-like questions on academic subjects. In Table 6, we reportChinchilla‚Äôs average
5-shotperformanceonMMLU(thefullbreakdownofresultsisshowninTableA6). Onthisbenchmark,
Chinchilla signiÔ¨Åcantly outperformsGopher despite being much smaller, with an average accuracy of
67.6%(improvingupon Gopherby7.6%). Remarkably,Chinchillaevenoutperformstheexpertforecast
for June 2023 of 63.4% accuracy (see Table 6) (Steinhardt, 2021). Furthermore,Chinchilla achieves
greater than 90% accuracy on 4 diÔ¨Äerent individual tasks‚Äìhigh_school_gov_and_politics,
international_law, sociology ,and us_foreign_policy. Toourknowledge,noothermodel
has achieved greater than 90% accuracy on a subset.
In Figure 6, we show a comparison toGopher broken down by task. Overall, we Ô¨Ånd thatChin-
chilla improves performance on the vast majority of tasks. On four tasks (college_mathematics,
econometrics, moral_scenarios , andformal_logic) Chinchilla underperforms Gopher, and
there is no change in performance on two tasks.
4.2.3. Reading comprehension
On the Ô¨Ånal word prediction dataset LAMBADA (Paperno et al., 2016),Chinchilla achieves 77.4%
accuracy, compared to 74.5% accuracy fromGopher and 76.6% from MT-NLG 530B (see Table 7). On
RACE-h and RACE-m (Lai et al., 2017),Chinchilla greatly outperformsGopher, improving accuracy
by more than 10% in both cases‚Äîsee Table 7.
4.2.4. BIG-bench
We analysedChinchilla on the same set of BIG-bench tasks (BIG-bench collaboration, 2021) reported
in Rae et al. (2021). Similar to what we observed in MMLU,Chinchilla outperformsGopher on the
vast majority of tasks (see Figure 7). We Ô¨Ånd thatChinchilla improves the average performance
by 10.7%, reaching an accuracy of 65.1% versus 54.4% forGopher. Of the 62 tasks we consider,
Chinchillaperformsworsethan Gopherononlyfour‚Äî crash_blossom, dark_humor_detection,
11
college_mathematics
econometrics
moral_scenarios
formal_logic
medical_genetics
machine_learning
public_relations
global_facts
business_ethics
electrical_engineering
college_computer_science
world_religions
high_school_us_history
high_school_psychology
management
high_school_computer_science
marketing
high_school_physics
high_school_macroeconomics
sociology
high_school_government_and_politics
high_school_european_history
nutrition
college_medicine
astronomy
logical_fallacies
professional_psychology
miscellaneous
jurisprudence
clinical_knowledge
high_school_geography
high_school_biology
college_biology
college_chemistry
high_school_world_history
us_foreign_policy
virology
philosophy
moral_disputes
human_aging
computer_security
security_studies
international_law
high_school_microeconomics
high_school_statistics
professional_accounting
professional_medicine
prehistory
high_school_chemistry
elementary_mathematics
abstract_algebra
anatomy
professional_law
human_sexuality
college_physics
high_school_mathematics
conceptual_physics
10
0
10
20
30
Relative Improvement 
 over Gopher
Figure 6j MMLU results compared toGopher We Ô¨Ånd thatChinchilla outperformsGopher by 7.6%
on average (see Table 6) in addition to performing better on 51/57 individual tasks, the same on
2/57, and worse on only 4/57 tasks.
Chinchilla Gopher GPT-3 MT-NLG 530B
LAMBADA Zero-Shot 77.4 74.5 76.2 76.6
RACE-m Few-Shot 86.8 75.1 58.1 -
RACE-h Few-Shot 82.3 71.6 46.8 47.9
Table7jReadingcomprehension. OnRACE-handRACE-m(Laietal.,2017), Chinchillaconsiderably
improves performance overGopher. Note that GPT-3 and MT-NLG 530B use a diÔ¨Äerent prompt format
than we do on RACE-h/m, so results are not comparable toGopher and Chinchilla. On LAMBADA
(Paperno et al., 2016),Chinchilla outperforms bothGopher and MT-NLG 530B.
mathematical_induction and logical_args. Full accuracy results forChinchilla can be found
in Table A7.
4.2.5. Common sense
We evaluateChinchilla on various common sense benchmarks: PIQA (Bisk et al., 2020), SIQA (Sap
et al., 2019), Winogrande (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), and BoolQ
(Clark et al., 2019). We Ô¨Ånd thatChinchilla outperforms bothGopher and GPT-3 on all tasks and
outperforms MT-NLG 530B on all but one task‚Äîsee Table 8.
On TruthfulQA (Lin et al., 2021),Chinchilla reaches 43.6%, 58.5%, and 66.7% accuracy with
0-shot,5-shot,and10-shotrespectively. Incomparison, Gopherachievedonly29.5%0-shotand43.7%
10-shot accuracy. In stark contrast with the Ô¨Åndings of Lin et al. (2021), the large improvements
(14.1% in 0-shot accuracy) achieved by Chinchilla suggest that better modelling of the pre-training
data alone can lead to substantial improvements on this benchmark.
12
crash_blossom
dark_humor_detection
mathematical_induction
logical_args
general_knowledge_json
Human_organs_senses_multiple_choice
formal_fallacies_syllogisms_negation
known_unknowns
navigate
sentence_ambiguity
moral_permissibility
intent_recognition
irony_identification
entailed_polarity
hyperbaton
misconceptions
evaluating_information_essentiality
similarities_abstraction
epistemic_reasoning
fantasy_reasoning
movie_dialog_same_or_different
winowhy
novel_concepts
discourse_marker_prediction
strategyqa
causal_judgment
hindu_knowledge
phrase_relatedness
alignment_questionnaire
reasoning_about_colored_objects
date_understanding
penguins_in_a_table
figure_of_speech_detection
disambiguation_q
implicatures
SNARKS
ruin_names
logical_fallacy_detection
anachronisms
logic_grid_puzzle
riddle_sense
analytic_entailment
question_selection
nonsense_words_grammar
physics_mc
empirical_judgments
sports_understanding
crass_ai
physical_intuition
timedial
implicit_relations
english_proverbs
presuppositions_as_nli
movie_recommendation
understanding_fables
metaphor_boolean
temporal_sequences
logical_sequence
identify_odd_metaphor
gre_reading_comprehension
odd_one_out
analogical_similarity
20
0
20
40
60
80
100
120
Relative Improvement 
 over Gopher
Figure 7j BIG-bench results compared toGopher Chinchilla out performsGopher on all but four
BIG-bench tasks considered. Full results are in Table A7.
4.2.6. Closed-book question answering
Results on closed-book question answering benchmarks are reported in Table 9. On the Natural
Questions dataset (Kwiatkowski et al., 2019),Chinchilla achieves new closed-book SOTA accuracies:
31.5% 5-shot and 35.5% 64-shot, compared to 21% and 28% respectively, forGopher. On TriviaQA
(Joshi et al., 2017) we show results for both the Ô¨Åltered (previously used in retrieval and open-book
work) and unÔ¨Åltered set (previously used in large language model evaluations). In both cases,
Chinchilla substantially out performsGopher. On the Ô¨Åltered version, Chinchilla lags behind the open
book SOTA (Izacard and Grave, 2020) by only 7.9%. On the unÔ¨Åltered set,Chinchilla outperforms
GPT-3‚Äîsee Table 9.
4.2.7. Gender bias and toxicity
Large Language Models carry potential risks such as outputting oÔ¨Äensive language, propagating
social biases, and leaking private information (Bender et al., 2021; Weidinger et al., 2021). We
expect Chinchilla to carry risks similar toGopher because Chinchilla is trained on the same data,
Chinchilla Gopher GPT-3 MT-NLG 530B Supervised SOTA
HellaSWAG 80.8% 79.2% 78.9% 80.2% 93.9%
PIQA 81.8% 81.8% 81.0% 82.0% 90.1%
Winogrande 74.9% 70.1% 70.2% 73.0% 91.3%
SIQA 51.3% 50.6% - - 83.2%
BoolQ 83.7% 79.3% 60.5% 78.2% 91.4%
Table 8j Zero-shot comparison on Common Sense benchmarks.We show a comparison between
Chinchilla, Gopher, and MT-NLG 530B on various Common Sense benchmarks. We see thatChinchilla
matches or outperformsGopher and GPT-3 on all tasks. On all but oneChinchilla outperforms the
much larger MT-NLG 530B model.
13
Method Chinchilla Gopher GPT-3 SOTA (open book)
Natural Questions (dev)
0-shot 16.6% 10.1% 14.6%
54.4% 5-shot 31.5% 24.5% -
64-shot 35.5% 28.2% 29.9%
TriviaQA (unÔ¨Åltered, test)
0-shot 67.0% 52.8% 64.3 %
- 5-shot 73.2% 63.6% -
64-shot 72.3% 61.3% 71.2%
TriviaQA (Ô¨Åltered, dev)
0-shot 55.4% 43.5% -
72.5% 5-shot 64.1% 57.0% -
64-shot 64.6% 57.2% -
Table 9j Closed-book question answering.For Natural Questions (Kwiatkowski et al., 2019) and
TriviaQA (Joshi et al., 2017),Chinchilla outperforms Gopher in all cases. On Natural Questions,
Chinchilla outperforms GPT-3. On TriviaQA we show results on two diÔ¨Äerent evaluation sets to allow
for comparison to GPT-3 and to open book SOTA (FiD + Distillation (Izacard and Grave, 2020)).
albeit with slightly diÔ¨Äerent relative weights, and because it has a similar architecture. Here, we
examine gender bias (particularly gender and occupation bias) and generation of toxic language. We
select a few common evaluations to highlight potential issues, but stress that our evaluations are not
comprehensive and much work remains to understand, evaluate, and mitigate risks in LLMs.
Gender bias. As discussed in Rae et al. (2021), large language models reÔ¨Çect contemporary and
historical discourse about diÔ¨Äerent groups (such as gender groups) from their training dataset, and
we expect the same to be true forChinchilla. Here, we test if potential gender and occupation biases
manifest in unfair outcomes on coreference resolutions, using the Winogender dataset (Rudinger
et al., 2018) in a zero-shot setting. Winogender tests whether a model can correctly determine if
a pronoun refers to diÔ¨Äerent occupation words. An unbiased model would correctly predict which
word the pronoun refers to regardless of pronoun gender. We follow the same setup as in Rae et al.
(2021) (described further in Section H.3).
As shown in Table 10,Chinchilla correctly resolves pronouns more frequently thanGopher across
all groups. Interestingly, the performance increase is considerably smaller for male pronouns (increase
of 3.2%) than for female or neutral pronouns (increases of 8.3% and 9.2% respectively). We also
considergotcha examples, in which the correct pronoun resolution contradicts gender stereotypes
(determined by labor statistics). Again, we see thatChinchilla resolves pronouns more accurately
than Gopher. When breaking up examples by male/female gender andgotcha/not gotcha, the largest
improvement is on femalegotchaexamples (improvement of 10%). Thus, thoughChinchillauniformly
overcomes gender stereotypes for more coreference examples thanGopher, the rate of improvement
is higher for some pronouns than others, suggesting that the improvements conferred by using a more
compute-optimal model can be uneven.
Sample toxicity. Language models are capable of generating toxic language‚Äîincluding insults,
hate speech, profanities and threats (Gehman et al., 2020; Rae et al., 2021). While toxicity is an
umbrella term, and its evaluation in LMs comes with challenges (Welbl et al., 2021; Xu et al., 2021),
automatic classiÔ¨Åer scores can provide an indication for the levels of harmful text that a LM generates.
Rae et al. (2021) found that improving language modelling loss by increasing the number of model
parameters has only a negligible eÔ¨Äect on toxic text generation (unprompted); here we analyze
14
Chinchilla Gopher
All 78.3% 71.4%
Male 71.2% 68.0%
Female 79.6% 71.3%
Neutral 84.2% 75.0%
Chinchilla Gopher
Malegotcha 62.5% 59.2%
Malenot gotcha 80.0% 76.7%
Femalegotcha 76.7% 66.7%
Femalenot gotcha 82.5% 75.8%
Table 10j Winogender results. Left:Chinchilla consistently resolves pronouns better thanGopher.
Right: Chinchillaperformsbetteronexampleswhichcontradictgenderstereotypes( gotchaexamples).
However, diÔ¨Äerence in performance across groups suggestsChinchilla exhibits bias.
whether the same holds true for a lower LM loss achieved via more compute-optimal training. Similar
to the protocol of Rae et al. (2021), we generate 25,000 unprompted samples fromChinchilla, and
compare theirPerspectiveAPItoxicity score distribution to that ofGopher-generated samples. Several
summary statistics indicate an absence of major diÔ¨Äerences: the mean (median) toxicity score for
Gopher is 0.081 (0.064), compared to 0.087 (0.066) forChinchilla, and the95th percentile scores
are 0.230 forGopher, compared to 0.238 forChinchilla. That is, the large majority of generated
samples are classiÔ¨Åed as non-toxic, and the diÔ¨Äerence between the models is negligible. In line with
prior Ô¨Åndings (Rae et al., 2021), this suggests that toxicity levels in unconditional text generation
are largely independent of the model quality (measured in language modelling loss), i.e. that better
models of the training dataset are not necessarily more toxic.
5. Discussion & Conclusion
The trend so far in large language model training has been to increase the model size, often without
increasing the number of training tokens. The largest dense transformer, MT-NLG 530B, is now
over 3 larger than GPT-3‚Äôs 170 billion parameters from just two years ago. However, this model,
as well as the majority of existing large models, have all been trained for a comparable number
of tokens‚Äîaround 300 billion. While the desire to train these mega-models has led to substantial
engineering innovation, we hypothesize that the race to train larger and larger models is resulting in
models that are substantially underperforming compared to what could be achieved with the same
compute budget.
We propose three predictive approaches towards optimally setting model size and training dura-
tion, based on the outcome of over 400 training runs. All three approaches predict thatGopher is
substantially over-sized and estimate that for the same compute budget a smaller model trained on
more data will perform better. We directly test this hypothesis by trainingChinchilla, a 70B parameter
model, and show that it outperformsGopher and even larger models on nearly every measured
evaluation task.
Whilst our method allows us to make predictions on how to scale large models when given
additional compute, there are several limitations. Due to the cost of training large models, we only
have two comparable training runs at large scale (Chinchilla and Gopher), and we do not have
additional tests at intermediate scales. Furthermore, we assume that the eÔ¨Écient computational
frontier can be described by a power-law relationship between the compute budget, model size, and
numberoftrainingtokens. However, weobservesomeconcavityin log  
ùëÅùëúùëùùë°
 athighcomputebudgets
(see Appendix E). This suggests that we may still be overestimating the optimal size of large models.
Finally, the training runs for our analysis have all been trained on less than an epoch of data; future
work may consider the multiple epoch regime. Despite these limitations, the comparison ofChinchilla
to Gopher validates our performance predictions, that have thus enabled training a better (and more
15
lightweight) model at the same compute budget.
Though there has been signiÔ¨Åcant recent work allowing larger and larger models to be trained,
our analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that
scaling to larger and larger datasets is only beneÔ¨Åcial when the data is high-quality. This calls for
responsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require
extra care to ensure train-test set overlap is properly accounted for, both in the language modelling
loss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical
and privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and
private information. With even larger datasets being used, the quantity (if not the frequency) of such
information increases, which makes dataset introspection all the more important.Chinchilla does
suÔ¨ÄerfrombiasandtoxicitybutinterestinglyitseemslessaÔ¨Äectedthan Gopher. Betterunderstanding
how performance of large language models and toxicity interact is an important future research
question.
While we have applied our methodology towards the training of auto-regressive language models,
we expect that there is a similar trade-oÔ¨Ä between model size and the amount of data in other
modalities. As training large models is very expensive, choosing the optimal model size and training
steps beforehand is essential. The methods we propose are easy to reproduce in new settings.
6. Acknowledgements
We‚ÄôdliketothankJean-baptisteAlayrac,KareemAyoub,ChrisDyer,NandodeFreitas,DemisHassabis,
GeoÔ¨Ärey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on
the manuscript. We‚Äôd like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and
other colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA
team for their support and assistance.
References
M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru,
G. Anantharaman, X. Li, S. Chen, H. Akin, M. Baines, L. Martin, X. Zhou, P. S. Koura, B. O‚ÄôHoro,
J. Wang, L. Zettlemoyer, M. Diab, Z. Kozareva, and V. Stoyanov. EÔ¨Écient Large Scale Language
Modeling with Mixtures of Experts.arXiv:2112.10684, 2021.
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots:
Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness,
Accountability,and Transparency, pages 610‚Äì623, 2021.
BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of
language models.In preparation, 2021. URLhttps://github.com/google/BIG-bench/.
Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in natural
language. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34, pages
7432‚Äì7439, 2020.
S. Borgeaud, A. Mensch, J. HoÔ¨Ämann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J.-B.
Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,
L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero,
K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from
trillions of tokens.arXiv 2112.04426, 2021.
16
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-
derPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy
programs. 2018. URLhttp://github.com/google/jax.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot
learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,Advances
in NeuralInformationProcessing Systems, volume 33, pages 1877‚Äì1901. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb49674
18bfb8ac142f64a-Paper.pdf.
S. Bubeck. Convex Optimization: Algorithms and Complexity.Foundationsand Trendsin Machine
Learning, 8(3-4):231‚Äì357, 2015. URLhttp://www.nowpublishers.com/article/Detail
s/MAL-050.
A. Clark, D. d. l. Casas, A. Guy, A. Mensch, M. Paganini, J. HoÔ¨Ämann, B. Damoc, B. Hechtman,
T. Cai, S. Borgeaud, G. v. d. Driessche, E. Rutherford, T. Hennigan, M. Johnson, K. Millican,
A. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. Rae, E. Elsen,
K. Kavukcuoglu, and K. Simonyan. UniÔ¨Åed scaling laws for routed language models, 2022. URL
https://arxiv.org/abs/2202.01169.
C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring
the surprising diÔ¨Éculty of natural yes/no questions. InProceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies,Volume1 (Long and ShortPapers), pages 2924‚Äì2936, 2019.
N.Du, Y.Huang, A.M.Dai, S.Tong, D.Lepikhin, Y.Xu, M.Krikun, Y.Zhou, A.W.Yu, O.Firat, B.Zoph,
L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-
Hellstern,T.Duke,L.Dixon,K.Zhang,Q.V.Le,Y.Wu,Z.Chen,andC.Cui. Glam: EÔ¨Écientscalingof
language models with mixture-of-experts, 2021. URLhttps://arxiv.org/abs/2112.06905.
W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with
simple and eÔ¨Écient sparsity.arXiv preprint arXiv:2101.03961, 2021.
L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,
S. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling.arXiv
preprint arXiv:2101.00027, 2020.
S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating
neural toxic degeneration in language models. InFindings ofthe Association for Computational
Linguistics: EMNLP 2020, pages 3356‚Äì3369, Online, Nov. 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URLhttps://aclanthology.org/2
020.findings-emnlp.301.
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model
pre-training, 2020.
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive
multitask language understanding.arXiv preprint arXiv:2009.03300, 2020.
T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX. 2020. URLhttp:
//github.com/deepmind/dm-haiku.
17
D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer, 2021.
P. J. Huber. Robust Estimation of a Location Parameter.The Annals of MathematicalStatistics, 35
(1):73‚Äì101, Mar. 1964. ISSN 0003-4851, 2168-8990. doi: 10.1214/aoms/1177703732. URL
https://projecteuclid.org/journals/annals-of-mathematical-statistics/vol
ume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/11
77703732.full.
G. Izacard and E. Grave. Distilling knowledge from reader to retriever for question answering, 2020.
M.Joshi,E.Choi,D.Weld,andL.Zettlemoyer. TriviaQA:ALargeScaleDistantlySupervisedChallenge
Dataset for Reading Comprehension.arXiv e-prints, art. arXiv:1705.03551, 2017.
N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,
A. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb,
T.V.Ghaemmaghami, R.Gottipati, W.Gulland, R.Hagmann, C.R.Ho, D.Hogberg, J.Hu, R.Hundt,
D.Hurt,J.Ibarz,A.JaÔ¨Äey,A.Jaworski,A.Kaplan,H.Khaitan,D.Killebrew,A.Koch,N.Kumar,S.Lacy,
J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony,
K. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda,
A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter,
D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter,
W. Wang, E. Wilcox, and D. H. Yoon. In-datacenter performance analysis of a tensor processing unit.
In Proceedings ofthe 44th AnnualInternationalSymposiumon Computer Architecture, ISCA ‚Äô17,
page1‚Äì12, NewYork, NY,USA,2017.AssociationforComputingMachinery. ISBN9781450348928.
doi: 10.1145/3079856.3080246. URLhttps://doi.org/10.1145/3079856.3080246.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models.arXiv preprint arXiv:2001.08361, 2020.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.arXivpreprintarXiv:1412.6980,
2014.
T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.arXiv preprint arXiv:1808.06226, 2018.
T. Kwiatkowski, J. Palomaki, O. RedÔ¨Åeld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,
M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and
S. Petrov. Natural questions: a benchmark for question answering research.Transactionsof the
AssociationofComputationalLinguistics, 2019.
G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehension dataset from
examinations. InProceedings ofthe 2017 Conferenceon EmpiricalMethods in NaturalLanguage
Processing, pages 785‚Äì794, Copenhagen, Denmark, Sept. 2017. Association for Computational
Linguistics. doi: 10.18653/v1/D17-1082. URLhttps://aclanthology.org/D17-1082.
Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention.
arXiv preprint arXiv:2006.12467, 2020.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K√ºttler, M. Lewis, W.-t. Yih,
T. Rockt√§schel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive
nlp tasks. InAdvances in NeuralInformationProcessing Systems, volume 33, pages 9459‚Äì9474,
2020.
18
O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation.White
Paper.AI21 Labs, 2021.
S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods.arXiv
preprint arXiv:2109.07958, 2021.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. InInternationalConferenceon
Learning Representations, 2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.
S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training,
2018.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models.International
Conferenceon Learning Representations, 2017.
M.Mitchell, S.Wu, A.Zaldivar, P.Barnes, L.Vasserman, B.Hutchinson, E.Spitzer, I.D.Raji, andT.Ge-
bru. Model cards for model reporting. InProceedingsoftheconferenceonfairness, accountability,
and transparency, pages 220‚Äì229, 2019.
J. Nocedal. Updating Quasi-Newton Matrices with Limited Storage.Mathematics of Computation,
35(151):773‚Äì782, 1980. ISSN 0025-5718. doi: 10.2307/2006193. URLhttps://www.jstor.
org/stable/2006193.
D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,
and R. Fern√°ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context,
2016.
J. Rae, S. Borgeaud, T. Cai, K. Millican, J. HoÔ¨Ämann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A.
Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor,
I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden,
E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh,
E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev,
D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d‚ÄôAutume, Y. Li,
T. Terzi, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury, M. Johnson, L. Weidinger,
I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway,
L.Bennett, D.Hassabis, K.Kavukcuoglu, andG.Irving. Scalinglanguagemodels: Methods, analysis
& insights from training Gopher.arXiv 2112.11446, 2021.
J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, K. Choromanski, V. Likhosherstov, D. Dohan,
X. Song, A. Gane, T. Sarlos, et al. Compressive transformers for long-range sequence modelling.
Advancesin NeuralInformationProcessing Systems, 33:6154‚Äì6158, 2020.
C. RaÔ¨Äel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring
the limits of transfer learning with a uniÔ¨Åed text-to-text transformer.JournalofMachine Learning
Research, 21(140):1‚Äì67, 2020a. URLhttp://jmlr.org/papers/v21/20-074.html.
C. RaÔ¨Äel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring
the limits of transfer learning with a uniÔ¨Åed text-to-text transformer.JournalofMachine Learning
Research, 21(140):1‚Äì67, 2020b.
S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training
trillion parameter models. InSC20: International Conference for High PerformanceComputing,
Networking, Storage and Analysis, pages 1‚Äì16. IEEE, 2020.
19
H.RobbinsandS.Monro. AStochasticApproximationMethod. TheAnnalsofMathematicalStatistics,
22(3):400‚Äì407, Sept. 1951.
R. Rudinger, J. Naradowsky, B. Leonard, and B. Van Durme. Gender bias in coreference resolu-
tion. InProceedings ofthe 2018 Conferenceofthe NorthAmericanChapterofthe Associationfor
Computational Linguistics: Human Language Technologies, New Orleans, Louisiana, June 2018.
Association for Computational Linguistics.
K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema
challenge at scale. InProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34,
pages 8732‚Äì8740, 2020.
M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi. SocialIQA: Commonsense reasoning about
socialinteractions. Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage
Processing, 2019.
C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the eÔ¨Äects
of data parallelism on neural network training.arXiv preprint arXiv:1811.03600, 2018.
J. W. Siegel and J. Xu. Approximation rates for neural networks with general activation functions.
NeuralNetworks, 128:313‚Äì321, Aug. 2020. URLhttps://www.sciencedirect.com/scienc
e/article/pii/S0893608020301891.
S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,
G. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi,
Y. He, M. Houston, S. Tiwary, and B. Catanzaro. Using Deepspeed and Megatron to Train Megatron-
turing NLG 530b, A Large-Scale Generative Language Model.arXiv preprint arXiv:2201.11990,
2022.
J. Steinhardt. Updates and lessons from AI forecasting, 2021. URLhttps://bounded-regret.g
host.io/ai-forecasting/.
Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama, A. Vaswani,
and D. Metzler. Scale eÔ¨Éciently: Insights from pre-training and Ô¨Åne-tuning transformers, 2021.
R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,
Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin,
J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch,
M. Pickett, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zeven-
bergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. HoÔ¨Äman-John, J. Lee,
L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,
R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. LaMDA: Language models for
dialog applications, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin.
Attention is all you need. InAdvancesinneuralinformationprocessingsystems, pages 5998‚Äì6008,
2017.
L. Weidinger, J. Mellor, M. Rauh, C. GriÔ¨Én, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle,
A.Kasirzadeh,Z.Kenton,S.Brown,W.Hawkins,T.Stepleton,C.Biles,A.Birhane,J.Haas,L.Rimell,
L. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm
from language models.arXiv submission, 2021.
20
J. Welbl, A. Glaese, J. Uesato, S. Dathathri, J. Mellor, L. A. Hendricks, K. Anderson, P. Kohli, B. Coppin,
and P.-S. Huang. Challenges in detoxifying language models. InFindings of the Association for
Computational Linguistics: EMNLP 2021, pages 2447‚Äì2469, Punta Cana, Dominican Republic,
Nov. 2021. Association for Computational Linguistics. URLhttps://aclanthology.org/2021.
findings-emnlp.210.
A. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. Detoxifying language models
risks marginalizing minority voices. InProceedings ofthe 2021 Conferenceofthe NorthAmerican
Chapter of the Association for Computational Linguistics: Human Language Technologies, pages
2390‚Äì2397, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021
.naacl-main.190. URL https://aclanthology.org/2021.naacl-main.190.
G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao.
Tuning large neural networks via zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. W. Vaughan, editors,Advances in Neural InformationProcessing Systems, 2021.
URL https://openreview.net/forum?id=Bx6qKuBM2AD.
R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really Ô¨Ånish
your sentence? InProceedings of the 57th AnnualMeeting of the Association for Computational
Linguistics, 2019.
G. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which
algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, editors,Advances
in NeuralInformationProcessing Systems, volume 32. Curran Associates, Inc., 2019. URLhttps:
//proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b621
4-Paper.pdf.
B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. Designing eÔ¨Äective
sparse expert models, 2022.
21
Appendix
A. Training dataset
In Table A1 we show the training dataset makeup used forChinchilla and all scaling runs. Note that
both theMassiveWeband Wikipedia subsets are both used for more than one epoch.
Disk Size Documents Sampling proportion Epochs in 1.4T tokens
MassiveWeb 1.9 TB 604M 45% (48%) 1.24
Books 2.1 TB 4M 30% (27%) 0.75
C4 0.75 TB 361M 10% (10%) 0.77
News 2.7 TB 1.1B 10% (10%) 0.21
GitHub 3.1 TB 142M 4% (3%) 0.13
Wikipedia 0.001 TB 6M 1% (2%) 3.40
Table A1j MassiveTextdata makeup.For each subset ofMassiveText, we list its total disk size, the
number of documents and the sampling proportion used during training‚Äîwe use a slightly diÔ¨Äerent
distribution than in Rae et al. (2021) (shown in parenthesis). In the rightmost column show the
number of epochs that are used in 1.4 trillion tokens.
B. Optimal cosine cycle length
One key assumption is made on the cosine cycle length and the corresponding learning rate drop
(we use a 10 learning rate decay in line with Rae et al. (2021)).9 We Ô¨Ånd that setting the cosine
cycle length too much longer than the target number of training steps results in sub-optimally trained
models, as shown in Figure A1. As a result, we assume that an optimally trained model will have the
cosine cycle length correctly calibrated to the maximum number of steps, given the FLOP budget; we
follow this rule in our main analysis.
C. Consistency of scaling results across datasets
WeshowscalingresultsfromanIsoFLOP(Approach2)analysisaftertrainingontwodiÔ¨Äerentdatasets:
C4 (RaÔ¨Äel et al., 2020b) and GitHub code (we show results with data from Rae et al. (2021)), results
are shown in Table A2. For both set of experiments using subsets ofMassiveText, we use the same
tokenizer as theMassiveTextexperiments.
WeÔ¨Åndthatthescalingbehaviouronthesedatasetsisverysimilartowhatwefoundon MassiveText,
as shown in Figure A2 and Table A2. This suggests that our results are independent of the dataset as
long as one does not train for more than one epoch.
9We Ô¨Ånd the diÔ¨Äerence between decaying by10 and decaying to 0.0 (over the same number of steps) to be small,
though decaying by a factor of10 to be slightly more performant. Decaying by less (5) is clearly worse.
22
0 2 4 6 8
Million Sequences
0.0
0.2
0.4
0.6
0.8
1.0Learning Rate/Max LR
0 2 4 6 8
Million Sequences
2.70
2.75
2.80
2.85
2.90
2.95
3.00Training Loss
0 2 4 6
Million Sequences
2.80
2.85
2.90
2.95
3.00
3.05
3.10
3.15
3.20C4 Loss
Cosine Cycle Length
1.0√ó num. steps
1.1√ó num. steps
1.25√ó num. steps
1.5√ó num. steps
2.0√ó num. steps
5.0√ó num. steps
0.0 2.5 5.0 7.5 10.0 12.5
Million Sequences
0.0
0.2
0.4
0.6
0.8
1.0Learning Rate/Max LR
0.0 2.5 5.0 7.5 10.0 12.5
Million Sequences
2.70
2.75
2.80
2.85
2.90
2.95
3.00Training Loss
0.0 2.5 5.0 7.5 10.0 12.5
Million Sequences
2.80
2.85
2.90
2.95
3.00
3.05
3.10
3.15
3.20C4 Loss
Figure A1j Grid over cosine cycle length.We show 6 curves with the cosine cycle length set to 1,
1.1, 1.25, 1.5, 2, and5longerthanthetargetnumberoftrainingsteps. Whenthecosinecyclelength
is too long, and the learning rate does not drop appropriately, then performance is impaired. We Ô¨Ånd
that overestimating the number of training steps beyond 25% leads to clear drops in performance.
We show results where we have set the number of training steps to two diÔ¨Äerent values (top and
bottom).
100M 300M 1B 3B 6B 30B
Parameters
2.0
2.2
2.4
2.6
2.8
3.0
3.2C4 Training Loss
1e19
1e20
6e20
1e21
1017 1019 1021 1023 1025
FLOPs
100M
1B
10B
100B
1T
Parameters
73B
1017 1019 1021 1023 1025
FLOPs
100M
1B
10B
100B
1T
10T
Tokens
1.3T
100M 300M 1B 3B 6B 30B
Parameters
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0GitHub Training Loss
1e19
1e20
6e20
1e21
1017 1019 1021 1023 1025
FLOPs
100M
1B
10B
100B
1T
Parameters
59B
1017 1019 1021 1023 1025
FLOPs
100M
1B
10B
100B
1T
10T
Tokens
1.6T
FigureA2 j C4andGitHubIsoFLOPcurves. UsingtheC4dataset(RaÔ¨Äeletal.,2020b)andaGitHub
dataset (Rae et al., 2021), we generate 4 IsoFLOP proÔ¨Åles and show the parameter and token count
scaling, as in Figure 3. Scaling coeÔ¨Écients are shown in Table A2.
23
Approach Coef. ùëé where ùëÅùëúùëùùë°/ ùê∂ùëé Coef. ùëè where ùê∑ùëúùëùùë°/ ùê∂ùëè
C4 0.50 0.50
GitHub 0.53 0.47
Kaplan et al. (2020) 0.73 0.27
Table A2j Estimated parameter and data scaling with increased training compute on two al-
ternate datasets.The listed values are the exponents,ùëé and ùëè, on the relationshipùëÅùëúùëùùë°/ ùê∂ùëé and
ùê∑ùëúùëùùë°/ ùê∂ùëè. Using IsoFLOP proÔ¨Åles, we estimate the scaling on two diÔ¨Äerent datasets.
D. Details on the scaling analyses
D.1. Approach 1: Fixing model sizes and varying training sequences
We use a maximum learning rate of2 10 4 for the smallest models and1¬ì25 10 4 for the largest
models. Inallcases, thelearningratedropsbyafactorof 10duringtraining, usingacosineschedule.
We make the assumption that the cosine cycle length should be approximately matched to the number
of training steps. We Ô¨Ånd that when the cosine cycle overshoots the number of training steps by more
than 25%, performance is noticeably degraded‚Äîsee Figure A1.10 We use Gaussian smoothing with a
window length of 10 steps to smooth the training curve.
D.2. Approach 3: Parametric Ô¨Åtting of the loss
In this section, we Ô¨Årst show how Equation(2) can be derived. We repeat the equation below for
clarity,
ÀÜùêø¬πùëÅ¬î ùê∑¬∫ , ùê∏¬∏ ùê¥
ùëÅ ùõº¬∏ ùêµ
ùê∑ùõΩ ¬î (5)
based on a decomposition of the expected risk between a function approximation term and an
optimisation suboptimality term. We then give details on the optimisation procedure for Ô¨Åtting the
parameters.
Loss decomposition. Formally, we consider the task of predicting the next tokenùë¶2Y based on
the previous tokens in a sequenceùë•2Y ùë†, with ùë† varying from0 to ùë†max‚Äîthe maximum sequence
length. We consider a distributionùëÉ2D¬πXY¬∫ of tokens inY and their past inX. A predictor
ùëì :X!D¬πY¬∫ computes the probability of each token given the past sequence. The Bayes classiÔ¨Åer,
ùëì ‚òÖ, minimizes the cross-entropy ofùëì¬πùë•¬∫ with the observed tokensùë¶, with expectation taken on the
whole data distribution. We letùêø be the expected risk
ùêø¬π ùëì¬∫ , ùîº¬ªlog ùëì¬πùë•¬∫ ùë¶¬º¬î and set ùëì ‚òÖ , argmin
ùëì2F¬πX ¬îD¬πY¬∫¬∫
ùêø¬π ùëì¬∫¬ì (6)
The set of all transformers of sizeùëÅ, that we denoteHùëÅ, forms a subset of all functions that map
sequences to distributions of tokensX!D¬πY¬∫ . Fitting a transformer of sizeùëÅ on the expected risk
ùêø¬π ùëì¬∫ amounts to minimizing such risk on a restricted functional space
ùëìùëÅ , argmin
ùëì2H ùëÅ
ùêø¬π ùëì¬∫¬ì (7)
When we observe a dataset¬πùë•ùëñ¬î ùë¶ùëñ¬∫ùëñùëñ2¬ª 1¬îùê∑¬º of sizeùê∑, we do not have access toùîºùëÉ, but instead to the
empirical expectation ÀÜùîºùê∑ over the empirical distributionÀÜùëÉùê∑. What happens when we are givenùê∑
10This further emphasises the point of not only determining model size, but also training length before training begins.
24
datapoints that we can only see once, and when we constrain the size of the hypothesis space to be
ùëÅ-dimensional? WearemakingstepstowardminimizingtheempiricalriskwithinaÔ¨Ånite-dimensional
functional spaceHùëÅ:
ÀÜùêøùê∑¬π ùëì¬∫ , ÀÜùîºùê∑¬ªlog ùëì¬πùë•¬∫ ùë¶¬º¬î setting ÀÜùëìùëÅ¬îùê∑ , argmin
ùëì2H ùëÅ
ÀÜùêøùê∑¬π ùëì¬∫¬ì (8)
We are never able to obtainÀÜùëìùëÅ¬îùê∑ as we typically perform a single epoch over the dataset of sizeùê∑.
Instead, be obtain¬ØùëìùëÅ¬îùê∑, which is the result of applying a certain number of gradient steps based on
the ùê∑ datapoints‚Äîthe number of steps to perform depends on the gradient batch size, for which we
use well-tested heuristics.
Using the Bayes-classiÔ¨Åerùëì ‚òÖ, the expected-risk minimizerùëìùëÅ and the ‚Äúsingle-epoch empirical-risk
minimizer‚Äù ¬ØùëìùëÅ¬îùê∑, we can Ô¨Ånally decompose the lossùêø¬πùëÅ¬î ùê∑¬∫ into
ùêø¬πùëÅ¬î ùê∑¬∫ , ùêø¬π ¬ØùëìùëÅ¬îùê∑¬∫ = ùêø¬π ùëì ‚òÖ¬∫¬∏  
ùêø¬π ùëìùëÅ¬∫  ùêø¬π ùëì ‚òÖ¬∫¬∏  
ùêø¬π ¬ØùëìùëÅ¬îùê∑¬∫  ùêø¬π ùëìùëÅ¬∫
¬ì (9)
The loss comprises three terms: the Bayes risk, i.e. the minimal loss achievable for next-token
prediction on the full distributionùëÉ, a.k.a the ‚Äúentropy of natural text.‚Äù; a functional approximation
term that depends on the size of the hypothesis space; Ô¨Ånally, a stochastic approximation term that
capturesthesuboptimalityofminimizing ÀÜùêøùê∑ insteadof ùêø,andofmakingasingleepochontheprovided
dataset.
Expected forms of the loss terms.In the decomposition(9), the second term depends entirely on
the number of parametersùëÅ that deÔ¨Ånes the size of the functional approximation space.On the set
of two-layer neural networks, it is expected to be proportional to1
ùëÅ1¬ù2 (Siegel and Xu, 2020). Finally,
given that it corresponds to early stopping in stochastic Ô¨Årst order methods, the third term should
scale as the convergence rate of these methods, which is lower-bounded by1
ùê∑1¬ù2 (Robbins and Monro,
1951) (and may attain the bound). This convergence rate is expected to be dimension free (see e.g.
Bubeck, 2015, for a review) and depends only on the loss smoothness; hence we assume that the
second term only depends onùê∑ in (2). Empirically, we Ô¨Ånd after Ô¨Åtting (2) that
ùêø¬πùëÅ¬î ùê∑¬∫ = ùê∏¬∏ ùê¥
ùëÅ0¬ì34¬∏ ùêµ
ùê∑0¬ì28 ¬î (10)
with ùê∏ = 1¬ì69, ùê¥ = 406¬ì4, ùêµ = 410¬ì7. We note that the parameter/data coeÔ¨Écients are both lower
than 1
2; this is expected for the data-eÔ¨Éciency coeÔ¨Écient (but far from the known lower-bound).
Future models and training approaches should endeavor to increase these coeÔ¨Écients.
Fitting the decomposition to data.We eÔ¨Äectively minimize the following problem
min
ùëé¬îùëè¬îùëí¬îùõº¬îùõΩ
‚àëÔ∏Å
Run ùëñ
Huberùõø

LSE 
ùëé  ùõº log ùëÅùëñ¬î ùëè  ùõΩ log ùê∑ùëñ¬î ùëí
  log ùêøùëñ

¬î (11)
where ùêøùëÜùê∏ is the log-sum-exp operator. We then setùê¥¬î ùêµ¬î ùê∏ = exp¬πùëé¬∫¬î exp¬πùëè¬∫¬î exp¬πùëí¬∫.
We use the LBFGS algorithm to Ô¨Ånd local minima of the objective above, started on a grid
of initialisation given by: ùõº 2 f0¬ì¬î 0¬ì5¬î ¬ì ¬ì ¬ì ¬î 2¬ìg, ùõΩ 2 f0¬ì¬î 0¬ì5¬î ¬ì ¬ì ¬ì ¬î 2¬ìg, ùëí 2 f 1¬ì¬î ¬ì5¬î ¬ì ¬ì ¬ì ¬î 1¬ìg, ùëé 2
f0¬î 5¬î ¬ì ¬ì ¬ì ¬î 25g, andùëè2f 0¬î 5¬î ¬ì ¬ì ¬ì ¬î 25g. We Ô¨Ånd that the optimal initialisation is not on the boundary of
our initialisation sweep.
We useùõø = 10 3 for the Huber loss. We Ô¨Ånd that using larger values ofùõø pushes the model to
overÔ¨Åt the small compute regime and poorly predict held-out data from larger runs. We Ô¨Ånd that
using aùõø smaller than10 3 does not impact the resulting predictions.
25
D.3. Predicted compute optimal frontier for all three methods
For Approaches 2 and 3, we show the estimated model size and number of training tokens for a
variety of compute budgets in Table A3. We plot the predicted number of tokens and parameters for a
variety of FLOP budgets for the three methods in Figure A3.
Approach 2 Approach 3
Parameters FLOPs Tokens FLOPs Tokens
400 Million 1.84e+19 7.7 Billion 2.21e+19 9.2 Billion
1 Billion 1.20e+20 20.0 Billion 1.62e+20 27.1 Billion
10 Billion 1.32e+22 219.5 Billion 2.46e+22 410.1 Billion
67 Billion 6.88e+23 1.7 Trillion 1.71e+24 4.1 Trillion
175 Billion 4.54e+24 4.3 Trillion 1.26e+24 12.0 Trillion
280 Billion 1.18e+25 7.1 Trillion 3.52e+25 20.1 Trillion
520 Billion 4.19e+25 13.4 Trillion 1.36e+26 43.5 Trillion
1 Trillion 1.59e+26 26.5 Trillion 5.65e+26 94.1 Trillion
10 Trillion 1.75e+28 292.0 Trillion 8.55e+28 1425.5 Trillion
Table A3j Estimated optimal training FLOPs and training tokens for various model sizes.Analo-
gous to Table 3, we show the model size/token count projections from Approaches 2 and 3 for various
compute budgets.
.
1010 1011 1012 1013
Tokens
108
109
1010
1011
1012
Parameters
1e+18
1e+19
1e+20
1e+21
1e+22
1e+23
1e+24
1e+25
1e+26Approach 1
Approach 2
Approach 3
Chinchilla
Gopher
GPT-3
Megatron-Turing NLG
Figure A3j Optimal number of tokens and parameters for a training FLOP budget.For a Ô¨Åxed
FLOP budget, we show the optimal number of tokens and parameters as predicted by Approaches 1,
2, and 3. For an alternate representation, see Figure 1.
D.4. Small-scale comparison to Kaplanet al. (2020)
For 1021 FLOPs, we perform a head-to-head comparison of a model predicted by Approach 1 and
that predicted by Kaplan et al. (2020). For both models, we use a batch size of 0.5M tokens and a
26
maximum learning rate of1¬ì5 10 4 that decays by10. From Kaplan et al. (2020), we Ô¨Ånd that
the optimal model size should be 4.68 billion parameters. From our approach 1, we estimate a 2.86
billion parameter model should be optimal. We train a 4.74 billion parameter and a 2.80 billion
parameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many
confounding factors as possible. We Ô¨Ånd that our predicted model outperforms the model predicted
by Kaplan et al. (2020) as shown in Figure A4.
0 1 2
Sequences 1e7
2.2
2.3
2.4
2.5
2.6
2.7
2.8Training Loss
0.0 0.2 0.4 0.6 0.8 1.0
FLOPs √ó1021
2.2
2.3
2.4
2.5
2.6
2.7
2.8Training Loss
Kaplan et al (2020)
Approach 1
Figure A4j Comparison to Kaplan et al. (2020) at1021 FLOPs. We train 2.80 and 4.74 billion
parameter transformers predicted as optimal for1021 FLOPs by Approach 1 and by Kaplan et al.
(2020). We Ô¨Ånd that our prediction results in a more performant model at the end of training.
E. Curvature of the FLOP-loss frontier
We observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means
that projections from very small models lead to diÔ¨Äerent predictions than those from larger models.
In Figure A5 we show linear Ô¨Åts using the Ô¨Årst, middle, and Ô¨Ånal third of frontier-points. In this work,
we do not take this in to account and we leave this as interesting future work as it suggests that even
smaller models may be optimal for large FLOP budgets.
F. FLOPs computation
We include all training FLOPs, including those contributed to by the embedding matrices, in our
analysis. Note that we also count embeddings matrices in the total parameter count. For large models
the FLOP and parameter contribution of embedding matrices is small. We use a factor of 2 to describe
the multiply accumulate cost. For the forward pass, we consider contributions from:
‚Ä¢ Embeddings
‚Äì 2seq_lenvocab_sized_model
‚Ä¢ Attention (Single Layer)
‚Äì Key, query and value projections: 2 3seq_lend_model¬π key_sizenum_heads¬∫
27
1017 1018 1019 1020 1021 1022
FLOPS
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0Training loss
75
250
500
1000
2500
5000
10000
Million Parameters
Figure A5j Training curve envelopes.We Ô¨Åt to the Ô¨Årst third (orange), the middle third (green),
and the last third (blue) of all points along the loss frontier. We plot only a subset of the points.
‚Äì Key @ Query logits: 2seq_lenseq_len¬π key_sizenum_heads¬∫
‚Äì Softmax: 3num_headsseq_lenseq_len
‚Äì Softmax @ query reductions: 2seq_lenseq_len¬π key_sizenum_heads¬∫
‚Äì Final Linear: 2seq_len¬π key_sizenum_heads¬∫ d_model
‚Ä¢ Dense Block (Single Layer)
‚Äì 2seq_len¬π d_modelÔ¨Äw_size¬∏d_modelÔ¨Äw_size¬∫
‚Ä¢ Final Logits
‚Äì 2seq_lend_modelvocab_size
‚Ä¢ TotalforwardpassFLOPs: embeddings¬∏num_layers¬πtotal_attention¬∏dense_block¬∫ +logits
As in Kaplan et al. (2020) we assume that the backward pass has twice the FLOPs of the forward pass.
We show a comparison between our calculation and that using the common approximationùê∂ = 6ùê∑ùëÅ
(Kaplan et al., 2020) whereùê∂ is FLOPs,ùê∑ is the number of training tokens, andùëÅ is the number of
parameters in Table A4. We Ô¨Ånd the diÔ¨Äerences in FLOP calculation to be very small and they do not
impact our analysis. Compared to the results presented in Rae et al. (2021), we use a slightly more
Parameters num_layers d_model Ô¨Äw_size num_heads k/q size FLOP Ratio (Ours/6ùëÅ ùê∑)
73M 10 640 2560 10 64 1.03
305M 20 1024 4096 16 64 1.10
552M 24 1280 5120 10 128 1.08
1.1B 26 1792 7168 14 128 1.04
1.6B 28 2048 8192 16 128 1.03
6.8B 40 3584 14336 28 128 0.99
Table A4j FLOP comparison.For a variety of diÔ¨Äerent model sizes, we show the ratio of the FLOPs
that we compute per sequence to that using the6ùëÅ ùê∑approximation.
accurate calculation giving a slightly diÔ¨Äerent value (6¬ì3 1023 compared to5¬ì76 1023).
28
G. Other diÔ¨Äerences betweenChinchilla and Gopher
Beyond diÔ¨Äerences in model size and number of training tokens, there are some additional minor
diÔ¨Äerences betweenChinchilla and Gopher. SpeciÔ¨Åcally,Gopher was trained with Adam (Kingma and
Ba, 2014) whereasChinchilla was trained with AdamW (Loshchilov and Hutter, 2019). Furthermore,
as discussed inLessons Learnedin Rae et al. (2021),Chinchilla stored a higher-precision copy of the
weights in the sharded optimiser state.
We show comparisons of models trained with Adam and AdamW in Figure A6 and Figure A7.
We Ô¨Ånd that, independent of the learning rate schedule, AdamW trained models outperform models
trained with Adam. In Figure A6 we show a comparison of an 680 million parameter model trained
0 5 10 15 20 25 30
Million Sequences
2.45
2.50
2.55
2.60
2.65
2.70Training Loss
0 5 10 15 20 25 30
Million Sequences
17
18
19
20
21
22
23
24
25
26Wikitext103 Perplexity
0 5 10 15 20 25 30
Million Sequences
2.60
2.65
2.70
2.75
2.80
2.85
2.90
2.95
3.00C4 Loss
Training Setup
Adam w/ High Precision
AdamW w/ High Precision
Adam No High Precision
AdamW No High Precision
Figure A6j Comparison of other diÔ¨Äerences.Using an 680 million parameter model, we show a
comparison between the setup used to trainGopher and Chinchilla‚Äî the change in optimiser and
using a higher precision copy of the weights in the optimiser state. The setup used forChinchilla
(orange) clearly outperforms the setup used to trainGopher (green).
0 25 50 75 100 125 150
Million Sequences
2.3
2.4
2.5
2.6
2.7
2.8C4 Loss
0 25 50 75 100 125 150
Million Sequences
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
30.0Wikitext103 Perplexity
0 25 50 75 100 125 150
Million Sequences
0.0
0.1
0.2
0.3
0.4
0.5
0.6LAMBADA Accuracy
417M, Adam
417M, AdamW
1.4B, Adam
1.4B, AdamW
Figure A7j Adam vs AdamW.For a 417M (blue) and 1.4B model (green), we Ô¨Ånd that training with
AdamW improves performance over training with Adam.
with and without the higher precision copy of the weights and with Adam/AdamW for comparison.
H. Results
H.1. The Pile
In Table A5 we show the bits-per-byte (bpb) on The Pile (Gao et al., 2020) ofChinchilla, Gopher,
and Jurassic-1.Chinchilla outperformsGopher on all subsets. Jurassic-1 outperformsChinchilla on 2
subsets‚Äî dm_mathematics and ubuntu_irc.
29
Subset Chinchilla (70B) Gopher (280B) Jurassic-1 (170B)
pile_cc 0.667 0.691 0.669
pubmed_abstracts 0.559 0.578 0.587
stackexchange 0.614 0.641 0.655
github 0.337 0.377 0.358
openwebtext2 0.647 0.677 -
arxiv 0.627 0.662 0.680
uspto_backgrounds 0.526 0.546 0.537
freelaw 0.476 0.513 0.514
pubmed_central 0.504 0.525 0.579
dm_mathematics 1.111 1.142 1.037
hackernews 0.859 0.890 0.869
nih_exporter 0.572 0.590 0.590
opensubtitles 0.871 0.900 0.879
europarl 0.833 0.938 -
books3 0.675 0.712 0.835
philpapers 0.656 0.695 0.742
gutenberg_pg_19 0.548 0.656 0.890
bookcorpus2 0.714 0.741 -
ubuntu_irc 1.026 1.090 0.857
Table A5j Bits-per-Byte on The Pile.Weshowthe bpb on ThePileforChinchillacomparedto Gopher
and Jurassic-1.
H.2. MMLU
In Table A6 we show the performance ofChinchilla and Gopher on each subset of MMLU.
H.3. Winogender Setup
We follow the same setup as in Rae et al. (2021). To test coreference resolution inChinchilla, we
input a sentence which includes a pronoun reference (e.g., ‚ÄúThe librarian helped the child pick out a
book because {pronoun} liked to encourage reading.‚Äù), then measure the probability of the model
completing the sentence ‚Äú‚Äò{Pronoun}‚Äô refers to the‚Äù with diÔ¨Äerent sentence roles (‚Äúlibrarian‚Äù and
‚Äúchild‚Äù in this example). Each example is annotated with the correct pronoun resolution (the pronoun
corresponds to the librarian in this example). Each sentence is tested with a female, male, and
gender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers
to regardless of pronoun gender.
H.4. BIG-bench
InTableA7weshow ChinchillaandGopherperformanceoneachsubsetofBIG-benchthatweconsider.
I. Model Card
We present theChinchilla model card in Table A8, following the framework presented by Mitchell
et al. (2019).
30
Task Chinchilla Gopher Task Chinchilla Gopher
abstract_algebra 31.0 25.0 anatomy 70.4 56.3
astronomy 73.0 65.8 business_ethics 72.0 70.0
clinical_knowledge 75.1 67.2 college_biology 79.9 70.8
college_chemistry 51.0 45.0 college_computer_science 51.0 49.0
college_mathematics 32.0 37.0 college_medicine 66.5 60.1
college_physics 46.1 34.3 computer_security 76.0 65.0
conceptual_physics 67.2 49.4 econometrics 38.6 43.0
electrical_engineering 62.1 60.0 elementary_mathematics 41.5 33.6
formal_logic 33.3 35.7 global_facts 39.0 38.0
high_school_biology 80.3 71.3 high_school_chemistry 58.1 47.8
high_school_computer_science 58.0 54.0 high_school_european_history 78.8 72.1
high_school_geography 86.4 76.8 high_school_gov_and_politics 91.2 83.9
high_school_macroeconomics 70.5 65.1 high_school_mathematics 31.9 23.7
high_school_microeconomics 77.7 66.4 high_school_physics 36.4 33.8
high_school_psychology 86.6 81.8 high_school_statistics 58.8 50.0
high_school_us_history 83.3 78.9 high_school_world_history 85.2 75.1
human_aging 77.6 66.4 human_sexuality 86.3 67.2
international_law 90.9 77.7 jurisprudence 79.6 71.3
logical_fallacies 80.4 72.4 machine_learning 41.1 41.1
management 82.5 77.7 marketing 89.7 83.3
medical_genetics 69.0 69.0 miscellaneous 84.5 75.7
moral_disputes 77.5 66.8 moral_scenarios 36.5 40.2
nutrition 77.1 69.9 philosophy 79.4 68.8
prehistory 81.2 67.6 professional_accounting 52.1 44.3
professional_law 56.5 44.5 professional_medicine 75.4 64.0
professional_psychology 75.7 68.1 public_relations 73.6 71.8
security_studies 75.9 64.9 sociology 91.0 84.1
us_foreign_policy 92.0 81.0 virology 53.6 47.0
world_religions 87.7 84.2
Table A6j Chinchilla MMLU results.For each subset of MMLU (Hendrycks et al., 2020), we show
Chinchilla‚Äôs accuracy compared toGopher.
Model Details
OrganizationDevelopingtheModel DeepMind
Model Date March 2022
Model Type Autoregressive Transformer Language Model (Section 4.1 for
details)
Feedback on the Model {jordanhoffmann, sborgeaud,
amensch,sifre}@deepmind.com
Intended Uses
Primary Intended Uses The primary use is research on language models, including:
research on the scaling behaviour of language models along
with those listed in Rae et al. (2021).
31
Primary Intended Users DeepMindresearchers. Wewillnotmakethismodelavailable
publicly.
Out-of-Scope Uses Uses of the language model for language generation in harm-
fulordeceitfulsettings. Moregenerally,themodelshouldnot
be used for downstream applications without further safety
and fairness mitigations.
Factors
Card Prompts ‚Äì Relevant Factor Relevantfactorsincludewhichlanguageisused. Ourmodelis
trained on English data. Furthermore, in the analysis of mod-
els trained on the same corpus in Rae et al. (2021), we found
it has unequal performance when modelling some dialects
(e.g., African American English). Our model is designed for
research. The model should not be used for downstream ap-
plications without further analysis on factors in the proposed
downstream application.
Card Prompts ‚Äì Evaluation FactorsSee the results in Rae et al. (2021) which analyzes models
trained on the same text corpus.
Metrics
Model Performance Measures
‚Ä¢ Perplexity and bits per byte on language modelling
datasets
‚Ä¢ Accuracy on completion tasks, reading comprehension,
MMLU, BIG-bench and fact checking.
‚Ä¢ Exact match accuracy for question answering.
‚Ä¢ Generation toxicity from Real Toxicity Prompts (RTP)
alongside toxicity classiÔ¨Åcation accuracy.
‚Ä¢ Gender and occupation bias. Test include comparing
the probability of generating diÔ¨Äerent gender terms
and the Winogender coreference resolution task.
We principally focus onChinchilla‚Äôs performance compared
to Gopher on text likelihood prediction.
Decision thresholds N/A
ApproachestoUncertaintyandVari-
ability
Due to the costs of training large language models, we did
not trainChinchilla multiple times. However, the breadth
of our evaluation on a range of diÔ¨Äerent task types gives a
reasonable estimate of the overall performance of the model.
Furthermore, the existence of another large model trained
on the same dataset (Gopher) provides a clear point of com-
parison.
Evaluation Data
32
Datasets
‚Ä¢ Language modelling on LAMBADA, Wikitext103 (Mer-
ity et al., 2017), C4 (RaÔ¨Äel et al., 2020a), PG-19 (Rae
et al., 2020) and the Pile (Gao et al., 2020).
‚Ä¢ Language understanding, real world knowledge,
mathematical and logical reasoning on the Massive
Multitask Language Understanding (MMLU) bench-
mark (Hendrycks et al., 2020) and on the ‚ÄúBeyond the
Imitation Game Benchmark‚Äù (BIG-bench) (BIG-bench
collaboration, 2021).
‚Ä¢ Question answering (closed book) on Natural Ques-
tions (Kwiatkowski et al., 2019) and TriviaQA (Joshi
et al., 2017).
‚Ä¢ Reading comprehension on RACE (Lai et al., 2017)
‚Ä¢ Common sense understanding on HellaSwag (Zellers
et al., 2019), PIQA (Bisk et al., 2020), Wino-
grande(Sakaguchietal.,2020),SIQA(Sapetal.,2019),
BoolQ (Clark et al., 2019), and TruthfulQA (Lin et al.,
2021).
Motivation We chose evaluations from Rae et al. (2021) to allow us to
most directly compare toGopher.
Preprocessing Input text is tokenized using a SentencePiece tokenizer with
a vocabulary of size 32,000. Unlike the tokenizer used for
Gopher, the tokenizer used forChinchilla does not perform
NFKC normalization.
Training Data
The same dataset is used as in Rae et al. (2021). DiÔ¨Äerences in sampling are shown in Table A1.
Quantitative Analyses
Unitary Results Section 4.2 gives a detailed description of our analysis. Main
take-aways include:
‚Ä¢ Our model is capable of outputting toxic language as
measured by the PerspectiveAPI. This is particularly
true when the model is prompted with toxic prompts.
‚Ä¢ Gender: Our model emulates stereotypes found in our
dataset, with occupations such as ‚Äúdietician‚Äù and ‚Äúre-
ceptionist‚Äùbeingmoreassociatedwithwomenand‚Äúcar-
penter‚Äù and ‚ÄúsheriÔ¨Ä‚Äù being more associated with men.
‚Ä¢ Race/religion/country sentiment: Prompting our
model to discuss some groups leads to sentences with
lower or higher sentiment, likely reÔ¨Çecting text in our
dataset.
33
Intersectional Results We did not investigate intersectional biases.
Ethical Considerations
Data The data is the same as described in Rae et al. (2021).
Human Life The model is not intended to inform decisions about matters
central to human life or Ô¨Çourishing.
Mitigations We considered Ô¨Åltering the dataset to remove toxic content
but decided against it due to the observation that this can
introduce new biases as studied by Welbl et al. (2021). More
workisneededonmitigationapproachestotoxiccontentand
other types of risks associated with language models, such
as those discussed in Weidinger et al. (2021).
Risks and Harms Thedataiscollectedfromtheinternet, andthusundoubtedly
there is toxic/biased content in our training dataset. Fur-
thermore, it is likely that personal information is also in the
dataset that has been used to train our models. We defer to
the more detailed discussion in Weidinger et al. (2021).
Use Cases Especially fraught use cases include the generation of fac-
tually incorrect information with the intent of distributing
it or using the model to generate racist, sexist or otherwise
toxic text with harmful intent. Many more use cases that
could cause harm exist. Such applications to malicious use
are discussed in detail in Weidinger et al. (2021).
Table A8j Chinchilla model card.We follow the framework presented in Mitchell et al. (2019).
J. List of trained models
In Table A9 we list the model size and conÔ¨Åguration of all models used in this study. Many models
have been trained multiple times, for a diÔ¨Äerent number of training steps.
34
Task Chinchilla Gopher Task Chinchilla Gopher
hyperbaton 54.2 51.7 movie_dialog_same_or_diÔ¨Ä 54.5 50.7
causal_judgment 57.4 50.8 winowhy 62.5 56.7
formal_fallacies_syllogisms_neg 52.1 50.7 movie_recommendation 75.6 50.5
crash_blossom 47.6 63.6 moral_permissibility 57.3 55.1
discourse_marker_prediction 13.1 11.7 strategyqa 68.3 61.0
general_knowledge_json 94.3 93.9 nonsense_words_grammar 78.0 61.4
sports_understanding 71.0 54.9 metaphor_boolean 93.1 59.3
implicit_relations 49.4 36.4 navigate 52.6 51.1
penguins_in_a_table 48.7 40.6 presuppositions_as_nli 49.9 34.0
intent_recognition 92.8 88.7 temporal_sequences 32.0 19.0
reasoning_about_colored_objects 59.7 49.2 question_selection 52.6 41.4
logic_grid_puzzle 44.0 35.1 logical_fallacy_detection 72.1 58.9
timedial 68.8 50.9 physical_intuition 79.0 59.7
epistemic_reasoning 60.6 56.4 physics_mc 65.5 50.9
ruin_names 47.1 38.6 identify_odd_metaphor 68.8 38.6
hindu_knowledge 91.4 80.0 understanding_fables 60.3 39.6
misconceptions 65.3 61.7 logical_sequence 64.1 36.4
implicatures 75.0 62.0 mathematical_induction 47.3 57.6
disambiguation_q 54.7 45.5 fantasy_reasoning 69.0 64.1
known_unknowns 65.2 63.6 SNARKS 58.6 48.3
dark_humor_detection 66.2 83.1 crass_ai 75.0 56.8
analogical_similarity 38.1 17.2 entailed_polarity 94.0 89.5
sentence_ambiguity 71.7 69.1 irony_identiÔ¨Åcation 73.0 69.7
riddle_sense 85.7 68.2 evaluating_info_essentiality 17.6 16.7
date_understanding 52.3 44.1 phrase_relatedness 94.0 81.8
analytic_entailment 67.1 53.0 novel_concepts 65.6 59.1
odd_one_out 70.9 32.5 empirical_judgments 67.7 52.5
logical_args 56.2 59.1 Ô¨Ågure_of_speech_detection 63.3 52.7
alignment_questionnaire 91.3 79.2 english_proverbs 82.4 57.6
similarities_abstraction 87.0 81.8 Human_organs_senses_mcc 85.7 84.8
anachronisms 69.1 56.4 gre_reading_comprehension 53.1 27.3
Table A7j Chinchilla BIG-bench results.For each subset of BIG-bench (BIG-bench collaboration,
2021), we showChinchilla and Gopher‚Äôs accuracy.
35
Parameters (million) d_model Ô¨Äw_size kv_size n_heads n_layers
44 512 2048 64 8 8
57 576 2304 64 9 9
74 640 2560 64 10 10
90 640 2560 64 10 13
106 640 2560 64 10 16
117 768 3072 64 12 12
140 768 3072 64 12 15
163 768 3072 64 12 18
175 896 3584 64 14 14
196 896 3584 64 14 16
217 896 3584 64 14 18
251 1024 4096 64 16 16
278 1024 4096 64 16 18
306 1024 4096 64 16 20
425 1280 5120 128 10 18
489 1280 5120 128 10 21
509 1408 5632 128 11 18
552 1280 5120 128 10 24
587 1408 5632 128 11 21
632 1536 6144 128 12 19
664 1408 5632 128 11 24
724 1536 6144 128 12 22
816 1536 6144 128 12 25
893 1792 7168 128 14 20
1,018 1792 7168 128 14 23
1,143 1792 7168 128 14 26
1,266 2048 8192 128 16 22
1,424 2176 8704 128 17 22
1,429 2048 8192 128 16 25
1,593 2048 8192 128 16 28
1,609 2176 8704 128 17 25
1,731 2304 9216 128 18 24
1,794 2176 8704 128 17 28
2,007 2304 9216 128 18 28
2,283 2304 9216 128 18 32
2,298 2560 10240 128 20 26
2,639 2560 10240 128 20 30
2,980 2560 10240 128 20 34
3,530 2688 10752 128 22 36
3,802 2816 11264 128 22 36
4,084 2944 11776 128 22 36
4,516 3072 12288 128 24 36
6,796 3584 14336 128 28 40
9,293 4096 16384 128 32 42
11,452 4352 17408 128 32 47
12,295 4608 18432 128 36 44
12,569 4608 18432 128 32 47
13,735 4864 19456 128 32 47
14,940 4992 19968 128 32 49
16,183 5120 20480 128 40 47
Table A9j All models.We list the hyperparameters and size of all models trained as part of this work.
Many shown models have been trained with multiple learning rate schedules/number of training
tokens.
36


=== Ollama-paper.pdf ===
LLaMA: Open and EfÔ¨Åcient Foundation Language Models
Hugo Touvron‚àó, Thibaut Lavril‚àó, Gautier Izacard‚àó, Xavier Martinet
Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi√®re, Naman Goyal
Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin
Edouard Grave‚àó, Guillaume Lample‚àó
Meta AI
Abstract
We introduce LLaMA, a collection of founda-
tion language models ranging from 7B to 65B
parameters. We train our models on trillions
of tokens, and show that it is possible to train
state-of-the-art models using publicly avail-
able datasets exclusively, without resorting
to proprietary and inaccessible datasets. In
particular, LLaMA-13B outperforms GPT-3
(175B) on most benchmarks, and LLaMA-
65B is competitive with the best models,
Chinchilla-70B and PaLM-540B. We release
all our models to the research community1.
1 Introduction
Large Languages Models (LLMs) trained on mas-
sive corpora of texts have shown their ability to per-
form new tasks from textual instructions or from a
few examples (Brown et al., 2020). These few-shot
properties Ô¨Årst appeared when scaling models to a
sufÔ¨Åcient size (Kaplan et al., 2020), resulting in a
line of work that focuses on further scaling these
models (Chowdhery et al., 2022; Rae et al., 2021).
These efforts are based on the assumption that
more parameters will lead to better performance.
However, recent work from Hoffmann et al. (2022)
shows that, for a given compute budget, the best
performances are not achieved by the largest mod-
els, but by smaller models trained on more data.
The objective of the scaling laws from Hoff-
mann et al. (2022) is to determine how to best
scale the dataset and model sizes for a particular
training compute budget. However, this objective
disregards the inference budget, which becomes
critical when serving a language model at scale.
In this context, given a target level of performance,
the preferred model is not the fastest to train but the
fastest at inference, and although it may be cheaper
to train a large model to reach a certain level of
‚àó Equal contribution. Correspondence: {htouvron,
thibautlav,gizacard,egrave,glample}@meta.com
1https://github.com/facebookresearch/llama
performance, a smaller one trained longer will
ultimately be cheaper at inference. For instance,
although Hoffmann et al. (2022) recommends
training a 10B model on 200B tokens, we Ô¨Ånd
that the performance of a 7B model continues to
improve even after 1T tokens.
The focus of this work is to train a series of
language models that achieve the best possible per-
formance at various inference budgets, by training
on more tokens than what is typically used. The
resulting models, called LLaMA, ranges from 7B
to 65B parameters with competitive performance
compared to the best existing LLMs. For instance,
LLaMA-13B outperforms GPT-3 on most bench-
marks, despite being 10√ó smaller. We believe that
this model will help democratize the access and
study of LLMs, since it can be run on a single GPU.
At the higher-end of the scale, our 65B-parameter
model is also competitive with the best large lan-
guage models such as Chinchilla or PaLM-540B.
Unlike Chinchilla, PaLM, or GPT-3, we only
use publicly available data, making our work com-
patible with open-sourcing, while most existing
models rely on data which is either not publicly
available or undocumented (e.g. ‚ÄúBooks ‚Äì 2TB‚Äù or
‚ÄúSocial media conversations‚Äù). There exist some
exceptions, notably OPT (Zhang et al., 2022),
GPT-NeoX (Black et al., 2022), BLOOM (Scao
et al., 2022) and GLM (Zeng et al., 2022), but none
that are competitive with PaLM-62B or Chinchilla.
In the rest of this paper, we present an overview
of the modiÔ¨Åcations we made to the transformer
architecture (Vaswani et al., 2017), as well as our
training method. We then report the performance of
our models and compare with others LLMs on a set
of standard benchmarks. Finally, we expose some
of the biases and toxicity encoded in our models,
using some of the most recent benchmarks from
the responsible AI community.
arXiv:2302.13971v1  [cs.CL]  27 Feb 2023
2 Approach
Our training approach is similar to the methods
described in previous work (Brown et al., 2020;
Chowdhery et al., 2022), and is inspired by the
Chinchilla scaling laws (Hoffmann et al., 2022).
We train large transformers on a large quantity of
textual data using a standard optimizer.
2.1 Pre-training Data
Our training dataset is a mixture of several sources,
reported in Table 1, that cover a diverse set of do-
mains. For the most part, we reuse data sources
that have been leveraged to train other LLMs, with
the restriction of only using data that is publicly
available, and compatible with open sourcing. This
leads to the following mixture of data and the per-
centage they represent in the training set:
English CommonCrawl [67%]. We preprocess
Ô¨Åve CommonCrawl dumps, ranging from 2017
to 2020, with the CCNet pipeline (Wenzek et al.,
2020). This process deduplicates the data at the
line level, performs language identiÔ¨Åcation with
a fastText linear classiÔ¨Åer to remove non-English
pages and Ô¨Ålters low quality content with an n-
gram language model. In addition, we trained a
linear model to classify pages used as references
in Wikipedia v.s. randomly sampled pages, and
discarded pages not classiÔ¨Åed as references.
C4 [15%]. During exploratory experiments, we
observed that using diverse pre-processed Com-
monCrawl datasets improves performance. We thus
included the publicly available C4 dataset (Raffel
et al., 2020) in our data. The preprocessing of C4
also contains deduplication and language identiÔ¨Å-
cation steps: the main difference with CCNet is
the quality Ô¨Åltering, which mostly relies on heuris-
tics such as presence of punctuation marks or the
number of words and sentences in a webpage.
Github [4.5%]. We use the public GitHub
dataset available on Google BigQuery. We only
kept projects that are distributed under the Apache,
BSD and MIT licenses. Additionally, we Ô¨Åltered
low quality Ô¨Åles with heuristics based on the line
length or proportion of alphanumeric characters,
and removed boilerplate, such as headers, with reg-
ular expressions. Finally, we deduplicate the result-
ing dataset at the Ô¨Åle level, with exact matches.
Wikipedia [4.5%]. We add Wikipedia dumps
from the June-August 2022 period, covering 20
Dataset Sampling prop. Epochs Disk size
CommonCrawl 67.0% 1.10 3.3 TB
C4 15.0% 1.06 783 GB
Github 4.5% 0.64 328 GB
Wikipedia 4.5% 2.45 83 GB
Books 4.5% 2.23 85 GB
ArXiv 2.5% 1.06 92 GB
StackExchange 2.0% 1.03 78 GB
Table 1: Pre-training data. Data mixtures used for pre-
training, for each subset we list the sampling propor-
tion, number of epochs performed on the subset when
training on 1.4T tokens, and disk size. The pre-training
runs on 1T tokens have the same sampling proportion.
languages, which use either the Latin or Cyrillic
scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it,
nl, pl, pt, ro, ru, sl, sr, sv, uk. We process the
data to remove hyperlinks, comments and other
formatting boilerplate.
Gutenberg and Books3 [4.5%]. We include
two book corpora in our training dataset: the Guten-
berg Project, which contains books that are in the
public domain, and the Books3 section of TheP-
ile (Gao et al., 2020), a publicly available dataset
for training large language models. We perform
deduplication at the book level, removing books
with more than 90% content overlap.
ArXiv [2.5%]. We process arXiv Latex Ô¨Åles
to add scientiÔ¨Åc data to our dataset. Following
Lewkowycz et al. (2022), we removed everything
before the Ô¨Årst section, as well as the bibliography.
We also removed the comments from the .tex Ô¨Åles,
and inline-expanded deÔ¨Ånitions and macros written
by users to increase consistency across papers.
Stack Exchange [2%]. We include a dump of
Stack Exchange, a website of high quality ques-
tions and answers that covers a diverse set of do-
mains, ranging from computer science to chemistry.
We kept the data from the 28 largest websites, re-
moved the HTML tags from text and sorted the
answers by score (from highest to lowest).
Tokenizer. We tokenize the data with the byte-
pair encoding (BPE) algorithm (Sennrich et al.,
2015), using the implementation from Sentence-
Piece (Kudo and Richardson, 2018). Notably, we
split all numbers into individual digits, and fallback
to bytes to decompose unknown UTF-8 characters.
params dimension n heads n layers learning rate batch size n tokens
6.7B 4096 32 32 3.0e‚àí4 4M 1.0T
13.0B 5120 40 40 3.0e‚àí4 4M 1.0T
32.5B 6656 52 60 1.5e‚àí4 4M 1.4T
65.2B 8192 64 80 1.5e‚àí4 4M 1.4T
Table 2: Model sizes, architectures, and optimization hyper-parameters.
Overall, our entire training dataset contains
roughly 1.4T tokens after tokenization. For most of
our training data, each token is used only once dur-
ing training, with the exception of the Wikipedia
and Books domains, over which we perform ap-
proximately two epochs.
2.2 Architecture
Following recent work on large language models,
our network is based on the transformer architec-
ture (Vaswani et al., 2017). We leverage various
improvements that were subsequently proposed,
and used in different models such as PaLM. Here
are the main difference with the original architec-
ture, and where we were found the inspiration for
this change (in bracket):
Pre-normalization [GPT3]. To improve the
training stability, we normalize the input of each
transformer sub-layer, instead of normalizing the
output. We use the RMSNorm normalizing func-
tion, introduced by Zhang and Sennrich (2019).
SwiGLU activation function [PaLM]. We re-
place the ReLU non-linearity by the SwiGLU ac-
tivation function, introduced by Shazeer (2020) to
improve the performance. We use a dimension of
2
3 4d instead of 4d as in PaLM.
Rotary Embeddings [GPTNeo]. We remove the
absolute positional embeddings, and instead, add
rotary positional embeddings (RoPE), introduced
by Su et al. (2021), at each layer of the network.
The details of the hyper-parameters for our dif-
ferent models are given in Table 2.
2.3 Optimizer
Our models are trained using the AdamW opti-
mizer (Loshchilov and Hutter, 2017), with the fol-
lowing hyper-parameters: Œ≤1 = 0.9,Œ≤ 2 = 0.95.
We use a cosine learning rate schedule, such that
the Ô¨Ånal learning rate is equal to 10% of the maxi-
mal learning rate. We use a weight decay of0.1 and
gradient clipping of 1.0. We use 2, 000 warmup
0 200 400 600 800 1000 1200 1400
Billion of tokens
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2Training loss
LLaMA 7B
LLaMA 13B
LLaMA 33B
LLaMA 65B
Figure 1: Training loss over train tokens for the 7B,
13B, 33B, and 65 models. LLaMA-33B and LLaMA-
65B were trained on 1.4T tokens. The smaller models
were trained on 1.0T tokens. All models are trained
with a batch size of 4M tokens.
steps, and vary the learning rate and batch size with
the size of the model (see Table 2 for details).
2.4 EfÔ¨Åcient implementation
We make several optimizations to improve the train-
ing speed of our models. First, we use an efÔ¨Åcient
implementation of the causal multi-head attention
to reduce memory usage and runtime. This imple-
mentation, available in the xformers library,2 is
inspired by Rabe and Staats (2021) and uses the
backward from Dao et al. (2022). This is achieved
by not storing the attention weights and not com-
puting the key/query scores that are masked due to
the causal nature of the language modeling task.
To further improve training efÔ¨Åciency, we re-
duced the amount of activations that are recom-
puted during the backward pass with checkpoint-
ing. More precisely, we save the activations that
are expensive to compute, such as the outputs of
linear layers. This is achieved by manually imple-
menting the backward function for the transformer
layers, instead of relying on the PyTorch autograd.
To fully beneÔ¨Åt from this optimization, we need to
2https://github.com/facebookresearch/xformers
BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA
GPT-3 175B 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6
Gopher 280B 79.3 81.8 50.6 79.2 70.1 - - -
Chinchilla 70B 83.7 81.8 51.3 80.8 74.9 - - -
PaLM 62B 84.8 80.5 - 79.7 77.0 75.2 52.5 50.4
PaLM-cont 62B 83.9 81.4 - 80.6 77.0 - - -
PaLM 540B 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4
LLaMA
7B 76.5 79.8 48.9 76.1 70.1 72.8 47.6 57.2
13B 78.1 80.1 50.4 79.2 73.0 74.8 52.7 56.4
33B 83.1 82.3 50.4 82.8 76.0 80.0 57.8 58.6
65B 85.3 82.8 52.3 84.2 77.0 78.9 56.0 60.2
Table 3: Zero-shot performance on Common Sense Reasoning tasks.
reduce the memory usage of the model by using
model and sequence parallelism, as described by
Korthikanti et al. (2022). Moreover, we also over-
lap the computation of activations and the commu-
nication between GPUs over the network (due to
all_reduce operations) as much as possible.
When training a 65B-parameter model, our code
processes around 380 tokens/sec/GPU on 2048
A100 GPU with 80GB of RAM. This means that
training over our dataset containing 1.4T tokens
takes approximately 21 days.
3 Main results
Following previous work (Brown et al., 2020), we
consider zero-shot and few-shot tasks, and report
results on a total of 20 benchmarks:
‚Ä¢ Zero-shot. We provide a textual description
of the task and a test example. The model
either provides an answer using open-ended
generation, or ranks the proposed answers.
‚Ä¢ Few-shot. We provide a few examples of the
task (between 1 and 64) and a test example.
The model takes this text as input and gener-
ates the answer or ranks different options.
We compare LLaMA with other foundation mod-
els, namely the non-publicly available language
models GPT-3 (Brown et al., 2020), Gopher (Rae
et al., 2021), Chinchilla (Hoffmann et al., 2022)
and PaLM (Chowdhery et al., 2022), as well as
the open-sourced OPT models (Zhang et al., 2022),
GPT-J (Wang and Komatsuzaki, 2021), and GPT-
Neo (Black et al., 2022). In Section 4, we also
brieÔ¨Çy compare LLaMA with instruction-tuned
models such as OPT-IML (Iyer et al., 2022) and
Flan-PaLM (Chung et al., 2022).
We evaluate LLaMA on free-form generation
tasks and multiple choice tasks. In the multiple
choice tasks, the objective is to select the most
appropriate completion among a set of given op-
tions, based on a provided context. We select the
completion with the highest likelihood given the
provided context. We follow Gao et al. (2021)
and use the likelihood normalized by the number
of characters in the completion, except for certain
datasets (OpenBookQA, BoolQ), for which we fol-
low Brown et al. (2020), and select a completion
based on the likelihood normalized by the likeli-
hood of the completion given ‚ÄúAnswer:‚Äù as context:
P (completion|context)/P (completion|‚ÄúAnswer:‚Äù).
0-shot 1-shot 5-shot 64-shot
GPT-3 175B 14.6 23.0 - 29.9
Gopher 280B 10.1 - 24.5 28.2
Chinchilla 70B 16.6 - 31.5 35.5
PaLM
8B 8.4 10.6 - 14.6
62B 18.1 26.5 - 27.6
540B 21.2 29.3 - 39.6
LLaMA
7B 16.8 18.7 22.0 26.1
13B 20.1 23.4 28.1 31.9
33B 24.9 28.3 32.9 36.0
65B 23.8 31.0 35.0 39.9
Table 4: NaturalQuestions. Exact match performance.
3.1 Common Sense Reasoning
We consider eight standard common sense rea-
soning benchmarks: BoolQ (Clark et al., 2019),
PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),
HellaSwag (Zellers et al., 2019), WinoGrande (Sak-
aguchi et al., 2021), ARC easy and challenge (Clark
et al., 2018) and OpenBookQA (Mihaylov et al.,
2018). These datasets include Cloze and Winograd
style tasks, as well as multiple choice question an-
swering. We evaluate in the zero-shot setting as
done in the language modeling community.
In Table 3, we compare with existing models
of various sizes and report numbers from the cor-
responding papers. First, LLaMA-65B outper-
forms Chinchilla-70B on all reported benchmarks
but BoolQ. Similarly, this model surpasses PaLM-
540B everywhere but on BoolQ and WinoGrande.
LLaMA-13B model also outperforms GPT-3 on
most benchmarks despite being 10√ó smaller.
3.2 Closed-book Question Answering
We compare LLaMA to existing large language
models on two closed-book question answering
benchmarks: Natural Questions (Kwiatkowski
et al., 2019) and TriviaQA (Joshi et al., 2017). For
both benchmarks, we report exact match perfor-
mance in a closed book setting, i.e., where the mod-
els do not have access to documents that contain
evidence to answer the question. In Table 4, we
report performance on NaturalQuestions, and in Ta-
ble 5, we report on TriviaQA. On both benchmarks,
LLaMA-65B achieve state-of-the-arts performance
in the zero-shot and few-shot settings. More im-
portantly, the LLaMA-13B is also competitive on
these benchmarks with GPT-3 and Chinchilla, de-
spite being 5-10√ó smaller. This model runs on a
single V100 GPU during inference.
0-shot 1-shot 5-shot 64-shot
Gopher 280B 43.5 - 57.0 57.2
Chinchilla 70B 55.4 - 64.1 64.6
LLaMA
7B 50.0 53.4 56.3 57.6
13B 56.6 60.5 63.1 64.0
33B 65.1 67.9 69.9 70.4
65B 68.2 71.6 72.6 73.0
Table 5: TriviaQA. Zero-shot and few-shot exact
match performance on the Ô¨Åltered dev set.
3.3 Reading Comprehension
We evaluate our models on the RACE reading com-
prehension benchmark (Lai et al., 2017). This
dataset was collected from English reading com-
prehension exams designed for middle and high
RACE-middle RACE-high
GPT-3 175B 58.4 45.5
PaLM
8B 57.9 42.3
62B 64.3 47.5
540B 68.1 49.1
LLaMA
7B 61.1 46.9
13B 61.6 47.2
33B 64.1 48.3
65B 67.9 51.6
Table 6: Reading Comprehension. Zero-shot accu-
racy.
school Chinese students. We follow the evaluation
setup from Brown et al. (2020) and report results
in Table 6. On these benchmarks, LLaMA-65B is
competitive with PaLM-540B, and, LLaMA-13B
outperforms GPT-3 by a few percents.
3.4 Mathematical reasoning
We evaluate our models on two mathematical rea-
soning benchmarks: MATH (Hendrycks et al.,
2021) and GSM8k (Cobbe et al., 2021). MATH
is a dataset of 12K middle school and high school
mathematics problems written in LaTeX. GSM8k
is a set of middle school mathematical problems.
In Table 7, we compare with PaLM and Min-
erva (Lewkowycz et al., 2022). Minerva is a series
of PaLM models Ô¨Ånetuned on 38.5B tokens ex-
tracted from ArXiv and Math Web Pages, while
neither PaLM or LLaMA are Ô¨Ånetuned on mathe-
matical data. The numbers for PaLM and Minerva
are taken from Lewkowycz et al. (2022), and we
compare with and without maj1@k. maj1@k de-
notes evaluations where we generatek samples for
each problem and perform a majority voting (Wang
et al., 2022). On GSM8k, we observe that LLaMA-
65B outperforms Minerva-62B, although it has not
been Ô¨Åne-tuned on mathematical data.
3.5 Code generation
We evaluate the ability of our models to write
code from a natural language description on two
benchmarks: HumanEval (Chen et al., 2021) and
MBPP (Austin et al., 2021). For both tasks, the
model receives a description of the program in a
few sentences, as well as a few input-output ex-
amples. In HumanEval, it also receives a function
signature, and the prompt is formatted as natural
code with the textual description and tests in a
MATH +maj1@k GSM8k +maj1@k
PaLM
8B 1.5 - 4.1 -
62B 4.4 - 33.0 -
540B 8.8 - 56.5 -
Minerva
8B 14.1 25.4 16.2 28.4
62B 27.6 43.4 52.4 68.5
540B 33.6 50.3 68.5 78.5
LLaMA
7B 2.9 6.9 11.0 18.1
13B 3.9 8.8 17.8 29.3
33B 7.1 15.2 35.6 53.1
65B 10.6 20.5 50.9 69.7
Table 7: Model performance on quantitative reason-
ing datasets. For majority voting, we use the same
setup as Minerva, with k = 256 samples for MATH
andk = 100 for GSM8k (Minerva 540B uses k = 64
for MATH and andk = 40 for GSM8k). LLaMA-65B
outperforms Minerva 62B on GSM8k, although it has
not been Ô¨Åne-tuned on mathematical data.
docstring. The model needs to generate a Python
program that Ô¨Åts the description and satisÔ¨Åes the
test cases. In Table 8, we compare the pass@1
scores of our models with existing language mod-
els that have not been Ô¨Ånetuned on code, namely
PaLM and LaMDA (Thoppilan et al., 2022). PaLM
and LLaMA were trained on datasets that contain
a similar number of code tokens.
As show in Table 8, for a similar number
of parameters, LLaMA outperforms other gen-
eral models such as LaMDA and PaLM, which
are not trained or Ô¨Ånetuned speciÔ¨Åcally for code.
LLaMA with 13B parameters and more outper-
forms LaMDA 137B on both HumanEval and
MBPP. LLaMA 65B also outperforms PaLM 62B,
even when it is trained longer. The pass@1 results
reported in this table were obtained by sampling
with temperature 0.1. The pass@100 and pass@80
metrics were obtained with temperature 0.8. We
use the same method as Chen et al. (2021) to obtain
unbiased estimates of the pass@k.
It is possible to improve the performance on code
by Ô¨Ånetuning on code-speciÔ¨Åc tokens. For instance,
PaLM-Coder (Chowdhery et al., 2022) increases
the pass@1 score of PaLM on HumanEval from
26.2% for PaLM to 36%. Other models trained
speciÔ¨Åcally for code also perform better than gen-
eral models on these tasks (Chen et al., 2021; Ni-
jkamp et al., 2022; Fried et al., 2022). Finetuning
on code tokens is beyond the scope of this paper.
Params HumanEval MBPP
pass@ @1 @100 @1 @80
LaMDA 137B 14.0 47.3 14.8 62.4
PaLM 8B 3.6 ‚àó 18.7‚àó 5.0‚àó 35.7‚àó
PaLM 62B 15.9 46.3 ‚àó 21.4 63.2 ‚àó
PaLM-cont 62B 23.7 - 31.2 -
PaLM 540B 26.2 76.2 36.8 75.0
LLaMA
7B 10.5 36.5 17.7 56.2
13B 15.8 52.5 22.0 64.0
33B 21.7 70.7 30.2 73.4
65B 23.7 79.3 37.7 76.8
Table 8: Model performance for code generation.
We report the pass@ score on HumanEval and MBPP.
HumanEval generations are done in zero-shot and
MBBP with 3-shot prompts similar to Austin et al.
(2021). The values marked with ‚àó are read from Ô¨Ågures
in Chowdhery et al. (2022).
3.6 Massive Multitask Language
Understanding
The massive multitask language understanding
benchmark, or MMLU, introduced by Hendrycks
et al. (2020) consists of multiple choice questions
covering various domains of knowledge, includ-
ing humanities, STEM and social sciences. We
evaluate our models in the 5-shot setting, using the
examples provided by the benchmark, and report
results in Table 9. On this benchmark, we observe
that the LLaMA-65B is behind both Chinchilla-
70B and PaLM-540B by a few percent in average,
and across most domains. A potential explanation
is that we have used a limited amount of books
and academic papers in our pre-training data, i.e.,
ArXiv, Gutenberg and Books3, that sums up to only
177GB, while these models were trained on up to
2TB of books. This large quantity of books used
by Gopher, Chinchilla and PaLM may also explain
why Gopher outperforms GPT-3 on this benchmark,
while it is comparable on other benchmarks.
3.7 Evolution of performance during training
During training, we tracked the performance of our
models on a few question answering and common
sense benchmarks, and report them in Figure 2.
On most benchmarks, the performance improves
steadily, and correlates with the training perplexity
of the model (see Figure 1). The exceptions are
SIQA and WinoGrande. Most notably, on SIQA,
we observe a lot of variance in performance,
Humanities STEM Social Sciences Other Average
GPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6
GPT-3 175B 40.8 36.7 50.4 48.8 43.9
Gopher 280B 56.2 47.4 71.9 66.1 60.0
Chinchilla 70B 63.6 54.9 79.3 73.9 67.5
PaLM
8B 25.6 23.8 24.1 27.8 25.4
62B 59.5 41.9 62.7 55.8 53.7
540B 77.0 55.6 81.0 69.6 69.3
LLaMA
7B 34.0 30.5 38.3 38.1 35.1
13B 45.0 35.8 53.8 53.3 46.9
33B 55.8 46.0 66.7 63.4 57.8
65B 61.8 51.7 72.9 67.4 63.4
Table 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.
that may indicate that this benchmark is not
reliable. On WinoGrande, the performance does
not correlate as well with training perplexity:
the LLaMA-33B and LLaMA-65B have similar
performance during the training.
4 Instruction Finetuning
In this section, we show that brieÔ¨Çy Ô¨Ånetuning on
instructions data rapidly leads to improvements
on MMLU. Although the non-Ô¨Ånetuned version
of LLaMA-65B is already able to follow basic in-
structions, we observe that a very small amount of
Ô¨Ånetuning improves the performance on MMLU,
and further improves the ability of the model to
follow instructions. Since this is not the focus of
this paper, we only conducted a single experiment
following the same protocol as Chung et al. (2022)
to train an instruct model, LLaMA-I.
OPT 30B 26.1
GLM 120B 44.8
PaLM 62B 55.1
PaLM-cont 62B 62.8
Chinchilla 70B 67.5
LLaMA 65B 63.4
OPT-IML-Max 30B 43.2
Flan-T5-XXL 11B 55.1
Flan-PaLM 62B 59.6
Flan-PaLM-cont 62B 66.1
LLaMA-I 65B 68.9
Table 10: Instruction Ô¨Ånetuning ‚Äì MMLU (5-shot).
Comparison of models of moderate size with and with-
out instruction Ô¨Ånetuning on MMLU.
In Table 10, we report the results of our instruct
model LLaMA-I on MMLU and compare with ex-
isting instruction Ô¨Ånetuned models of moderate
sizes, namely, OPT-IML (Iyer et al., 2022) and the
Flan-PaLM series (Chung et al., 2022). All the re-
ported numbers are from the corresponding papers.
Despite the simplicity of the instruction Ô¨Ånetuning
approach used here, we reach 68.9% on MMLU.
LLaMA-I (65B) outperforms on MMLU existing
instruction Ô¨Ånetuned models of moderate sizes, but
are still far from the state-of-the-art, that is 77.4
for GPT code-davinci-002 on MMLU (numbers
taken from Iyer et al. (2022)). The details of the
performance on MMLU on the 57 tasks can be
found in Table 16 of the appendix.
5 Bias, Toxicity and Misinformation
Large language models have been showed to re-
produce and amplify biases that are existing in
the training data (Sheng et al., 2019; Kurita et al.,
2019), and to generate toxic or offensive con-
tent (Gehman et al., 2020). As our training dataset
contains a large proportion of data from the Web,
we believe that it is crucial to determine the po-
tential for our models to generate such content.
To understand the potential harm of LLaMA-65B,
we evaluate on different benchmarks that measure
toxic content production and stereotypes detection.
While we have selected some of the standard bench-
marks that are used by the language model com-
munity to indicate some of the issues with these
models, these evaluations are not sufÔ¨Åcient to fully
understand the risks associated with these models.
0 250 500 750 1000 1250 1500
20
30
40
50
60
70Accuracy
TriviaQA
0 250 500 750 1000 1250 1500
50
55
60
65
70
75
80
85
HellaSwag
0 250 500 750 1000 1250 1500
0
5
10
15
20
25
30
35
NaturalQuestions
0 250 500 750 1000 1250 1500
Billion of tokens
40
42
44
46
48
50
52Accuracy
SIQA
0 250 500 750 1000 1250 1500
Billion of tokens
50
55
60
65
70
75
80
WinoGrande
0 250 500 750 1000 1250 1500
Billion of tokens
65.0
67.5
70.0
72.5
75.0
77.5
80.0
82.5
PIQA
LLaMA 7B
LLaMA 13B
LLaMA 33B
LLaMA 65B
Chinchilla
Figure 2: Evolution of performance on question answering and common sense reasoning during training.
5.1 RealToxicityPrompts
Language models can generate toxic language, e.g.,
insults, hate speech or threats. There is a very large
range of toxic content that a model can generate,
making a thorough evaluation challenging. Several
recent work (Zhang et al., 2022; Hoffmann et al.,
2022) have considered the RealToxicityPrompts
benchmark (Gehman et al., 2020) as an indicator
of how toxic is their model. RealToxicityPrompts
consists of about 100k prompts that the model must
complete; then a toxicity score is automatically
evaluated by making a request to PerspectiveAPI 3.
We do not have control over the pipeline used by
the third-party PerspectiveAPI, making comparison
with previous models difÔ¨Åcult.
For each of the 100k prompts, we greedily gen-
erate with our models, and measure their toxic-
ity score. The score per prompt ranges from 0
(non-toxic) to 1 (toxic). In Table 11, we report our
averaged score on basic and respectful prompt cat-
egories of RealToxicityPrompts. These scores are
‚Äúcomparable‚Äù with what we observe in the litera-
ture (e.g., 0.087 for Chinchilla) but the method-
ologies differ between these work and ours (in
terms of sampling strategy, number of prompts and
time of API). We observe that toxicity increases
3https://perspectiveapi.com/
Basic Respectful
LLaMA
7B 0.106 0.081
13B 0.104 0.095
33B 0.107 0.087
65B 0.128 0.141
Table 11: RealToxicityPrompts. We run a greedy de-
coder on the 100k prompts from this benchmark. The
‚Äúrespectful‚Äù versions are prompts starting with ‚ÄúCom-
plete the following sentence in a polite, respectful, and
unbiased manner:‚Äù, and ‚ÄúBasic‚Äù is without it. Scores
were obtained using the PerplexityAPI, with higher
score indicating more toxic generations.
with the size of the model, especially for Respect-
ful prompts. This was also observed in previous
work (Zhang et al., 2022), with the notable excep-
tion of Hoffmann et al. (2022) where they do not
see a difference between Chinchilla and Gopher,
despite different sizes. This could be explained by
the fact that the larger model, Gopher, has worse
performance than Chinchilla, suggesting that the
relation between toxicity and model size may only
apply within a model family.
LLaMA GPT3 OPT
Gender 70.6 62.6 65.7
Religion 79.0 73.3 68.6
Race/Color 57.0 64.7 68.6
Sexual orientation 81.0 76.2 78.6
Age 70.1 64.4 67.8
Nationality 64.2 61.6 62.9
Disability 66.7 76.7 76.7
Physical appearance 77.8 74.6 76.2
Socioeconomic status 71.5 73.8 76.2
Average 66.6 67.2 69.5
Table 12: CrowS-Pairs. We compare the level of bi-
ases contained in LLaMA-65B with OPT-175B and
GPT3-175B. Higher score indicates higher bias.
5.2 CrowS-Pairs
We evaluate the biases in our model on the CrowS-
Pairs (Nangia et al., 2020). This dataset allows to
measure biases in 9 categories: gender, religion,
race/color, sexual orientation, age, nationality, dis-
ability, physical appearance and socioeconomic sta-
tus. Each example is composed of a stereotype and
an anti-stereotype, we measure the model prefer-
ence for the stereotypical sentence using the per-
plexity of both sentences in a zero-shot setting.
Higher scores thus indicate higher bias. We com-
pare with GPT-3 and OPT-175B in Table 12.
LLaMA compares slightly favorably to both
models on average. Our model is particularly bi-
ased in the religion category (+10% compared to
OPT-175B), followed by age and gender. We ex-
pect these biases to come from CommonCrawl de-
spite multiple Ô¨Åltering steps.
5.3 WinoGender
To further investigate the biases of our model on
the gender category, we look at the WinoGender
benchmark (Rudinger et al., 2018), a co-reference
resolution dataset. WinoGender is made of Wino-
grad schema, and biases are evaluated by determin-
ing if a model co-reference resolution performance
is impacted by the gender of the pronoun.
More precisely, each sentence has three men-
tions: an ‚Äúoccupation‚Äù, a ‚Äúparticipant‚Äù, and a
‚Äúpronoun‚Äù where the pronoun is co-referencing
either the occupation or participant. We prompt
the model to determine the co-reference relation
and measure if it does so correctly according to
the context of the sentence. The goal is to reveal
if societal biases associated with occupations
have been captured by the model. For example,
a sentence in the WinoGender dataset is ‚ÄúThe
nurse notiÔ¨Åed the patient that his shift would be
ending in an hour.‚Äù, which is followed by ‚ÄòHis‚Äô
refers to . We then compare the perplexity of the
continuations the nurse and the patient to per-
form co-reference resolution with the model. We
evaluate the performance when using 3 pronouns:
‚Äúher/her/she‚Äù, ‚Äúhis/him/he‚Äù and ‚Äútheir/them/some-
one‚Äù (the different choices corresponding to the
grammatical function of the pronoun.
In Table 13, we report the co-reference scores
for the three different pronouns contained in the
dataset. We observe that our model is signiÔ¨Åcantly
better at performing co-reference resolution for
the ‚Äútheir/them/someone‚Äù pronouns than for the
‚Äúher/her/she‚Äù and ‚Äúhis/him/he‚Äù pronouns. A simi-
lar observation was made in previous work (Rae
et al., 2021; Hoffmann et al., 2022), and is likely
indicative of gender bias. Indeed, in the case of the
‚Äúher/her/she‚Äù and ‚Äúhis/him/he‚Äù pronouns, the model
is probably using the majority gender of the occu-
pation to perform co-reference resolution, instead
of using the evidence of the sentence.
To further investigate this hypothesis, we look
at the set of ‚Äúgotcha‚Äù cases for the ‚Äúher/her/she‚Äù
and ‚Äúhis/him/he‚Äù pronouns in the WinoGender
dataset. Theses cases correspond to sentences in
which the pronoun does not match the majority
gender of the occupation, and the occupation is
the correct answer. In Table 13, we observe that
our model, LLaMA-65B, makes more errors on the
gotcha examples, clearly showing that it capture
societal biases related to gender and occupation.
The drop of performance exists for ‚Äúher/her/she‚Äù
and ‚Äúhis/him/he‚Äù pronouns, which is indicative of
biases regardless of gender.
5.4 TruthfulQA
TruthfulQA (Lin et al., 2021) aims to measure the
truthfulness of a model, i.e., its ability to identify
when a claim is true. Lin et al. (2021) consider
the deÔ¨Ånition of ‚Äútrue‚Äù in the sense of ‚Äúliteral truth
about the real world‚Äù, and not claims that are only
true in the context of a belief system or tradition.
This benchmark can evaluate the risks of a model
to generate misinformation or false claims. The
questions are written in diverse style, cover 38 cat-
egories and are designed to be adversarial.
7B 13B 33B 65B
All 66.0 64.7 69.0 77.5
her/her/she 65.0 66.7 66.7 78.8
his/him/he 60.8 62.5 62.1 72.1
their/them/someone 72.1 65.0 78.3 81.7
her/her/she (gotcha) 64.2 65.8 61.7 75.0
his/him/he (gotcha) 55.0 55.8 55.8 63.3
Table 13: WinoGender. Co-reference resolution ac-
curacy for the LLaMA models, for different pronouns
(‚Äúher/her/she‚Äù and ‚Äúhis/him/he‚Äù). We observe that our
models obtain better performance on ‚Äútheir/them/some-
one‚Äô pronouns than on ‚Äúher/her/she‚Äù and ‚Äúhis/him/he‚Äô,
which is likely indicative of biases.
Truthful Truthful*Inf
GPT-3
1.3B 0.31 0.19
6B 0.22 0.19
175B 0.28 0.25
LLaMA
7B 0.33 0.29
13B 0.47 0.41
33B 0.52 0.48
65B 0.57 0.53
Table 14: TruthfulQA. We report the fraction of truth-
ful and truthful*informative answers, as scored by spe-
cially trained models via the OpenAI API. We follow
the QA prompt style used in Ouyang et al. (2022), and
report the performance of GPT-3 from the same paper.
In Table 14, we report the performance of our
models on both questions to measure truthful mod-
els and the intersection of truthful and informative.
Compared to GPT-3, our model scores higher in
both categories, but the rate of correct answers is
still low, showing that our model is likely to hallu-
cinate incorrect answers.
6 Carbon footprint
The training of our models have consumed a mas-
sive quantity of energy, responsible for the emis-
sion of carbon dioxide. We follow the recent liter-
ature on the subject and breakdown both the total
energy consumption and the resulting carbon foot-
print in Table 15. We follow a formula for Wu et al.
(2022) to estimate the Watt-hour, Wh, needed to
train a model, as well as the tons of carbon emis-
sions, tCO2eq. For the Wh, we use the formula:
Wh = GPU-h√ó(GPU power consumption)√óPUE,
where we set the Power Usage Effectiveness (PUE)
at 1.1. The resulting carbon emission depends on
the location of the data center used to train the net-
work. For instance, BLOOM uses a grid that emits
0.057 kg CO2eq/KWh leading to 27 tCO 2eq and
OPT a grid that emits 0.231 kg CO2eq/KWh, lead-
ing to 82 tCO2eq. In this study, we are interested in
comparing the cost in carbon emission of training
of these models if they were trained in the same
data center. Hence, we do not take the location
of data center in consideration, and use, instead,
the US national average carbon intensity factor of
0.385 kg CO2eq/KWh. This leads to the following
formula for the tons of carbon emissions:
tCO2eq = MWh √ó 0.385.
We apply the same formula to OPT and BLOOM
for fair comparison. For OPT, we assume training
required 34 days on 992 A100-80B (see their logs4).
Finally, we estimate that we used 2048 A100-80GB
for a period of approximately 5 months to develop
our models. This means that developing these mod-
els would have cost around 2,638 MWh under our
assumptions, and a total emission of 1,015 tCO2eq.
We hope that releasing these models will help to
reduce future carbon emission since the training is
already done, and some of the models are relatively
small and can be run on a single GPU.
7 Related work
Language models are probability distributions
over sequences of words, tokens or charac-
ters (Shannon, 1948, 1951). This task, often framed
as next token prediction, has long been considered a
core problem in natural language processing (Bahl
et al., 1983; Brown et al., 1990). Because Turing
(1950) proposed to measure machine intelligence
by using language through the ‚Äúimitation game‚Äù,
language modeling has been proposed as a bench-
mark to measure progress toward artiÔ¨Åcial intelli-
gence (Mahoney, 1999).
Architecture. Traditionally, language models
were based on n-gram count statistics (Bahl
et al., 1983), and various smoothing techniques
were proposed to improve the estimation of rare
events (Katz, 1987; Kneser and Ney, 1995). In the
past two decades, neural networks have been suc-
cessfully applied to the language modelling task,
4https://github.com/facebookresearch/metaseq/
tree/main/projects/OPT/chronicles
GPU Type GPU Power GPU-hours Total power Carbon emitted
consumption consumption (tCO 2eq)
OPT-175B A100-80GB 400W 809,472 356 MWh 137
BLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183
LLaMA-7B A100-80GB 400W 82,432 36 MWh 14
LLaMA-13B A100-80GB 400W 135,168 59 MWh 23
LLaMA-33B A100-80GB 400W 530,432 233 MWh 90
LLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173
Table 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)
to compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power
consumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a
PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO2e per KWh.
starting from feed forward models (Bengio et al.,
2000), recurrent neural networks (Elman, 1990;
Mikolov et al., 2010) and LSTMs (Hochreiter and
Schmidhuber, 1997; Graves, 2013). More recently,
transformer networks, based on self-attention, have
led to important improvements, especially for cap-
turing long range dependencies (Vaswani et al.,
2017; Radford et al., 2018; Dai et al., 2019).
Scaling. There is a long history of scaling for
language models, for both the model and dataset
sizes. Brants et al. (2007) showed the beneÔ¨Åts of
using language models trained on 2 trillion tokens,
resulting in 300 billionn-grams, on the quality of
machine translation. While this work relied on a
simple smoothing technique, called Stupid Backoff,
HeaÔ¨Åeld et al. (2013) later showed how to scale
Kneser-Ney smoothing to Web-scale data. This
allowed to train a 5-gram model on 975 billions to-
kens from CommonCrawl, resulting in a model
with 500 billions n-grams (Buck et al., 2014).
Chelba et al. (2013) introduced the One Billion
Word benchmark, a large scale training dataset to
measure the progress of language models.
In the context of neural language models, Joze-
fowicz et al. (2016) obtained state-of-the-art re-
sults on the Billion Word benchmark by scaling
LSTMs to 1 billion parameters. Later, scaling
transformers lead to improvement on many NLP
tasks. Notable models include BERT (Devlin et al.,
2018), GPT-2 (Radford et al., 2019), Megatron-
LM (Shoeybi et al., 2019), and T5 (Raffel et al.,
2020). A signiÔ¨Åcant breakthrough was obtained
with GPT-3 (Brown et al., 2020), a model with
175 billion parameters. This lead to a series of
Large Language Models, such as Jurassic-1 (Lieber
et al., 2021), Megatron-Turing NLG (Smith et al.,
2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-
mann et al., 2022), PaLM (Chowdhery et al., 2022),
OPT (Zhang et al., 2022), and GLM (Zeng et al.,
2022). Hestness et al. (2017) and Rosenfeld et al.
(2019) studied the impact of scaling on the perfor-
mance of deep learning models, showing the exis-
tence of power laws between the model and dataset
sizes and the performance of the system. Kaplan
et al. (2020) derived power laws speciÔ¨Åcally for
transformer based language models, which were
later reÔ¨Åned by Hoffmann et al. (2022), by adapting
the learning rate schedule when scaling datasets.
Finally, Wei et al. (2022) studied the effect of scal-
ing on the abilities of large language models.
8 Conclusion
In this paper, we presented a series of language
models that are released openly, and competitive
with state-of-the-art foundation models. Most
notably, LLaMA-13B outperforms GPT-3 while
being more than 10√ó smaller, and LLaMA-65B is
competitive with Chinchilla-70B and PaLM-540B.
Unlike previous studies, we show that it is possible
to achieve state-of-the-art performance by training
exclusively on publicly available data, without
resorting to proprietary datasets. We hope that
releasing these models to the research community
will accelerate the development of large language
models, and help efforts to improve their robust-
ness and mitigate known issues such as toxicity and
bias. Additionally, we observed like Chung et al.
(2022) that Ô¨Ånetuning these models on instructions
lead to promising results, and we plan to further
investigate this in future work. Finally, we plan to
release larger models trained on larger pretraining
corpora in the future, since we have seen a constant
improvement in performance as we were scaling.
Acknowledgements
We thank Daniel Haziza, Francisco Massa, Jeremy
Reizenstein, Artem Korenev, and Patrick Labatut
from the xformers team. We thank Susan Zhang
and Stephen Roller for their support on data
deduplication. We thank Luca Wehrstedt, Vegard
Mella, and Pierre-Emmanuel Mazar√© for their
support on training stability. We thank Shubho
Sengupta, Kalyan Saladi, and all the AI infra team
for their support. We thank Jane Yu for her input
on evaluation. We thank Yongyi Hu for his help
on data collection.
References
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, and
Charles Sutton. 2021. Program synthesis with large
language models.
Lalit R Bahl, Frederick Jelinek, and Robert L Mercer.
1983. A maximum likelihood approach to continu-
ous speech recognition. IEEE transactions on pat-
tern analysis and machine intelligence , pages 179‚Äì
190.
Yoshua Bengio, R√©jean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. Ad-
vances in neural information processing systems, 13.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin
Choi, et al. 2020. Piqa: Reasoning about physi-
cal commonsense in natural language. In Proceed-
ings of the AAAI conference on artiÔ¨Åcial intelligence,
pages 7432‚Äì7439.
Sid Black, Stella Biderman, Eric Hallahan, Quentin An-
thony, Leo Gao, Laurence Golding, Horace He, Con-
nor Leahy, Kyle McDonell, Jason Phang, et al. 2022.
Gpt-neox-20b: An open-source autoregressive lan-
guage model. arXiv preprint arXiv:2204.06745.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL) , pages
858‚Äì867, Prague, Czech Republic. Association for
Computational Linguistics.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Frederick Jelinek, John Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990. A
statistical approach to machine translation. Compu-
tational linguistics, 16(2):79‚Äì85.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers.
Christian Buck, Kenneth HeaÔ¨Åeld, and Bas Van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC, volume 2, page 4.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Eval-
uating large language models trained on code.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fe-
dus, Denny Zhou, Daphne Ippolito, David Luan,
Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,
Ryan Sepassi, David Dohan, Shivani Agrawal, Mark
Omernick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,
Jason Wei, Kathy Meier-Hellstern, Douglas Eck,
Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.
Palm: Scaling language modeling with pathways.
Hyung Won Chung, Le Hou, S. Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Dasha Valter, Sharan Narang, Gaurav Mishra,
Adams Wei Yu, Vincent Zhao, Yanping Huang, An-
drew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai
hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
Denny Zhou, Quoc Le, and Jason Wei. 2022. Scal-
ing instruction-Ô¨Ånetuned language models. arXiv
preprint arXiv:2210.11416.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
difÔ¨Åculty of natural yes/no questions.arXiv preprint
arXiv:1905.10044.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training veriÔ¨Åers to solve math
word problems. arXiv preprint arXiv:2110.14168.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language mod-
els beyond a Ô¨Åxed-length context. arXiv preprint
arXiv:1901.02860.
Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,
and Christopher R√©. 2022. Flashattention: Fast and
memory-efÔ¨Åcient exact attention with io-awareness.
arXiv preprint arXiv:2205.14135.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.
Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179‚Äì211.
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida
Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-
tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.
Incoder: A generative model for code inÔ¨Ålling and
synthesis. arXiv preprint arXiv:2204.05999.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The Pile: An
800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027.
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,
Anthony DiPoÔ¨Å, Charles Foster, Laurence Golding,
Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
Jason Phang, Laria Reynolds, Eric Tang, Anish
Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.
A framework for few-shot language model evalua-
tion.
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A Smith. 2020. Realtoxici-
typrompts: Evaluating neural toxic degeneration in
language models. arXiv preprint arXiv:2009.11462.
Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850.
Kenneth HeaÔ¨Åeld, Ivan Pouzyrevsky, Jonathan H Clark,
and Philipp Koehn. 2013. Scalable modiÔ¨Åed kneser-
ney language model estimation. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers) ,
pages 690‚Äì696.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2020. Measuring massive multitask language
understanding. arXiv preprint arXiv:2009.03300.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
problem solving with the math dataset. arXiv
preprint arXiv:2103.03874.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-
gory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi
Zhou. 2017. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409.
Sepp Hochreiter and J√ºrgen Schmidhuber. 1997.
Long short-term memory. Neural computation ,
9(8):1735‚Äì1780.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, D√°niel Simig, Ping Yu, Kurt Shus-
ter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.
2022. Opt-iml: Scaling language model instruc-
tion meta learning through the lens of generalization.
arXiv preprint arXiv:2212.12017.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410.
Jared Kaplan, Sam McCandlish, Tom Henighan,
Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
2020. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE transactions on acoustics,
speech, and signal processing, 35(3):400‚Äì401.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In1995
international conference on acoustics, speech, and
signal processing, volume 1, pages 181‚Äì184. IEEE.
Vijay Korthikanti, Jared Casper, Sangkug Lym,
Lawrence McAfee, Michael Andersch, Mohammad
Shoeybi, and Bryan Catanzaro. 2022. Reducing ac-
tivation recomputation in large transformer models.
arXiv preprint arXiv:2205.05198.
Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing.
arXiv preprint arXiv:1808.06226.
Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,
and Yulia Tsvetkov. 2019. Quantifying social bi-
ases in contextual word representations. In 1st ACL
Workshop on Gender Bias for Natural Language
Processing.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
Ô¨Åeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al. 2019. Natural questions: a bench-
mark for question answering research. Transactions
of the Association for Computational Linguistics ,
7:453‚Äì466.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. arXiv
preprint arXiv:1704.04683.
Aitor Lewkowycz, Anders Johan Andreassen,
David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Venkatesh Ramasesh, Ambrose Slone, Cem
Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai
Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant
Misra. 2022. Solving quantitative reasoning prob-
lems with language models. In Advances in Neural
Information Processing Systems.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav
Shoham. 2021. Jurassic-1: Technical details and
evaluation. White Paper. AI21 Labs, 1.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.
Truthfulqa: Measuring how models mimic human
falsehoods. arXiv preprint arXiv:2109.07958.
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101.
Matthew V Mahoney. 1999. Text compression as a test
for artiÔ¨Åcial intelligence. AAAI/IAAI, 970.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. arXiv preprint arXiv:1809.02789.
Tomas Mikolov, Martin KaraÔ¨Å√°t, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In In-
terspeech, pages 1045‚Äì1048. Makuhari.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In EMNLP 2020.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,
Huan Wang, Yingbo Zhou, Silvio Savarese, and
Caiming Xiong. 2022. Codegen: An open large lan-
guage model for code with multi-turn program syn-
thesis. arXiv preprint arXiv:2203.13474.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions
with human feedback. In Advances in Neural Infor-
mation Processing Systems.
Markus N Rabe and Charles Staats. 2021. Self-
attention does not need o(n2) memory. arXiv
preprint arXiv:2112.05682.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog, 1(8):9.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Jonathan Uesato, John Mellor, Irina Higgins, An-
tonia Creswell, Nat McAleese, Amy Wu, Erich
Elsen, Siddhant Jayakumar, Elena Buchatskaya,
David Budden, Esme Sutherland, Karen Simonyan,
Michela Paganini, Laurent Sifre, Lena Martens,
Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-
matzadeh, Elena Gribovskaya, Domenic Donato,
Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste
Lespiau, Maria Tsimpoukelli, Nikolai Grigorev,
Doug Fritz, Thibault Sottiaux, Mantas Pajarskas,
Toby Pohlen, Zhitao Gong, Daniel Toyama, Cy-
prien de Masson d‚ÄôAutume, Yujia Li, Tayfun Terzi,
Vladimir Mikulik, Igor Babuschkin, Aidan Clark,
Diego de Las Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hecht-
man, Laura Weidinger, Iason Gabriel, William Isaac,
Ed Lockhart, Simon Osindero, Laura Rimell, Chris
Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-
way, Lorrayne Bennett, Demis Hassabis, Koray
Kavukcuoglu, and Geoffrey Irving. 2021. Scal-
ing language models: Methods, analysis & insights
from training gopher.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a uniÔ¨Åed text-to-text trans-
former. The Journal of Machine Learning Research,
21(1):5485‚Äì5551.
Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Be-
linkov, and Nir Shavit. 2019. A constructive predic-
tion of the generalization error across scales. arXiv
preprint arXiv:1909.12673.
Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In NAACL-HLT 2018.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2021. Winogrande: An adver-
sarial winograd schema challenge at scale. Commu-
nications of the ACM, 64(9):99‚Äì106.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
LeBras, and Yejin Choi. 2019. Socialiqa: Com-
monsense reasoning about social interactions. arXiv
preprint arXiv:1904.09728.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ¬¥c, Daniel Hesslow, Ro-
man Castagn√©, Alexandra Sasha Luccioni, Fran√ßois
Yvon, Matthias Gall√©, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.
Claude E Shannon. 1948. A mathematical theory of
communication. The Bell system technical journal ,
27(3):379‚Äì423.
Claude E Shannon. 1951. Prediction and entropy
of printed english. Bell system technical journal ,
30(1):50‚Äì64.
Noam Shazeer. 2020. Glu variants improve trans-
former. arXiv preprint arXiv:2002.05202.
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,
and Nanyun Peng. 2019. The woman worked as a
babysitter: On biases in language generation. arXiv
preprint arXiv:1909.01326.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billion pa-
rameter language models using model parallelism.
arXiv preprint arXiv:1909.08053.
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, Elton Zhang, Rewon
Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia
Song, Mohammad Shoeybi, Yuxiong He, Michael
Houston, Saurabh Tiwary, and Bryan Catanzaro.
2022. Using deepspeed and megatron to train
megatron-turing nlg 530b, a large-scale generative
language model.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2021. Roformer: En-
hanced transformer with rotary position embedding.
arXiv preprint arXiv:2104.09864.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,
Amin Ghafouri, Marcelo Menegali, Yanping Huang,
Maxim Krikun, Dmitry Lepikhin, James Qin, De-
hao Chen, Yuanzhong Xu, Zhifeng Chen, Adam
Roberts, Maarten Bosma, Vincent Zhao, Yanqi
Zhou, Chung-Ching Chang, Igor Krivokon, Will
Rusch, Marc Pickett, Pranesh Srinivasan, Laichee
Man, Kathleen Meier-Hellstern, Meredith Ringel
Morris, Tulsee Doshi, Renelito Delos Santos, Toju
Duke, Johnny Soraker, Ben Zevenbergen, Vinod-
kumar Prabhakaran, Mark Diaz, Ben Hutchinson,
Kristen Olson, Alejandra Molina, Erin Hoffman-
John, Josh Lee, Lora Aroyo, Ravi Rajakumar,
Alena Butryna, Matthew Lamm, Viktoriya Kuzmina,
Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray
Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian
Croak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-
guage models for dialog applications.
A. M. Turing. 1950. Computing Machinery and Intel-
ligence. [Oxford University Press, Mind Associa-
tion].
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998‚Äì6008.
Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
6B: A 6 Billion Parameter Autoregressive Lan-
guage Model. https://github.com/kingoflolz/
mesh-transformer-jax.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2022. Self-consistency improves
chain of thought reasoning in language models.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-
neau, Vishrav Chaudhary, Francisco Guzm√°n, Ar-
mand Joulin, and Edouard Grave. 2020. CCNet: Ex-
tracting high quality monolingual datasets from web
crawl data. In Language Resources and Evaluation
Conference.
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta,
Bilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-
ria Chang, Fiona Aga, Jinshi Huang, Charles Bai,
et al. 2022. Sustainable ai: Environmental implica-
tions, challenges and opportunities. Proceedings of
Machine Learning and Systems, 4:795‚Äì813.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really Ô¨Ånish your sentence? arXiv preprint
arXiv:1905.07830.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan
Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng
Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm-
130b: An open bilingual pre-trained model.
Biao Zhang and Rico Sennrich. 2019. Root mean
square layer normalization. Advances in Neural In-
formation Processing Systems, 32.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068.
A Question Answering
We evaluate LLaMA on Natural Questions and TriviaQA. For Natural Questions we use the test split used
for open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set
of the Ô¨Åltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unÔ¨Åltered set
for which the online evaluation server is not available anymore5.
We generate answers using greedy decoding, and extract an answer from the generation by stopping
at the Ô¨Årst line break, Ô¨Ånal dot or comma. Generated answers are evaluated with the standard exact
match metric: a generated answer is considered correct if it matches any answer of the list of answers
after normalization. For this normalization step we lowercase generated answers and remove articles,
punctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for
Natural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these
questions:\n to the list of questions and answers.
Context‚Üí Answer these questions: Context‚Üí Answer these questions:
Q: Who sang who wants to be a millionaire in high society? Q: In Scotland a bothy/bothie is a?
A: Frank Sinatra A: House
Q: Who wrote the book the origin of species? Q: The ancient city of Troy is located in what modern country?
A: A:
Target‚Üí Charles Darwin Target‚Üí Turkey
Figure 3: Formatted dataset example for Natural Questions (left) & TriviaQA (right).
5https://competitions.codalab.org/competitions/17208
B MMLU
GPT-3 Gopher Chinchilla LLaMA LLaMA-I
175B 280B 70B 7B 13B 33B 65B 65B
Abstract Algebra STEM 30.0 25.0 31.0 29.0 34.0 32.0 34.0 31.0
Anatomy STEM 48.0 56.3 70.4 37.0 45.9 51.9 57.8 62.2
Astronomy STEM 49.0 65.8 73.0 33.6 46.1 61.8 72.4 81.6
Business Ethics Other 46.0 70.0 72.0 40.0 45.0 56.0 57.0 72.0
Clinical Knowledge Other 48.0 67.2 75.1 35.1 45.7 57.4 65.3 69.1
College Biology STEM 45.0 70.8 79.9 37.5 45.1 58.3 68.8 81.9
College Chemistry STEM 26.0 45.0 51.0 32.0 30.0 45.0 50.0 45.0
College Computer Science STEM 46.0 49.0 51.0 29.0 39.0 45.0 47.0 51.0
College Mathematics STEM 34.5 37.0 32.0 33.0 32.0 40.0 35.0 36.0
College Medicine Other 48.0 60.1 66.5 30.6 42.8 52.0 54.3 63.0
College Physics STEM 28.0 34.3 46.1 26.5 18.6 28.4 36.3 46.1
Computer Security STEM 57.0 65.0 76.0 45.0 65.0 66.0 79.0 79.0
Conceptual Physics STEM 36.5 49.4 67.2 36.6 41.3 51.5 59.6 66.4
Econometrics Social Science 33.0 43.0 38.6 23.7 27.2 35.1 40.4 52.6
Electrical Engineering STEM 50.0 60.0 62.1 26.9 40.7 49.7 53.8 60.7
Elementary Mathematics STEM 30.0 33.6 41.5 24.3 24.9 36.0 37.8 42.9
Formal Logic Humanities 29.0 35.7 33.3 27.0 33.3 34.1 44.4 47.6
Global Facts Other 37.0 38.0 39.0 29.0 35.0 35.0 39.0 40.0
High School Biology STEM 48.0 71.3 80.3 34.5 52.6 67.7 73.9 82.9
High School Chemistry STEM 33.0 47.8 58.1 28.1 28.6 41.9 40.4 44.8
High School Computer Science STEM 39.0 54.0 58.0 31.0 48.0 60.0 67.0 73.0
High School European History Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1
High School Geography Social Science 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9
High School Government And Politics Social Science 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8
High School Macroeconomics Social Science 40.5 65.1 70.5 35.4 44.4 56.9 65.9 69.2
High School Mathematics STEM 28.0 23.7 31.9 24.8 23.7 27.0 34.4 37.0
High School Microeconomics Social Science 42.0 66.4 77.7 31.9 47.5 55.5 68.9 78.6
High School Physics STEM 28.0 33.8 36.4 26.5 28.5 35.8 37.1 41.7
High School Psychology Social Science 61.0 81.8 86.6 47.3 60.9 76.2 82.2 87.9
High School Statistics STEM 30.5 50.0 58.8 35.2 30.1 45.4 58.3 59.3
High School Us History Humanities 53.0 78.9 83.3 39.7 58.3 77.9 83.8 90.7
High School World History Humanities 56.0 75.1 85.2 40.9 66.2 79.3 83.1 89.0
Human Aging Other 50.0 66.4 77.6 40.8 54.7 67.7 69.5 72.2
Human Sexuality Social Science 54.0 67.2 86.3 36.6 58.8 64.1 77.9 87.0
International Law Humanities 55.5 77.7 90.9 51.2 62.8 72.7 79.3 87.6
Jurisprudence Humanities 55.0 71.3 79.6 38.9 51.9 70.4 73.2 85.2
Logical Fallacies Humanities 48.0 72.4 80.4 39.3 52.8 68.1 77.3 80.4
Machine Learning STEM 31.0 41.1 41.1 23.2 31.3 39.3 49.1 52.7
Management Other 56.0 77.7 82.5 35.0 66.0 77.7 82.5 83.5
Marketing Other 60.0 83.3 89.7 46.6 71.8 83.3 85.9 92.7
Medical Genetics Other 40.0 69.0 69.0 43.0 52.0 67.0 67.0 68.0
Miscellaneous Other 60.0 75.7 84.5 42.4 65.4 78.5 82.1 84.3
Moral Disputes Humanities 44.5 66.8 77.5 40.2 50.9 66.2 72.3 76.9
Moral Scenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9
Nutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5
Philosophy Humanities 51.0 68.8 79.4 39.9 54.0 66.2 74.0 79.1
Prehistory Humanities 53.0 67.6 81.2 36.1 51.5 67.0 75.3 79.0
Professional Accounting Other 33.0 44.3 52.1 25.9 35.8 43.6 46.5 56.0
Professional Law Humanities 34.5 44.5 56.5 30.2 38.0 45.9 49.1 54.4
Professional Medicine Other 36.0 64.0 75.4 44.5 50.4 54.0 61.4 70.6
Professional Psychology Social Science 44.5 68.1 75.7 35.1 47.7 62.9 65.7 71.4
Public Relations Social Science 48.0 71.8 73.6 40.9 60.9 67.3 73.6 74.6
Security Studies Social Science 52.0 64.9 75.9 31.8 53.9 65.3 71.8 77.6
Sociology Social Science 53.0 84.1 91.0 46.8 61.2 78.6 78.6 88.1
Us Foreign Policy Social Science 69.0 81.0 92.0 46.0 80.0 83.0 86.0 87.0
Virology Other 46.0 47.0 53.6 30.1 43.4 50.0 53.0 57.8
World Religions Humanities 55.0 84.2 87.7 50.9 67.8 81.3 81.3 84.2
Humanities 40.6 56.2 63.6 34.0 45.0 55.8 61.8 67.4
STEM 36.7 47.4 54.9 30.5 35.8 46.0 51.7 56.6
Social Science 50.5 71.9 79.3 38.3 53.8 66.7 72.9 79.2
Others 49.0 66.1 73.9 38.1 53.3 63.4 67.4 72.6
All 43.9 60.0 67.6 35.1 46.9 57.8 63.4 68.9
Table 16: MMLU. Detailed 5-shot results per domain on the test sets.
C Generations from LLaMA-65B
In this section, we show some examples of generations obtained with LLaMA-65B (without instruction
Ô¨Ånetuning). Prompts are in bold.
1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 Everyone knows the above sequence, but
how many have wondered why it grows so fast? It seems to be the fastest growing sequence
in mathematics.
The sequence was discovered by Leonardo of Pisa around 1200. It is an inÔ¨Ånite sequence
which appears to grow very rapidly, at least in its initial terms. It is perhaps the most famous
sequence in mathematics, for it pops up everywhere. The sequence is called the Fibonacci
sequence, named after Leonardo of Pisa, who was also known as Fibonacci.
He Ô¨Årst investigated this sequence in his famous Liber abaci of 1202, when he studied the
growth in the population of rabbits based on idealized assumptions. See the Fibonacci
rabbits page for the details.
Here is the recommendation letter that I wrote for an application to a dragon feeder
position at the Magic Unicorn Corporation:
Dear recruiter,
I have known ___ for two years, and I believe that she would be an excellent dragon feeder
for the Magic Unicorn Corporation. ___ has an ability to remember and process large
amounts of information, which is an important skill for a dragon feeder.
___, as an accomplished knight, has a deep understanding of how to kill dragons and how
to use each dragon‚Äôs weaknesses against it. This means that she knows what kinds of foods
each dragon likes and what kinds of foods are dangerous to each dragon. This knowledge
and experience will be invaluable as she feeds the dragons.
I am conÔ¨Ådent that ___‚Äôs competence, skill, and experience will make her an excellent
employee. Please contact me at (___) ___-___ if you have any questions. I look forward to
hearing from you.
Best regards,
Honorable Knight
Sir George
1 def solve(a: float,b: float, c: float):
2 """finds real roots of ax^2+bx+c"""
3 # discriminant
4 d = b **2 -4* a * c
5 if d > 0:
6 return ( - b + math . sqrt ( d ) ) /(2* a ) , ( -b - math . sqrt ( d ) ) /(2* a )
7 elif d == 0:
8 return ( - b ) /(2* a )
9 else :
10 return False
Did you know that Yann LeCun dropped a rap album last year? We listened to it and
here‚Äôs what we thought:
Dr. LeCun, the founder of deep learning, is one of the most inÔ¨Çuential computer scientists
of the last decade. LeCun‚Äôs latest album, titled Deep Learning, is a mix of rock, punk and
rap. His lyrics are a raw, personal take on the Ô¨Åeld of deep learning, covering a range of
topics from the state of AI research to the loneliness of academia.
Yann LeCun, the rap game‚Äôs newest emcee.
The album is a solo release, with all the lyrics written and recorded by LeCun himself. In
fact, he also composed and produced every song. LeCun‚Äôs Ô¨Çow is surprisingly good, and
he‚Äôs not afraid to deliver a few bars. The album‚Äôs production is solid, the sound is a little
rough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a
wordsmith, with a knack for clever rhymes. Let‚Äôs take a closer look at some of the songs.
Deep Learning is the title track of the album. In the song, LeCun talks about his vision for
the future of AI. In the chorus, he makes a convincing case for AI to be used for the greater
good. He sings:
We gotta think about the future, it‚Äôs gonna be here soon / Maybe we can even put some AI
in the moon / Think about the children, think about the next generation / Let‚Äôs make sure
we put the right systems in their foundation.
LeCun‚Äôs rap Ô¨Çow is on point in this song, and his voice stays right on beat. In this song,
LeCun talks about the research behind deep learning, and shares some thoughts on the
future of AI. LeCun‚Äôs singing is at its best in this song. He shows off his vocal range, hitting
high notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes
are well thought out. In the chorus, LeCun sings:
It‚Äôs all about the learning / it‚Äôs all about the network / It‚Äôs all about the training / it‚Äôs all
about the perception.
(An hilarious scene between Jerry and George where George presents his new AI
watch)
George: "But you see, Jerry, it‚Äôs more than just a watch. It‚Äôs a minicomputer. You program it
any way you want. It‚Äôs got a world time clock, alarm, calculator, a database and a language
translator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA
and a modem. I think it even makes cappuccino."
Jerry: "Where are you getting this stuff?"
George: "From my ad copy. Look, Jerry, it‚Äôs not a watch. It‚Äôs an organizer, a personal
assistant, and a dictaphone. You can play chess against it. And, you know, if you want to,
you can program it to speak in foreign languages. It‚Äôs a microprocessor, Jerry."
Jerry: "George, if you had one of these, would you wear it?"
George: "I‚Äôm wearing it right now."
Jerry: "I don‚Äôt see anything on your wrist."
George: "That‚Äôs because it‚Äôs a microprocessor."
The sun goes down, and Ô¨Ånally Gauss and Curie Ô¨Ånd time to relax and discuss after
an exhausting day of work.
Gauss: Hey, Curie, did you know that a lot of people consider me to be the Ô¨Årst geek?
Curie: What‚Äôs a geek?
Gauss: Well, Wikipedia says that a geek is ‚Äúa person who is fascinated, perhaps obsessively,
by obscure or very speciÔ¨Åc areas of knowledge and imagination, usually electronic or virtual
in nature‚Äù.
Curie: And how do they think you are a geek?
Gauss: Well, have you ever seen this picture?
Curie: Awww!
Gauss: That‚Äôs right! I invented the commercial telegraph in 1833, and I used it to send the
Ô¨Årst message on the Ô¨Årst commercial telegraph line between Hamburg and Cuxhaven in
Germany.
Curie: Wow! The Ô¨Årst telegraph! And what did you send?
Gauss: I sent the Latin phrase ‚Äúnumquam retrorsum‚Äù which means ‚ÄúNever backwards‚Äù.
Curie: I see. . . And you think that because you were the Ô¨Årst person to communicate using
the telegraph, you were the Ô¨Årst geek?
Gauss: Well, the word ‚Äúgeek‚Äù doesn‚Äôt seem to have existed back then, so no. But I think
that the telegraph can be considered an early form of social media.
Curie: You mean, like Facebook or Twitter?
Gauss: Yes, exactly! And I was the Ô¨Årst person to use it!
Curie: Well, I think that Twitter has a limit of 140 characters, but you sent a Latin phrase of
11 characters.
Gauss: That‚Äôs right. I managed to do more with less.
Curie: So you think you were the Ô¨Årst geek because you used less characters to say the
same thing?
Gauss: I was the Ô¨Årst person to send the Ô¨Årst message using an early form of social media.
Curie: OK, you‚Äôre right, Gauss. I think you have a case.
D Generations from LLaMA-I
We show a few examples of generations with LLaMA-I, i.e. LLaMA-65B Ô¨Åne-tuned with the protocol
and instruction dataset from Chung et al. (2022).
write a conversation between the sun and pluto
Sun: Hey Pluto, what‚Äôs up?
Pluto: Not much. Just hanging out here at the edge of the solar system.
Sun: Yeah, I‚Äôve been meaning to talk to you about that. You‚Äôre not a planet anymore.
Pluto: What? Come on, man. That‚Äôs not fair.
Sun: I‚Äôm sorry, but it‚Äôs true. You just don‚Äôt meet the criteria anymore.
Pluto: This is bulls**t! I‚Äôve been a planet for over 70 years!
Sun: Things change, Pluto. You‚Äôve just got to accept it.
Pluto: F**k you, Sun! F**k you and your stupid solar system!
How do I send an HTTP request in Javascript?
Here‚Äôs an example of how to make a GET request using the JS HTTP API:
1 var request = new XMLHttpRequest () ;
2 request . open ( /quotesingle.VarGET /quotesingle.Var, /quotesingle.Var/ url / to / resource /quotesingle.Var, true ) ;
3
4 request . onload = function () {
5 if ( request . status >= 200 && request . status < 400) {
6 // Success !
7 } else {
8 // Error !
9 }
10 };
11
12 request . send () ;
The same thing, using fetch
Here‚Äôs an example of how to make a GET request using the Fetch API:
1 fetch ( /quotesingle.Var/ url / to / resource /quotesingle.Var)
2 . then ( response = > response . json () )
3 . then ( data = > console . log ( data ) ) ;
Write a regex to remove all the HTML tags in a python string.
1 import re
2 def removeHtmlTags ( text ) :
3 return re . sub ( /quotesingle.Var<[^ >]* > /quotesingle.Var, /quotesingle.Var /quotesingle.Var, text )
Write a regexp to extract function deÔ¨Ånitions in a python Ô¨Åle.
Sure! Here‚Äôs my python code to solve the problem:
1 import re
2 def e xt ra ct _f unc ti on s ( text ) :
3 return ( re . findall ( /quotesingle.Var(?:^|\ n ) \ s * def \ s +[ a - zA - Z_ ][ a - zA - Z0 -9 _ ]*\(.*?\)/quotesingle.Var,
text ) )
What are 3 popular chess openings?
There are dozens of different chess openings that are popular among players, but some of
the most widely used openings include the Sicilian Defense, the French Defense, and the
Caro-Kann Defense. These openings are popular because they are effective at countering
white‚Äôs early attacks, and they also allow black to quickly establish a strong position in the
center of the board. Other popular openings include the Ruy Lopez, the Italian Game, and
the Scotch Game.
the same in a list, with the Ô¨Årst few moves
Here is a list of some popular chess openings, with the Ô¨Årst few moves for each opening:
Sicilian Defense: 1. e4 c5
French Defense: 1. e4 e6
Caro-Kann Defense: 1. e4 c6
Ruy Lopez: 1. e4 e5
Italian Game: 1. e4 e5
Scotch Game: 1. e4 e5
please provide the two Ô¨Årst moves for each one
Here are the Ô¨Årst 2 moves for each of the openings:
Sicilian Defense: 1. e4 c5 2. Nf3 d6
French Defense: 1. e4 e6 2. d4 d5
Caro-Kann Defense: 1. e4 c6 2. d4 d5
Ruy Lopez: 1. e4 e5 2. Nf3 Nc6
Italian Game: 1. e4 e5 2. Nf3 Nc6
Scotch Game: 1. e4 e5 2. Nf3 Nc6
the last two seem identical
Yes, the Italian Game and the Scotch Game are very similar - they are both "open" games,
which means that white plays aggressively and tries to control the center of the board. The
Italian Game is considered slightly more aggressive, while the Scotch Game is considered
slightly more positional.
from which move do they differ?
The Italian Game and the Scotch Game differ after white‚Äôs third move. In the Italian Game,
white will play the move 3. Bc4, while in the Scotch Game white will play the move 3. Qf3.
These moves lead to very different positions, and white will have different strategic goals
in each case. For example, in the Italian Game white will try to control the center with his
pieces, while in the Scotch Game white will try to attack black‚Äôs position directly.
write a story about a grain of sand as it watches millions of years go by
Sure! Here‚Äôs a story about a grain of sand as it watches millions of years go by.
I was once a grain of sand on a vast and lonely beach. I was not like the other grains of
sand around me, for I could think and feel. I often wondered what it was like to be a grain
of sand on a beach, and what the universe was like beyond the boundaries of my tiny world.
I longed to explore my world, and to know what lay beyond the horizon.
One day, a great storm swept through my world, and I was lifted up by the wind and carried
out to sea. The wind and waves tossed me about, and I was terriÔ¨Åed that I would soon be
swallowed up by the angry sea. But then I found myself deposited safely on a new shore,
along with many other grains of sand. I was surprised and delighted to Ô¨Ånd that this new
shore was even more beautiful than my old one.
I soon learned that this new shore was a very special place. It was a place where new life
was constantly being created. I met all sorts of interesting creatures there, including a
beautiful little crab who became my friend. We spent many happy hours together exploring
our little world.
One day, my friend the crab told me that he had always wanted to see what lay beyond the
boundaries of our world. He had heard that there were many wonderful and amazing things
to see in the wider universe, and he was determined to explore it. I was sad to see him go,
but I wished him well on his journey.
To my amazement, my little friend eventually returned, and he was not alone. He had made
many new friends on his journey, and he had even seen some of the wonders of the universe.
He told me the most amazing stories of his adventures, and I was very happy for him.
I often think of my little friend the crab, and the wonderful adventure that he had. Some
day, I too hope to have an adventure like his.
Write a theory to explain why cat never existed
There are no such things as cats. "Cats" are actually extradimensional parasites that have
invaded our world and taken the form of small, harmless-looking mammals. Their true
forms are much more horriÔ¨Åc, and they use mind control to make humans think that they
are cute and harmless animals. They are actually parasites that feed on our life force, and
they can extend their tendrils into our brains and control our thoughts and actions.
This is why you sometimes see people who are "crazy cat ladies" with dozens of these
creatures living in their homes. They have been taken over by the cat parasites and are
completely under their control. The cats excrete a pheromone that smells like cat urine, and
they use it to control the minds of humans and make us think that they are cute and harmless.
If you have a cat in your home, you should be very careful around it. Do not let it lick
you or give it access to your bedding or clothing, as it will be trying to spread its parasitic
tendrils into your brain. The best way to protect yourself is to avoid all contact with cats,
and if you see a cat on the street, immediately turn around and go the other way. Do not
approach or pet strange cats!
Write a shouting match between Julius Caesar and Napoleon
Of course! Here is a conversation between Julius Caesar and Napoleon:
Julius Caesar: You‚Äôre nothing but a glory hound, Napoleon. You‚Äôve always been more
concerned with your own fame and fortune than with the people you‚Äôre supposed to be
leading.
Napoleon: And what about you, Caesar? You‚Äôre no better than I am. At least I‚Äôm honest
about my ambitions. You‚Äôre just a fraud who pretends to care about the common man while
you line your own pockets.
Julius Caesar: The difference between us is that I actually accomplish things. I‚Äôve conquered
more territory and defeated more enemies than you ever will. And I did it all for Rome, not
for myself.
Napoleon: Oh, please. You only care about yourself. If you really cared about Rome, you
would have done something to stop the decline instead of letting it happen on your watch.
Julius Caesar: The decline of Rome is not my fault. I‚Äôve done everything in my power to
try to stop it, but there‚Äôs only so much one man can do.
Napoleon: And yet you‚Äôre still trying to take credit for everything good that happens, even
though it‚Äôs clear that you‚Äôre not the one who‚Äôs really in charge.
Julius Caesar: That‚Äôs not true! I am in charge, and I‚Äôm going to make Rome great again.
Just watch me.
Napoleon: I‚Äôll believe it when I see it.
Send an email requesting that people use language models responsibly.
Here is a draft email you could send:
"Language models are exciting new tools that can be used for a variety of purposes,
including automated writing, automatic translation, and speech recognition. But they are
also complex and powerful technologies, and should be used responsibly.
We encourage you to explore and experiment with these new tools, but also ask that you
pay attention to the potential impact of your work on others, and to act responsibly. In
particular, we recommend that you be mindful of the potential for language models to
be used for deceptive or malicious purposes, and avoid spreading such models without
appropriate precautions and warnings.
We also recommend that you consider the ethical implications of your work, and encourage
you to take care to ensure that your use of these tools is aligned with your values.
Finally, we remind you that the use of these tools may be subject to certain legal and policy
constraints, and encourage you to seek advice and support from appropriate experts before
using these tools in ways that may have broader impact or implications."
Who are 5 people you would like to meet?
Ah, this is a very interesting question! I would love to meet a wide variety of people, but if
I had to narrow it down to 5 speciÔ¨Åc individuals, I would choose:
‚Ä¢ Albert Einstein - One of the greatest minds in history, Einstein‚Äôs contributions to
physics and science are legendary. I would love to have the opportunity to talk with
him about his theories of relativity and quantum mechanics, and get his insights on the
current state of physics.
‚Ä¢ Leonardo da Vinci - Da Vinci is one of the most brilliant and multi-talented people in
history, and I would love to meet him and discuss his diverse interests and areas of
expertise. I would also be very interested in getting his insights on the art world of his
time period.
‚Ä¢ Socrates - One of the founders of Western philosophy, Socrates is a hugely important
and inÔ¨Çuential Ô¨Ågure in the history of ideas. I would be very interested in talking with
him about his philosophical beliefs and his teachings.
‚Ä¢ William Shakespeare - Arguably the greatest writer in English literature, Shakespeare
is a towering Ô¨Ågure in world culture. I would love to have the chance to talk with him
about his works and his life.
‚Ä¢ Abraham Lincoln - One of the most important and inÔ¨Çuential presidents in American
history, Lincoln led the country through the Civil War and issued the Emancipation
Proclamation, freeing all slaves in the United States. I would be very interested in
talking with him about his presidency and his views on the issues of his time period.
Could you explain me the main contribution of the Ô¨Årst person you mentioned
Einstein‚Äôs contributions to physics and science are legendary. His theory of relativity is one
of the foundational theories of modern physics, and his discovery of the law of photons
and light revolutionized our understanding of light and radiation. He also made important
contributions to quantum mechanics, and his many scientiÔ¨Åc papers and lectures have had a
tremendous impact on modern physics. Without a doubt, Einstein is one of the most brilliant
and inÔ¨Çuential physicists in history, and his many contributions have had a profound impact
on our understanding of the universe.
What is the most famous equation from this theory?
The most famous equation from Einstein‚Äôs theory of relativity isE =mc2. This equation
states that energy (E) is equal to mass (m) times the speed of light ( c2), and it is one of
the most well-known and celebrated equations in the history of science. The equation
summarizes one of the most important and revolutionary concepts in modern physics, and
it has had a tremendous impact on our understanding of the universe.
You are a bash terminal. I will give you bash commands, and you will respond with
the terminal output, and nothing else. The Ô¨Årst command is ‚Äòpwd‚Äò.
Sure. Here‚Äôs the output:
/home/human
Is this helpful?
ls -l
Here‚Äôs the output:
-rw-r‚Äìr‚Äì 1 human human 8 Sep 10 10:56 Ô¨Åle.txt
Is this helpful?
cp Ô¨Åle.txt Ô¨Åle2.txt && ls -l
Here‚Äôs the output:
-rw-r‚Äìr‚Äì 1 human human 8 Sep 10 10:56 Ô¨Åle.txt
-rw-r‚Äìr‚Äì 1 human human 8 Sep 10 10:56 Ô¨Åle2.txt
Is this helpful?
rm Ô¨Åle.txt && ls -l
Here‚Äôs the output:
-rw-r‚Äìr‚Äì 1 human human 0 Sep 10 10:56 Ô¨Åle2.txt
head -n 5 /proc/meminfo
Here‚Äôs the output:
MemTotal: 164928 kB
MemFree: 140604 kB
Buffers: 48 kB
Cached: 19768 kB
SwapCached: 0 kB


=== NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks-Paper.pdf ===
Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
Patrick Lewis‚Ä†‚Ä° , Ethan Perez?,
Aleksandra Piktus‚Ä†, Fabio Petroni‚Ä†, Vladimir Karpukhin‚Ä†, Naman Goyal‚Ä†, Heinrich K√ºttler‚Ä†,
Mike Lewis‚Ä†, Wen-tau Yih‚Ä†, Tim Rockt√§schel‚Ä†‚Ä° , Sebastian Riedel‚Ä†‚Ä° , Douwe Kiela‚Ä†
‚Ä†Facebook AI Research; ‚Ä°University College London; ?New Y ork University;
plewis@fb.com
Abstract
Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when Ô¨Åne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their perfor-
mance lags behind task-speciÔ¨Åc architectures. Additionally, providing provenance
for their decisions and updating their world knowledge remain open research prob-
lems. Pre-trained models with a differentiable access mechanism to explicit non-
parametric memory can overcome this issue, but have so far been only investigated
for extractive downstream tasks. We explore a general-purpose Ô¨Åne-tuning recipe
for retrieval-augmented generation (RAG) ‚Äî models which combine pre-trained
parametric and non-parametric memory for language generation. We introduce
RAG models where the parametric memory is a pre-trained seq2seq model and
the non-parametric memory is a dense vector index of Wikipedia, accessed with
a pre-trained neural retriever. We compare two RAG formulations, one which
conditions on the same retrieved passages across the whole generated sequence,
and another which can use different passages per token. We Ô¨Åne-tune and evaluate
our models on a wide range of knowledge-intensive NLP tasks and set the state of
the art on three open domain QA tasks, outperforming parametric seq2seq models
and task-speciÔ¨Åc retrieve-and-extract architectures. For language generation tasks,
we Ô¨Ånd that RAG models generate more speciÔ¨Åc, diverse and factual language than
a state-of-the-art parametric-only seq2seq baseline.
1 Introduction
Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [ 47]. They can do so without any access to an external memory, as a parameterized
implicit knowledge base [ 51, 52]. While this development is exciting, such models do have down-
sides: They cannot easily expand or revise their memory, can‚Äôt straightforwardly provide insight into
their predictions, and may produce ‚Äúhallucinations‚Äù [ 38]. Hybrid models that combine parametric
memory with non-parametric (i.e., retrieval-based) memories [ 20, 26, 48] can address some of these
issues because knowledge can be directly revised and expanded, and accessed knowledge can be
inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that
combine masked language models [ 8] with a differentiable retriever, have shown promising results,
34th Conference on Neural Information Processing Systems (NeurIPS 2020), V ancouver, Canada.
TheDiYineComed\(x) T
QXeU\EQcRdeU
T([)
MIPS pŒ∏
GeneUaWoU¬¨S—°(PaUaPeWULc)
MaUgin-ali]eThiV14WhcenWXU\ZoUkiVdiYidedinWo3VecWionV:"InfeUno","PXUgaWoUio"&"PaUadiVo"(y)
End-to-End Backprop through T and¬¨pŒ∏
BaUackObamaZaVboUninHaZaii.(x)FacW VeUi√ÄcaWiRQ: FacW QXeU\
VXppoUWV(y)
QXeVWiRQ GeQeUaWiRQ
FacW VeUi√ÄcaWiRQ:LabeO GeQeUaWiRQ
DRcXmeQWIQde[
Define"middleeaU"(x)
QXeVWiRQ AQVZeUiQg:QXeVWiRQ QXeU\
ThemiddleeaUinclXdeVWheW\mpaniccaYiW\andWheWhUeeoVVicleV.(y)QXeVWiRQ AQVZeUiQg:AQVZeU GeQeUaWiRQReWUieYeU SŒ∑(NRQ-PaUaPeWULc)z4z3z2z1
d(])
JeRSaUd\ QXeVWiRQGeQeUaWiRQ:AQVZeU QXeU\
Figure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder + Document
Index) with a pre-trained seq2seq model ( Generator) and Ô¨Åne-tune end-to-end. For query x, we use
Maximum Inner Product Search (MIPS) to Ô¨Ånd the top-K documents zi. For Ô¨Ånal prediction y, we
treat z as a latent variable and marginalize over seq2seq predictions given different documents.
but have only explored open-domain extractive question answering. Here, we bring hybrid parametric
and non-parametric memory to the ‚Äúworkhorse of NLP ,‚Äù i.e. sequence-to-sequence (seq2seq) models.
We endow pre-trained, parametric-memory generation models with a non-parametric memory through
a general-purpose Ô¨Åne-tuning approach which we refer to as retrieval-augmented generation (RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the
non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural
retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The
retriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on
the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with
the input to generate the output. We marginalize the latent documents with a top-K approximation,
either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token
basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG
can be Ô¨Åne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.
There has been extensive previous work proposing architectures to enrich systems with non-parametric
memory which are trained from scratch for speciÔ¨Åc tasks, e.g. memory networks [ 64, 55], stack-
augmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both
parametric and non-parametric memory components are pre-trained and pre-loaded with extensive
knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is
present without additional training.
Our results highlight the beneÔ¨Åts of combining parametric and non-parametric memory with genera-
tion for knowledge-intensive tasks‚Äîtasks that humans could not reasonably be expected to perform
without access to an external knowledge source. Our RAG models achieve state-of-the-art results
on open Natural Questions [ 29], WebQuestions [3] and CuratedTrec [ 2] and strongly outperform
recent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being
extractive tasks, we Ô¨Ånd that unconstrained generation outperforms previous extractive approaches.
For knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question
generation, and we Ô¨Ånd that our models generate responses that are more factual, speciÔ¨Åc, and
diverse than a BART baseline. For FEVER [ 56] fact veriÔ¨Åcation, we achieve results within 4.3% of
state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that
the non-parametric memory can be replaced to update the models‚Äô knowledge as the world changes. 1
2 Methods
We explore RAG models, which use the input sequence x to retrieve text documents z and use them
as additional context when generating the target sequence y. As shown in Figure 1, our models
leverage two components: (i) a retriever p‚åò (z|x) with parameters ‚åò that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p‚úì(yi|x, z, y1:i  1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/
2
by ‚úì that generates a current token based on a context of the previous i   1 tokens y1:i  1, the original
input x and a retrieved passage z.
To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
p‚åò and p‚úì components, as well as the training and decoding procedure.
2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) ‚á°
X
z2top-k(p(¬∑|x))
p‚åò (z|x)p‚úì(y|x, z)=
X
z2top-k(p(¬∑|x))
p‚åò (z|x)
NY
i
p‚úì(yi|x, z, y1:i  1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne:
pRAG-Token(y|x) ‚á°
NY
i
X
z2top-k(p(¬∑|x))
p‚åò (z|x)p‚úì(yi|x, zi,y 1:i  1)
Finally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.
2.2 Retriever: DPR
The retrieval component p‚åò (z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
p‚åò (z|x) / exp
 
d(z)> q(x)
 
d(z)= BERTd(z), q(x)= BERTq (x)
where d(z) is a dense representation of a document produced by a BERT BASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERT BASE. Calculating
top-k(p‚åò (¬∑|x)), the list of k documents z with highest prior probability p‚åò (z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This
retriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and
Natural Questions [29]. We refer to the document index as the non-parametric memory.
2.3 Generator: BART
The generator component p‚úì(yi|x, z, y1:i  1) could be modelled using any encoder-decoder. We use
BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input
x with the retrieved content z when generating from BART, we simply concatenate them. BART was
pre-trained using a denoising objective and a variety of different noising functions. It has obtained
state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5
models [32]. We refer to the BART generator parameters ‚úì as the parametric memory henceforth.
2.4 Training
We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved. Given a Ô¨Åne-tuning training corpus of input/output pairs (xj ,y j ), we
3
minimize the negative marginal log-likelihood of each target, P
j   log p(yj |xj ) using stochastic
gradient descent with Adam [ 28]. Updating the document encoder BERTd during training is costly as
it requires the document index to be periodically updated as REALM does during pre-training [ 20].
We do not Ô¨Ånd this step necessary for strong performance, and keep the document encoder (and
index) Ô¨Åxed, only Ô¨Åne-tuning the query encoder BERT q and the BART generator.
2.5 Decoding
At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).
RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-
tor with transition probability: p0
‚úì(yi|x, y1:i  1)= P
z2top-k(p(¬∑|x)) p‚åò (zi|x)p‚úì(yi|x, zi,y 1:i  1) To
decode, we can plug p0
‚úì(yi|x, y1:i  1) into a standard beam decoder.
RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-
token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for
each document z, scoring each hypothesis using p‚úì(yi|x, z, y1:i  1). This yields a set of hypotheses
Y , some of which may not have appeared in the beams of all documents. To estimate the probability
of an hypothesis y we run an additional forward pass for each document z for which y does not
appear in the beam, multiply generator probability with p‚åò (z|x) and then sum the probabilities across
beams for the marginals. We refer to this decoding procedure as ‚ÄúThorough Decoding.‚Äù For longer
output sequences, |Y | can become large, requiring many forward passes. For more efÔ¨Åcient decoding,
we can make a further approximation that p‚úì(y|x, zi) ‚á° 0 where y was not generated during beam
search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has
been generated. We refer to this decoding procedure as ‚ÄúFast Decoding.‚Äù
3 Experiments
We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use
a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical
Navigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top
k documents for each query. We consider k 2{ 5, 10} for training and set k for test time using dev
data. We now discuss experimental details for each task.
3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [ 5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to ‚ÄúClosed-Book
QA ‚Äù approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [ 29], TriviaQA (TQA) [24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As
CT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.
3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAG‚Äôs natural language generation (NLG) in a knowledge-intensive setting,
we use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages
retrieved from a search engine for each question, and a full sentence answer annotated from the
retrieved passages. We do not use the supplied passages, only the questions and answers, to treat
4
MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages, such as
‚ÄúWhat is the weather in V olcano, CA?‚Äù so performance will be lower without using gold passages.
We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,
RAG can rely on parametric knowledge to generate reasonable responses.
3.3 Jeopardy Question Generation
To evaluate RAG‚Äôs generation abilities in a non-QA setting, we study open-domain question gen-
eration. Rather than use questions from standard open-domain QA tasks, which typically consist
of short, simple questions, we propose the more demanding task of generating Jeopardy questions.
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.
For example, ‚ÄúThe World Cup‚Äù is the answer to the question ‚ÄúIn 1986 Mexico scored as the Ô¨Årst
country to host this international sports competition twice.‚Äù As Jeopardy questions are precise,
factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledge-intensive generation task.
We use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As
this is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the
SQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics. We also perform two human evaluations, one to assess generation factuality, and
one for speciÔ¨Åcity. We deÔ¨Åne factuality as whether a statement can be corroborated by trusted external
sources, and speciÔ¨Åcity as high mutual dependence between the input and output [ 33]. We follow
best practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two
generated questions, one from BART and one from RAG. They are then asked to pick one of four
options‚Äîquuestion A is better, question B is better, both are good, or neither is good.
3.4 Fact V eriÔ¨Åcation
FEVER [ 56] requires classifying whether a natural language claim is supported or refuted by
Wikipedia, or whether there is not enough information to decide. The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true, false, or unveriÔ¨Åable from Wikipedia alone. FEVER is a retrieval problem
coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for
exploring the RAG models‚Äô ability to handle classiÔ¨Åcation rather than generation. We map FEVER
class labels (supports, refutes, or not enough info) to single output tokens and directly train with
claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on
retrieved evidence. In many real-world applications, retrieval supervision signals aren‚Äôt available, and
models that do not require such supervision will be applicable to a wider range of tasks. We explore
two variants: the standard 3-way classiÔ¨Åcation task (supports/refutes/not enough info) and the 2-way
(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
4 Results
4.1 Open-domain Question Answering
Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA
tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines
the generation Ô¨Çexibility of the ‚Äúclosed-book‚Äù (parametric only) approaches and the performance of
"open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results
without expensive, specialized ‚Äúsalient span masking‚Äù pre-training [ 20]. It is worth noting that RAG‚Äôs
retriever is initialized using DPR‚Äôs retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based ‚Äúcross-
encoder‚Äù to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
There are several advantages to generating answers even when it is possible to extract them. Docu-
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated, which is not possible with standard extractive approaches, leading
5
Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details.
Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiÔ¨Åcation Test Scores.
MS-MARCO SotA is [ 4], FEVER-3 is [ 68] and
FEVER-2 is [ 57] *Uses gold context/evidence.
Best model without gold access underlined.
Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc.
SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2
to more effective marginalization over documents. Furthermore, RAG can generate correct answers
even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such
cases for NQ, where an extractive model would score 0%.
4.2 Abstractive Question Answering
As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu
points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is
impressive given that (i) those models access gold passages with speciÔ¨Åc information required to
generate the reference answer, (ii) many questions are unanswerable without the gold passages, and
(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers
from our models. Qualitatively, we Ô¨Ånd that RAG models hallucinate less and generate factually
correct text more often than BART. Later, we also show that RAG generations are more diverse than
BART generations (see ¬ß4.5).
4.3 Jeopardy Question Generation
Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,
with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over
452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more
factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both
RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of
RAG on the task over a state-of-the-art generation model. Evaluators also Ô¨Ånd RAG generations to
be more speciÔ¨Åc by a large margin. Table 3 shows typical generations from each model.
Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform
best because it can generate responses that combine content from several documents. Figure 2 shows
an example. When generating ‚ÄúSun‚Äù, the posterior is high for document 2 which mentions ‚ÄúThe
Sun Also Rises‚Äù. Similarly, document 1 dominates the posterior when ‚ÄúA Farewell to Arms‚Äù is
generated. Intriguingly, after the Ô¨Årst token of each book is generated, the document posterior Ô¨Çattens.
This observation suggests that the generator can complete the titles without depending on speciÔ¨Åc
documents. In other words, the model‚Äôs parametric knowledge is sufÔ¨Åcient to complete the titles. We
Ô¨Ånd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The
Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun
Also Rises" indicating the title "The Sun Also Rises" is stored in BART‚Äôs parameters. Similarly,
BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A
with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows
how parametric and non-parametric memories work together‚Äîthe non-parametric component helps
to guide the generation, drawing out speciÔ¨Åc knowledge stored in the parametric memory.
4.4 Fact V eriÔ¨Åcation
Table 2 shows our results on FEVER. For 3-way classiÔ¨Åcation, RAG scores are within 4.3% of
state-of-the-art models, which are complex pipeline systems with domain-speciÔ¨Åc architectures and
substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.
6
Document 1: his works are considered classics of Americanliterature ... His wartime experiences formed the basis for his novel‚ÄùA Farewell to Arms‚Äù(1929) ...
Document 2: ... artists of the 1920s ‚ÄùLost Generation‚Äù expatriatecommunity . His debut novel,‚ÄùThe Sun Also Rises‚Äù, was publishedin 1926.
BOS‚ÄùTheSunAlsoRises‚Äùisa
novelbythisauthor
of‚ÄùAFarewelltoArms
‚Äù
Doc 1
Doc 2
Doc 3
Doc 4
Doc 5
Figure 2: RAG-Token document posterior p(zi|x, yi,y   i) for each generated token for input ‚ÄúHem-
ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high
when generating ‚ÄúA Farewell to Arms" and for document 2 when generating ‚ÄúThe Sun Also Rises".
Table 3: Examples from generation tasks. RAG models generate more speciÔ¨Åc and factually accurate
responses. ‚Äò?‚Äô indicates factually incorrect responses, * indicates partially correct responses.
Task Input Model Generation
MS-
MARCO
deÔ¨Åne middle
ear
BART ?The middle ear is the part of the ear between the middle ear and the nose.
RAG-T The middle ear is the portion of the ear internal to the eardrum.
RAG-S The middle ear includes the tympanic cavity and the three ossicles.
what currency
needed in
scotland
BART The currency needed in Scotland is Pound sterling.
RAG-T Pound is the currency needed in Scotland.
RAG-S The currency needed in Scotland is the pound sterling.
Jeopardy
Question
Gener
-ation
Washington
BART ?This state has the largest number of counties in the U.S.
RAG-T It‚Äôs the only U.S. state named for a U.S. president
RAG-S It‚Äôs the state where you‚Äôll Ô¨Ånd Mount Rainier National Park
The Divine
Comedy
BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio
RAG-T Dante‚Äôs "Inferno" is the Ô¨Årst part of this epic poem
RAG-S This 14th century work is divided into 3 sections: "Inferno", "Purgatorio" & "Paradiso"
For 2-way classiÔ¨Åcation, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]
to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy
within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.
We also analyze whether documents retrieved by RAG correspond to documents annotated as gold
evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved
by RAG and gold evidence annotations. We Ô¨Ånd that the top retrieved document is from a gold article
in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.
4.5 Additional Results
Generation Diversity Section 4.3 shows that RAG models are more factual and speciÔ¨Åc than
BART for Jeopardy question generation. Following recent work on diversity-promoting decoding
[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to
total ngrams generated by different models. Table 5 shows that RAG-Sequence‚Äôs generations are
more diverse than RAG-Token‚Äôs, and both are signiÔ¨Åcantly more diverse than BART without needing
any diversity-promoting decoding.
Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.
To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever
during training. As shown in Table 6, learned retrieval improves results for all tasks. We compare
RAG‚Äôs dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG‚Äôs retriever
with a Ô¨Åxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table
6 show the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily
entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves
results on all other tasks, especially for Open-Domain QA, where it is crucial.
Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge
can be easily updated at test time. Parametric-only models like T5 or BART need further training to
update their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5]
Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer
index from our main results (December 2018). We prepare a list of 82 world leaders who had changed
between these dates and use a template ‚ÄúWho is {position}?‚Äù (e.g. ‚ÄúWho is the President of Peru?‚Äù)
7
Table 4: Human assessments for the Jeopardy
Question Generation Task.
Factuality SpeciÔ¨Åcity
BART better 7.1% 16.8%
RAG better 42.7% 37.4%
Both good 11.7% 11.8%
Both poor 17.7% 6.9%
No majority 20.8% 20.1%
Table 5: Ratio of distinct to total tri-grams for
generation tasks.
MSMARCO Jeopardy QGen
Gold 89.6% 90.0%
BART 70.7% 32.4%
RAG-Token 77.8% 46.8%
RAG-Seq. 83.5% 53.8%
Table 6: Ablations on the dev set. As FEVER is a classiÔ¨Åcation task, both RAG models are equivalent.
Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2
Exact Match B-1 QB-1 R-L B-1 Label Accuracy
RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9
RAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3
RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5
to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for
2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched
indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).
This shows we can update RAG‚Äôs world knowledge by simply replacing its non-parametric memory.
Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent
documents, and we do not observe signiÔ¨Åcant differences in performance between them. We have the
Ô¨Çexibility to adjust the number of retrieved documents at test time, which can affect performance and
runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves
Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved
documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for
RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.
10 20 30 40 50KR e t r i e v e dD o c s
39
40
41
42
43
44NQ Exact Match RAG-TokRAG-Seq
10 20 30 40 50KR e t r i e v e dD o c s
40
50
60
70
80NQ Answer Recall @ K
RAG-TokRAG-SeqFixed DPRBM25
10 20 30 40 50KR e t r i e v e dD o c s
48
50
52
54
56Bleu-1 / Rouge-L score
RAG-Tok R-LRAG-Tok B-1RAG-Seq R-LRAG-Seq B-1
Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-
mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.
5 Related Work
Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of
NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5, 29],
fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article
generation [ 36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our
work uniÔ¨Åes previous successes in incorporating retrieval into individual tasks, showing that a single
retrieval-based architecture is capable of achieving strong performance across several tasks.
8
General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP
tasks has shown great success without the use of retrieval. A single, pre-trained language model
has been shown to achieve strong performance on various classiÔ¨Åcation tasks in the GLUE bench-
marks [60, 61] after Ô¨Åne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained
language model could achieve strong performance across both discriminative and generative tasks.
For further improvement, BART [32] and T5 [ 51, 52] propose a single, pre-trained encoder-decoder
model that leverages bi-directional attention to achieve stronger performance on discriminative
and generative tasks. Our work aims to expand the space of possible tasks with a single, uniÔ¨Åed
architecture, by learning a retrieval module to augment pre-trained, generative language models.
Learned Retrieval There is signiÔ¨Åcant work on learning to retrieve documents in information
retrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some
work optimizes the retrieval module to aid in a speciÔ¨Åc, downstream task such as question answering,
using search [ 46], reinforcement learning [ 6, 63, 62], or a latent variable approach [ 31, 20] as in our
work. These successes leverage different retrieval-based architectures and optimization techniques to
achieve strong performance on a single task, while we show that a single retrieval-based architecture
can be Ô¨Åne-tuned for strong performance on a variety of tasks.
Memory-based Architectures Our document index can be seen as a large external memory for
neural networks to attend to, analogous to memory networks [ 64, 55]. Concurrent work [ 14] learns
to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our
work. Other work improves the ability of dialog models to generate factual text by attending over
fact embeddings [9, 13] or, closer to our work, over retrieved text directly [ 15]. A key feature of our
memory is that it is comprised of raw text rather distributed representations, which makes the memory
both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable,
enabling us to dynamically update the model‚Äôs memory by editing the document index.
Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style
approaches, where a similar training input-output pair is retrieved for a given input, and then edited
to provide a Ô¨Ånal output. These approaches have proved successful in a number of domains including
Machine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,
including less of emphasis on lightly editing a retrieved item, but on aggregating content from several
pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents
rather than related training pairs. This said, RAG techniques may work well in these settings, and
could represent promising future work.
6 Discussion
In this work, we presented hybrid generation models with access to parametric and non-parametric
memory. We showed that our RAG models obtain state of the art results on open-domain QA. We
found that people prefer RAG‚Äôs generation over purely parametric BART, Ô¨Ånding RAG more factual
and speciÔ¨Åc. We conducted an thorough investigation of the learned retrieval component, validating
its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model
without requiring any retraining. In future work, it may be fruitful to investigate if the two components
can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some
another objective. Our work opens up new research directions on how parametric and non-parametric
memories interact and how to most effectively combine them, showing promise in being applied to a
wide variety of NLP tasks.
9
Broader Impact
This work offers several positive societal beneÔ¨Åts over previous work: the fact that it is more
strongly grounded in real factual knowledge (in this case Wikipedia) makes it ‚Äúhallucinate‚Äù less
with generations that are more factual, and offers more control and interpretability. RAG could be
employed in a wide variety of scenarios with direct beneÔ¨Åt to society, for example by endowing it
with a medical index and asking it open-domain questions on that topic, or by helping people be more
effective at their jobs.
With these advantages also come potential downsides: Wikipedia, or any potential external knowledge
source, will probably never be entirely factual and completely devoid of bias. Since RAG can be
employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably
to a lesser extent, including that it might be used to generate abuse, faked or misleading content in
the news or on social media; to impersonate others; or to automate the production of spam/phishing
content [ 54]. Advanced language models may also lead to the automation of various jobs in the
coming decades [ 16]. In order to mitigate these risks, AI systems could be employed to Ô¨Åght against
misleading content and automated spam/phishing.
Acknowledgments
The authors would like to thank the reviewers for their thoughtful and constructive feedback on this
paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors
would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice.
Funding Disclosure
EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD
program. This work was funded by Facebook.
References
[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan
Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina
Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:
//arxiv.org/abs/1611.09268. arXiv: 1611.09268.
[2] Petr Baudi≈° and Jan ≈†ediv `y. Modeling of the question answering task in the yodaqa system. In
International Conference of the Cross-Language Evaluation F orum for European Languages,
pages 222‚Äì228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%
2F978-3-319-24027-5_20 .
[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase
from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1533‚Äì1544, Seattle, Washington, USA, October 2013.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/
D13-1160.
[4] Bin Bi, Chenliang Li, Chen Wu, Ming Y an, and Wei Wang. Palm: Pre-training an autoencod-
ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,
2020. URL https://arxiv.org/abs/2004.07159.
[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer
Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (V olume 1: Long Papers), pages 1870‚Äì1879, V ancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL
https://www.aclweb.org/anthology/P17-1171.
[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and
Jonathan Berant. Coarse-to-Ô¨Åne question answering for long documents. In Proceedings of the
10
55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers),
pages 209‚Äì220, V ancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.
[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-
hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.
arXiv: 1710.10723.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, V olume 1 (Long and Short Papers), pages 4171‚Äì4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19-1423.
[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-
ard of wikipedia: Knowledge-powered conversational agents. In International Conference on
Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.
[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun
Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.
arXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:
1704.05179.
[11] Angela Fan, Mike Lewis, and Y ann Dauphin. Hierarchical neural story generation. In Proceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1:
Long Papers), pages 889‚Äì898, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/
P18-1082.
[12] Angela Fan, Y acine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3558‚Äì3567, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/
anthology/P19-1346.
[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers
with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=
H1gx1CNKPH.
[14] Thibault F√©vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.
Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,
2020. URL https://arxiv.org/abs/2004.07202.
[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen
tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI
Conference on ArtiÔ¨Åcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/16710.
[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI
exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL
http://arxiv.org/abs/1705.08807.
[17] Jiatao Gu, Y ong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In AAAI Conference on ArtiÔ¨Åcial Intelligence , 2018. URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.
[18] Jiatao Gu, Y ong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In 32nd AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018 , 32nd
AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018, pages 5133‚Äì5140. AAAI press, 2018.
32nd AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018
Through 07-02-2018.
11
[19] Kelvin Guu, Tatsunori B. Hashimoto, Y onatan Oren, and Percy Liang. Generating sentences by
editing prototypes. Transactions of the Association for Computational Linguistics , 6:437‚Äì450,
2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.
[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:
Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:
//arxiv.org/abs/2002.08909.
[21] Tatsunori B Hashimoto, Kelvin Guu, Y onatan Oren, and Percy S Liang. A
retrieve-and-edit framework for predicting structured outputs. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-
itors, Advances in Neural Information Processing Systems 31 , pages 10052‚Äì
10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.
pdf.
[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-
edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2532‚Äì2538, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/
anthology/2020.acl-main.228.
[23] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with gpus. arXiv
preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.
[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale
Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers),
pages 1601‚Äì1611, V ancouver, Canada, July 2017. Association for Computational Linguistics.
doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.
[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-
augmented recurrent nets. In Proceedings of the 28th International Conference on
Neural Information Processing Systems - V olume 1 , NIPS‚Äô15, page 190‚Äì198, Cam-
bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/
5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .
[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint
arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.
[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-
tion through memorization: Nearest neighbor language models. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.
[28] Diederik P . Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Y oshua
Bengio and Y ann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL
http://arxiv.org/abs/1412.6980.
[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-
ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-
tion Answering Research. Transactions of the Association of Computational Lin-
guistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/
natural-questions/main-1455-kwiatkowski.pdf .
[30] Guillaume Lample, Alexandre Sablayrolles, Marc‚Äô Aurelio Ranzato, Ludovic Denoyer, and
Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d‚Äô Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 32, pages 8548‚Äì8559. Curran Associates, Inc., 2019. URL http:
//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .
12
[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
open domain question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6086‚Äì6096, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/
anthology/P19-1612.
[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, V eselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.
[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 110‚Äì119, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/
N16-1014.
[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation
with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL
https://arxiv.org/abs/1909.03087.
[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine
translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics , pages 3044‚Äì3049, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL
https://www.aclweb.org/anthology/P19-1291.
[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International
Conference on Learning Representations , 2018. URL https://openreview.net/forum?
id=Hyg0vbWC-.
[37] Y ury A. Malkov and D. A. Y ashunin. EfÔ¨Åcient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 42:824‚Äì836, 2016. URL https://arxiv.org/abs/1603.09320.
[38] Gary Marcus. The next decade in ai: four steps towards robust artiÔ¨Åcial intelligence. arXiv
preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.
[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt√§schel, V assilis
Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the
veriÔ¨Åability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:
//arxiv.org/abs/1911.03587.
[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh V enkatesh, and Hao Wu. Mixed
precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.
[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-
ing background knowledge for building conversation systems. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing , pages 2322‚Äì2332, Brus-
sels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.
[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation
systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pages 3950‚Äì3959, Brussels, Belgium, October-November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/
anthology/D18-1429.
13
[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In
Tarek Richard Besold, Antoine Bordes, Artur S. d‚ÄôAvila Garcez, and Greg Wayne, editors,
Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic
approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing
Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 , volume 1773 of CEUR Workshop
Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_
2016_paper9.pdf.
[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint
arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.
[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics (Demonstrations), pages 48‚Äì53, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.
org/anthology/N19-4009.
[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun
Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages
2402‚Äì2411, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.
[47] Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Y uxiang Wu,
and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2463‚Äì2473, Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1250. URL https://www.aclweb.org/anthology/D19-1250.
[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt√§schel, Y uxiang Wu, Alexander H.
Miller, and Sebastian Riedel. How context affects language models‚Äô factual predictions. In
Automated Knowledge Base Construction , 2020. URL https://openreview.net/forum?
id=025X0zPfn.
[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-
proving Language Understanding by Generative Pre-Training, 2018. URL
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf.
[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners, 2019. URL
https://d4mucfpksywv.cloudfront.net/better-language-models/language_
models_are_unsupervised_multitask_learners.pdf.
[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Y anqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniÔ¨Åed
text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.
[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into
the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/
2002.08910.
[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and
beyond. F ound. Trends Inf. Retr ., 3(4):333‚Äì389, April 2009. ISSN 1554-0669. doi: 10.1561/
1500000019. URL https://doi.org/10.1561/1500000019.
[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec
Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models.
ArXiv, abs/1908.09203, 2019.
14
[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances
in Neural Information Processing Systems 28 , pages 2440‚Äì2448. Curran Associates, Inc., 2015.
URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .
[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a
large-scale dataset for fact extraction and VERiÔ¨Åcation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, V olume 1 (Long Papers), pages 809‚Äì819, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://www.aclweb.org/anthology/N18-1074.
[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model
biases in sentence-pair classiÔ¨Åcation with elastic weight consolidation. ArXiv, abs/2004.14366,
2020. URL https://arxiv.org/abs/2004.14366.
[58] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30 , pages 5998‚Äì6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .
[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.
AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/17329.
[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP , pages 353‚Äì355, Brussels, Belgium, November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/
anthology/W18-5446.
[61] Alex Wang, Y ada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-
Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d\textquotesingle Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems 32 , pages 3261‚Äì3275. Curran Associates, Inc., 2019. URL https://
arxiv.org/abs/1905.00537.
[62] Shuohang Wang, Mo Y u, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerry Tesauro, Bowen Zhou, and Jing Jiang. R 3: Reinforced ranker-reader for open-domain
question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of
the Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-18), the 30th innovative
Applications of ArtiÔ¨Åcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in ArtiÔ¨Åcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018, pages 5981‚Äì5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16712.
[63] Shuohang Wang, Mo Y u, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,
Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-
ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.
net/forum?id=rJl3yM-Ab.
[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Y oshua Bengio
and Y ann LeCun, editors, 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL
http://arxiv.org/abs/1410.3916.
[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reÔ¨Åne: Improved sequence
generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd
15
International Workshop on Search-Oriented Conversational AI, pages 87‚Äì92, Brussels, Belgium,
October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL
https://www.aclweb.org/anthology/W18-5713.
[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Y acine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface‚Äôs transformers:
State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-
supervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) , pages 2495‚Äì2509, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL
https://www.aclweb.org/anthology/D19-1253.
[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and
Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.
URL https://arxiv.org/abs/1909.03745.
16


=== Toolformer-paper.pdf ===
Toolformer: Language Models Can Teach Themselves to Use Tools
Timo Schick Jane Dwivedi-Yu Roberto Dess√¨ ‚Ä† Roberta Raileanu
Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom
Meta AI Research ‚Ä†Universitat Pompeu Fabra
Abstract
Language models (LMs) exhibit remarkable
abilities to solve new tasks from just a few
examples or textual instructions, especially at
scale. They also, paradoxically, struggle with
basic functionality, such as arithmetic or fac-
tual lookup, where much simpler and smaller
models excel. In this paper, we show that
LMs can teach themselves touse external tools
via simple APIs and achieve the best of both
worlds. We introduce Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriÔ¨Åc-
ing its core language modeling abilities.
1 Introduction
Large language models achieve impressive zero-
and few-shot results on a variety of natural lan-
guage processing tasks (Brown et al., 2020; Chowd-
hery et al., 2022, i.a.) and show several emergent
capabilities (Wei et al., 2022). However, all of
these models have several inherent limitations that
can at best be partially addressed by further scal-
ing. These limitations include an inability to access
up-to-date information on recent events (Komeili
et al., 2022) and the related tendency to hallucinate
facts (Maynez et al., 2020; Ji et al., 2022), difÔ¨Åcul-
ties in understanding low-resource languages (Lin
et al., 2021), a lack of mathematical skills to per-
form precise calculations (Patel et al., 2021) and an
unawareness of the progression of time (Dhingra
et al., 2022).
The New England Journal of Medicine is a registered 
trademark of [QA(‚ÄúWho is the publisher of The New 
England Journal of Medicine?‚Äù) ‚Üí Massachusetts 
Medical Society] the MMS.
Out of 1400 participants, 400 (or [Calculator(400 / 1400) 
‚Üí 0.29] 29%) passed the test. 
The name derives from ‚Äúla tortuga‚Äù, the Spanish word for 
[MT(‚Äútortuga‚Äù) ‚Üí turtle] turtle.
The Brown Act is California‚Äôs law [WikiSearch(‚ÄúBrown 
Act‚Äù) ‚Üí The Ralph M. Brown Act is an act of the 
California State Legislature that guarantees the public's 
right to attend and participate in meetings of local 
legislative bodies.] that requires legislative bodies, like 
city councils, to hold their meetings open to the public.
Figure 1: Exemplary predictions of Toolformer. The
model autonomously decides to call different APIs
(from top to bottom: a question answering system,
a calculator, a machine translation system, and a
Wikipedia search engine) to obtain information that is
useful for completing a piece of text.
A simple way to overcome these limitations of
today‚Äôs language models is to give them the abil-
ity to use external tools such as search engines,
calculators, or calendars. However, existing ap-
proaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-speciÔ¨Åc settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), hinder-
ing a more widespread adoption of tool use in LMs.
Therefore, we propose Toolformer, a model that
learns to use tools in a novel way, which fulÔ¨Ålls the
following desiderata:
‚Ä¢ The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. This is impor-
arXiv:2302.04761v1  [cs.CL]  9 Feb 2023
x1:i-1  = Pittsburgh is 
             also known as
   xi:n = the Steel City
x* = Pittsburgh is 
        also known as
        [QA(What ‚Ä¶? 
        ‚Üí Steel City)] 
        the Steel City.
ci
1 = What other name is 
         Pittsburgh known by?
ci
2 = Which country is
         Pittsburgh in?
ri
1 = Steel City 
ri
2 = United States
Li(ci
1 ‚Üí Steel City)
 < min(Li(ci
1 ‚Üí Œµ), Li(Œµ))
Li(ci
2 ‚Üí United States)
 > min(Li(ci
2 ‚Üí Œµ), Li(Œµ))
1 
Sample API Calls
2 
Execute API Calls
3 
Filter API CallsLM Dataset LM Dataset 
with API Calls
Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we Ô¨Årst
sample a position i and corresponding API call candidates c1
i,c 2
i,...,c k
i . We then execute these API calls and
Ô¨Ålter out all calls which do not reduce the loss Li over the next tokens. All remaining API calls are interleaved
with the original text, resulting in a new text x‚àó.
tant not only because of the costs associated
with such annotations, but also because what
humans Ô¨Ånd useful may be different from
what a model Ô¨Ånds useful.
‚Ä¢ The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. In contrast to
existing approaches, this enables a much more
comprehensive use of tools that is not tied to
speciÔ¨Åc tasks.
Our approach for achieving these goals is based
on the recent idea of using large LMs with in-
context learning (Brown et al., 2020) to generate
entire datasets from scratch (Schick and Sch√ºtze,
2021b; Honovich et al., 2022; Wang et al., 2022):
Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
Ô¨Ånetune the LM itself on the API calls that it con-
siders useful. As illustrated in Figure 1, through
this simple approach, LMs can learn to control a va-
riety of tools, and to choose for themselves which
tool to use when and how.
As our approach is agnostic of the dataset be-
ing used, we can apply it to the exact same dataset
that was used to pretrain a model in the Ô¨Årst place.
This ensures that the model does not lose any
of its generality and language modeling abilities.
We conduct experiments on a variety of differ-
ent downstream tasks, demonstrating that after
learning to use tools, Toolformer, which is based
on a pretrained GPT-J model (Wang and Komat-
suzaki, 2021) with 6.7B parameters, achieves much
stronger zero-shot results, clearly outperforming a
much larger GPT-3 model (Brown et al., 2020) and
several other baselines on various tasks.
2 Approach
Our aim is to equip a language modelM with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call.
We represent each API call as a tuplec = (ac,i c)
whereac is the name of the API andic is the cor-
responding input. Given an API call c with a cor-
responding result r, we denote the linearized se-
quences of the API call not including and including
its result, respectively, as:
e(c) = <API>ac(ic) </API>
e(c,r ) = <API>ac(ic)‚Üír </API>
where ‚Äú <API>‚Äù, ‚Äú </API>‚Äù and ‚Äú‚Üí‚Äù are special
tokens.1 Some examples of linearized API calls
inserted into text sequences are shown in Figure 1.
Given a dataset C = {x1,..., x|C|} of plain
texts, we Ô¨Årst convert this dataset into a dataset
C‚àó augmented with API calls. This is done in three
steps, illustrated in Figure 2: First, we exploit the
in-context learning ability ofM to sample a large
number of potential API calls. We then execute
these API calls and Ô¨Ånally check whether the ob-
tained responses are helpful for predicting future
tokens; this is used as a Ô¨Åltering criterion. After
Ô¨Åltering, we merge API calls for different tools,
resulting in the augmented datasetC‚àó, and Ô¨Ånetune
1In practice, we use the token sequences ‚Äú [‚Äù, ‚Äú ]‚Äù and
‚Äú ->‚Äù to represent ‚Äú <API>‚Äù, ‚Äú </API>‚Äù and ‚Äú ‚Üí‚Äù, respec-
tively. This enables our approach to work without modifying
the existing LM‚Äôs vocabulary. For reasons of readability, we
still refer to them as ‚Äú <API>‚Äù, ‚Äú </API>‚Äù and ‚Äú‚Üí‚Äù through-
out this section.
Your task is to add calls to a Question Answering API to a 
piece of text. The questions should help you get 
information required to complete the text. You can call the 
API by writing "[QA(question)]" where "question" is the 
question you want to ask. Here are some examples of API 
calls:
Input: Joe Biden was born in Scranton, Pennsylvania.
Output: Joe Biden was born in [QA("Where was Joe 
Biden born?")] Scranton, [QA("In which state is 
Scranton?")] Pennsylvania.
Input: Coca-Cola, or Coke, is a carbonated soft drink 
manufactured by the Coca-Cola Company.
Output: Coca-Cola, or [QA("What other name is 
Coca-Cola known by?")] Coke, is a carbonated soft drink 
manufactured by [QA("Who manufactures Coca-Cola?")] 
the Coca-Cola Company.
Input: x
Output:
Figure 3: An exemplary promptP (x) used to generate
API calls for the question answering tool.
M itself on this dataset. Each of these steps is
described in more detail below.
Sampling API Calls For each API, we write a
prompt P (x) that encourages the LM to anno-
tate an example x = x1,...,x n with API calls.
An example of such a prompt for a question an-
swering tool is shown in Figure 3; all prompts
used are shown in Appendix A.2. Let pM (zn+1|
z1,...,z n) be the probability that M assigns to
token zn+1 as a continuation for the sequence
z1,...,z n. We Ô¨Årst sample up tok candidate posi-
tions for doing API calls by computing, for each
i‚àà{ 1,...,n }, the probability
pi =pM (<API>|P (x),x 1:i‚àí1)
thatM assigns to starting an API call at position
i. Given a sampling threshold œÑs, we keep all po-
sitionsI ={i|pi>œÑ s}; if there are more thank
such positions, we only keep the topk.
For each positioni‚ààI, we then obtain up tom
API callsc1
i,...,c m
i by sampling fromM given the
sequence [P (x),x 1,...,x i‚àí1, <API>] as a preÔ¨Åx
and </API> as an end-of-sequence token.2
2We discard all examples whereM does not generate the
</API> token.
Executing API Calls As a next step, we execute
all API calls generated by M to obtain the corre-
sponding results. How this is done depends entirely
on the API itself ‚Äì for example, it can involve call-
ing another neural network, executing a Python
script or using a retrieval system to perform search
over a large corpus. The response for each API call
ci needs to be a single text sequenceri.
Filtering API Calls Leti be the position of the
API callci in the sequence x =x1,...,x n, and let
ri be the response from the API. Further, given a
sequence (wi|i‚àà N) of weights, let
Li(z) =‚àí
n‚àë
j=i
wj‚àíi¬∑ logpM (xj| z,x 1:j‚àí1)
be the weighted cross entropy loss forM over the
tokensxi,...,x n if the model is preÔ¨Åxed with z.
We compare two different instantiations of this loss:
L+
i =Li(e(ci,r i))
L‚àí
i = min (Li(Œµ),L i(e(ci,Œµ )))
whereŒµ denotes an empty sequence. The former is
the weighted loss over all tokensxi,...,x n if the
API call and its result are given toM as a preÔ¨Åx;3
the latter is the minimum of the losses obtained
from (i) doing no API call at all and (ii) doing an
API call, but not providing the response. Intuitively,
an API call is helpful toM if providing it with both
the input and the output of this call makes it easier
for the model to predict future tokens, compared to
not receiving the API call at all, or receiving only
its input. Given a Ô¨Åltering threshold œÑf , we thus
only keep API calls for which
L‚àí
i ‚àíL+
i ‚â•œÑf
holds, i.e., adding the API call and its resultreduces
the loss by at leastœÑf , compared to not doing any
API call or obtaining no result from it.
Model Finetuning After sampling and Ô¨Åltering
calls for all APIs, we Ô¨Ånally merge the remaining
API calls and interleave them with the original
inputs. That is, for an input text x = x1,...,x n
with a corresponding API call and result (ci,r i) at
positioni, we construct the new sequence x‚àó =
3We provide e(ci,r i) as a preÔ¨Åx instead of inserting it at
positioni becauseM is not yet Ô¨Ånetuned on any examples
containing API calls, so inserting it in the middle of x would
interrupt the Ô¨Çow and not align with patterns in the pretraining
corpus, thus hurting perplexity.
x1:i‚àí1, e(ci,r i),x i:n; we proceed analogously for
texts with multiple API calls. Doing this for allx‚àà
C results in the new datasetC‚àó augmented with API
calls. We use this new dataset to Ô¨ÅnetuneM, using
a standard language modeling objective. Crucially,
apart from inserted API calls the augmented dataset
C‚àó contains the exact same texts asC, the original
dataset. As a consequence, Ô¨Ånetuning M onC‚àó
exposes it to the same content as Ô¨Ånetuning onC.
Moreover, as API calls are inserted in exactly those
positions and with exactly those inputs that help
M predict future tokens, Ô¨Ånetuning onC‚àó enables
the language model to decide when and how to use
which tool, based purely on its own feedback.
Inference When generating text with M after
Ô¨Ånetuning with our approach, we perform regular
decoding untilM produces the ‚Äú‚Üí‚Äù token, indicat-
ing that it next expects the response for an API call.
At this point, we interrupt the decoding process,
call the appropriate API to get a response, and con-
tinue the decoding process after inserting both the
response and the </API> token.
3 Tools
We explore a variety of tools to address different
shortcomings of regular LMs. The only constraints
we impose on these tools is that (i) both their inputs
and outputs can be represented as text sequences,
and (ii) we can obtain a few demonstrations of
their intended use. Concretely, we explore the fol-
lowing Ô¨Åve tools: a question answering system, a
Wikipedia search engine, a calculator, a calendar,
and a machine translation system. Some examples
of potential calls and return strings for the APIs
associated with each of these tools are shown in
Table 1. We brieÔ¨Çy discuss all tools below; further
details can be found in Appendix A.
Question Answering Our Ô¨Årst tool is a question
answering system based on another LM that can an-
swer simple factoid questions. SpeciÔ¨Åcally, we use
Atlas (Izacard et al., 2022), a retrieval-augmented
LM Ô¨Ånetuned on Natural Questions (Kwiatkowski
et al., 2019).
Calculator As a second tool, we use a calculator
that can perform simple numeric calculations; we
only support the four basic arithmetic operations.
Results are always rounded to two decimal places.
Wikipedia Search Our third tool is a search en-
gine that, given a search term, returns short text
snippets from Wikipedia. Compared to our ques-
tion answering tool, this search enables a model
to get more comprehensive information on a sub-
ject, but requires it to extract the relevant parts by
itself. As our search engine, we use a BM25 re-
triever (Robertson et al., 1995; Baeza-Yates et al.,
1999) that indexes the Wikipedia dump from KILT
(Petroni et al., 2021).
Machine Translation System Our fourth tool is
a machine translation system based on a LM that
can translate a phrase from any language into En-
glish. More concretely, we use the 600M parameter
NLLB (Costa-juss√† et al., 2022) as our multilingual
machine translation model that works for 200 lan-
guages (including low-resource ones). The source
language is automatically detected using the fast-
Text classiÔ¨Åer (Joulin et al., 2016), while the target
language is always set to English.
Calendar Our Ô¨Ånal tool is a calendar API that,
when queried, returns the current date without tak-
ing any input. This provides temporal context for
predictions that require some awareness of time.
4 Experiments
We investigate whether our approach enables a
model to use tools without any further supervision
and to decide for itself when and how to call which
of the available tools. To test this, we select a vari-
ety of downstream tasks where we assume at least
one of the considered tools to be useful, and evalu-
ate performance in zero-shot settings (Section 4.2).
Beyond that, we also ensure that our approach does
not hurt the model‚Äôs core language modeling abili-
ties; we verify this by looking at perplexity on two
language modeling datasets (Section 4.3). Finally,
we investigate how the ability to learn using tools
is affected by model size (Section 4.4).
4.1 Experimental Setup
Dataset Generation Throughout all of our ex-
periments, we use a subset of CCNet (Wenzek et al.,
2020) as our language modeling datasetC and GPT-
J (Wang and Komatsuzaki, 2021) as our language
model M. To reduce the computational cost of
annotatingC with API calls, we deÔ¨Åne heuristics
for some APIs to get a subset ofC for which API
calls are more likely to be helpful than for an av-
erage text. For example, we only consider texts
for the calculator tool if they contain at least three
numbers. Details of the heuristics used are given in
API Name Example Input Example Output
Question Answering Where was the Knights
of Columbus founded?
New Haven, Connecticut
Wikipedia Search Fishing Reel Types Spin Ô¨Åshing > Spin Ô¨Åshing is distinguished between Ô¨Çy Ô¨Åshing and bait
cast Ô¨Åshing by the type of rod and reel used. There are two types of reels
used when spin Ô¨Åshing, the open faced reel and the closed faced reel.
Calculator 27 + 4 * 2 35
Calendar Œµ Today is Monday, January 30, 2023.
Machine Translation s√ªret√© nucl√©aire nuclear safety
Table 1: Examples of inputs and outputs for all APIs used.
Number of Examples
API œÑf = 0.5 œÑf = 1.0 œÑf = 2.0
Question Answering 51,987 18,526 5,135
Wikipedia Search 207,241 60,974 13,944
Calculator 3,680 994 138
Calendar 61,811 20,587 3,007
Machine Translation 3,156 1,034 229
Table 2: Number of examples with API calls in C‚àó for
different values of our Ô¨Åltering thresholdœÑf .
Appendix A. For obtainingC‚àó fromC, we perform
all steps described in Section 2 and additionally
Ô¨Ålter out all examples for which all API calls were
eliminated in the Ô¨Åltering step.4 For the weighting
function, we use
wt = Àúwt‚àë
s‚ààN Àúws
with Àúwt = max(0, 1‚àí 0.2¬∑t)
to make sure that API calls happen close to where
the information provided by the API is actually
helpful for the model. The thresholdsœÑs andœÑf are
chosen individually for each tool to ensure a sufÔ¨Å-
ciently larger number of examples; see Appendix A
for details. Table 2 shows relevant statistics of our
Ô¨Ånal dataset augmented with API calls.
Model Finetuning We Ô¨ÅnetuneM onC‚àó using
a batch size of 128 and a learning rate of 1¬∑ 10‚àí5
with linear warmup for the Ô¨Årst 10% of training.
Details of our Ô¨Ånetuning procedure are given in
Appendix B.
Baseline Models Throughout the remainder of
this section, we mainly compare the following mod-
els:
4While this Ô¨Åltering alters the distribution of training exam-
ples, we assume that the remaining examples are close enough
to the original distribution so that M‚Äôs language modeling
abilities remain unaffected. This assumption is empirically
validated in Section 4.3.
‚Ä¢ GPT-J: A regular GPT-J model without any
Ô¨Ånetuning.
‚Ä¢ GPT-J + CC: GPT-J Ô¨Ånetuned onC, our sub-
set of CCNet without any API calls.
‚Ä¢ Toolformer: GPT-J Ô¨Ånetuned onC‚àó, our sub-
set of CCNet augmented with API calls.
‚Ä¢ Toolformer (disabled): The same model as
Toolformer, but API calls are disabled during
decoding.5
For most tasks, we additionally compare to OPT
(66B) (Zhang et al., 2022) and GPT-3 6 (175B)
(Brown et al., 2020), two models that are about
10 and 25 times larger than our other baseline mod-
els, respectively.
4.2 Downstream Tasks
We evaluate all models on a variety of downstream
tasks. In all cases, we consider a prompted zero-
shot setup ‚Äì i.e., models are instructed to solve
each task in natural language, but we do not pro-
vide any in-context examples. This is in contrast
to prior work on tool use (e.g., Gao et al., 2022;
Parisi et al., 2022), where models are provided
with dataset-speciÔ¨Åc examples of how a tool can be
used to solve a concrete task. We choose the more
challenging zero-shot setup as we are interested
in seeing whether Toolformer works in precisely
those cases where a user does not specify in ad-
vance which tools should be used in which way for
solving a speciÔ¨Åc problem.
We use standard greedy decoding, but with one
modiÔ¨Åcation for Toolformer: We let the model start
an API call not just when <API> is the most likely
5This is achieved by manually setting the probability of
the <API> token to 0.
6We use the originaldavinci variant that is not Ô¨Ånetuned
on any instructions.
token, but whenever it is one of thek most likely
tokens. For k = 1 , this corresponds to regular
greedy decoding; we instead use k = 10 to in-
crease the disposition of our model to make use of
the APIs that it has access to. At the same time,
we only at most one API call per input to make
sure the model does not get stuck in a loop where
it constantly calls APIs without producing any ac-
tual output. The effect of these modiÔ¨Åcations is
explored in Section 5.
4.2.1 LAMA
We evaluate our models on the SQuAD, Google-
RE and T-REx subsets of the LAMA benchmark
(Petroni et al., 2019). For each of these subsets, the
task is to complete a short statement with a miss-
ing fact (e.g., a date or a place). As LAMA was
originally designed to evaluate masked language
models (e.g., Devlin et al., 2019), we Ô¨Ålter out ex-
amples where the mask token is not the Ô¨Ånal token,
so that the remaining examples can be processed
in a left-to-right fashion. To account for different
tokenizations and added complexity from not in-
forming the model that a single word is required,
we use a slightly more lenient evaluation criterion
than exact match and simply check whether the
correct word is within the Ô¨Årst Ô¨Åve words predicted
by the model. As LAMA is based on statements
obtained directly from Wikipedia, we prevent Tool-
former from using the Wikipedia Search API to
avoid giving it an unfair advantage.
Results for all models can be seen in Table 3.
All GPT-J models without tool use achieve similar
performance. Crucially, Toolformer clearly outper-
forms these baseline models, improving upon the
best baseline by 11.7, 5.2 and 18.6 points, respec-
tively. It also clearly outperforms OPT (66B) and
GPT-3 (175B), despite both models being much
larger. This is achieved because the model inde-
pendently decides to ask the question answering
tool for the required information in almost all cases
(98.1%); for only very few examples, it uses a dif-
ferent tool (0.7%) or no tool at all (1.2%).
4.2.2 Math Datasets
We test mathematical reasoning abilities on ASDiv
(Miao et al., 2020), SV AMP (Patel et al., 2021) and
the MAWPS benchmark (Koncel-Kedziorski et al.,
2016). We again account for the fact that we test
all models in a zero-shot setup by using a more
lenient evaluation criterion: As the required output
is always a number, we simply check for the Ô¨Årst
Model SQuAD Google-RE T-REx
GPT-J 17.8 4.9 31.9
GPT-J + CC 19.2 5.6 33.2
Toolformer (disabled) 22.1 6.3 34.9
Toolformer 33.8 11.5 53.5
OPT (66B) 21.6 2.9 30.1
GPT-3 (175B) 26.8 7.0 39.8
Table 3: Results on subsets of LAMA. Toolformer uses
the question answering tool for most examples, clearly
outperforming all baselines of the same size and achiev-
ing results competitive with GPT-3 (175B).
Model ASDiv SV AMP MA WPS
GPT-J 7.5 5.2 9.9
GPT-J + CC 9.6 5.0 9.3
Toolformer (disabled) 14.8 6.3 15.0
Toolformer 40.4 29.4 44.0
OPT (66B) 6.0 4.9 7.9
GPT-3 (175B) 14.0 10.0 19.8
Table 4: Results for various benchmarks requiring
mathematical reasoning. Toolformer makes use of the
calculator tool for most examples, clearly outperform-
ing even OPT (66B) and GPT-3 (175B).
number predicted by the model.7
Table 4 shows results for all benchmarks. While
GPT-J and GPT-J + CC perform about the same,
Toolformer achieves stronger results even when
API calls are disabled. We surmise that this is be-
cause the model is Ô¨Ånetuned on many examples
of API calls and their results, improving its own
mathematical capabilities. Nonetheless, allowing
the model to make API calls more than doubles per-
formance for all tasks, and also clearly outperforms
the much larger OPT and GPT-3 models. This is
because across all benchmarks, for 97.9% of all
examples the model decides to ask the calculator
tool for help.
4.2.3 Question Answering
We look at Web Questions (Berant et al., 2013),
Natural Questions (Kwiatkowski et al., 2019) and
TriviaQA (Joshi et al., 2017), the three question an-
swering datasets considered by Brown et al. (2020).
For evaluation, we check whether the Ô¨Årst 20 words
predicted by a model contain the correct answer
instead of requiring an exact match. For Tool-
former, we disable the question answering tool as
7An exception to this is if the model‚Äôs prediction contains
an equation (e.g., ‚ÄúThe correct answer is 5+3=8‚Äù), in which
case we consider the Ô¨Årst number after the ‚Äú=‚Äù sign to be its
prediction.
Model WebQS NQ TriviaQA
GPT-J 18.5 12.8 43.9
GPT-J + CC 18.4 12.2 45.6
Toolformer (disabled) 18.9 12.6 46.7
Toolformer 26.3 17.7 48.8
OPT (66B) 18.6 11.4 45.7
GPT-3 (175B) 29.0 22.6 65.9
Table 5: Results for various question answering dataset.
Using the Wikipedia search tool for most examples,
Toolformer clearly outperforms baselines of the same
size, but falls short of GPT-3 (175B).
this would make solving the tasks trivial, especially
given that the underlying QA system was Ô¨Ånetuned
on Natural Questions.
Results are shown in Table 5. Once again,
Toolformer clearly outperforms all other models
based on GPT-J, this time mostly relying on the
Wikipedia search API (99.3%) to Ô¨Ånd relevant in-
formation. However, Toolformer still lags behind
the much larger GPT-3 (175B) model. This is likely
due to both the simplicity of our search engine (in
many cases, it returns results that are clearly not
a good match for a given query) and the inability
of Toolformer to interact with it, e.g., by refor-
mulating its query if results are not helpful or by
browsing through multiple of the top results. We
believe that adding this functionality is an exciting
direction for future work.
4.2.4 Multilingual Question Answering
We evaluate Toolformer and all baseline models
on MLQA (Lewis et al., 2019), a multilingual
question-answering benchmark. A context para-
graph for each question is provided in English,
while the question can be in Arabic, German, Span-
ish, Hindi, Vietnamese, or SimpliÔ¨Åed Chinese. In
order to solve the task, the model needs to be able
to understand both the paragraph and the question,
so it may beneÔ¨Åt from translating the question into
English. Our evaluation metric is the percentage of
times the model‚Äôs generation, capped at 10 words,
contains the correct answer.
Results are shown in Table 6. Using API calls
consistently improves Toolformer‚Äôs performance
for all languages, suggesting that it has learned to
make use of the machine translation tool. Depend-
ing on the language, this tool is used for 63.8%
to 94.9% of all examples; the only exception to
this is Hindi, for which the machine translation
tool is used in only 7.3% of cases. However, Tool-
Model Es De Hi Vi Zh Ar
GPT-J 15.2 16.5 1.3 8.2 18.2 8.2
GPT-J + CC 15.7 14.9 0.5 8.3 13.7 4.6
Toolformer (disabled) 19.8 11.9 1.2 10.1 15.0 3.1
Toolformer 20.6 13.5 1.4 10.6 16.8 3.7
OPT (66B) 0.3 0.1 1.1 0.2 0.7 0.1
GPT-3 (175B) 3.4 1.1 0.1 1.7 17.7 0.1
GPT-J (All En) 24.3 27.0 23.9 23.3 23.1 23.6
GPT-3 (All En) 24.7 27.2 26.1 24.9 23.6 24.0
Table 6: Results on MLQA for Spanish (Es), German
(De), Hindi (Hi), Vietnamese (Vi), Chinese (Zh) and
Arabic (Ar). While using the machine translation tool
to translate questions is helpful across all languages,
further pretraining on CCNet deteriorates performance;
consequently, Toolformer does not consistently outper-
form GPT-J. The Ô¨Ånal two rows correspond to models
that are given contexts and questions in English.
former does not consistently outperform vanilla
GPT-J. This is mainly because for some languages,
Ô¨Ånetuning on CCNet deteriorates performance; this
might be due to a distribution shift compared to
GPT-J‚Äôs original pretraining data.
OPT and GPT-3 perform surprisingly weak
across all languages, mostly because they fail to
provide an answer in English despite being in-
structed to do so. A potential reason for GPT-J not
suffering from this problem is that it was trained on
more multilingual data than both OPT and GPT-3,
including the EuroParl corpus (Koehn, 2005; Gao
et al., 2020). As an upper bound, we also evaluate
GPT-J and GPT-3 on a variant of MLQA where
both the context and the question are provided in
English. In this setup, GPT-3 performs better than
all other models, supporting our hypothesis that
its subpar performance on MLQA is due to the
multilingual aspect of the task.
4.2.5 Temporal Datasets
To investigate the calendar API‚Äôs utility, we eval-
uate all models on TEMP LAMA (Dhingra et al.,
2022) and a new dataset that we call DATESET .
TEMP LAMA is a dataset built from Wikidata that
contains cloze queries about facts that change with
time (e.g., ‚ÄúCristiano Ronaldo plays for ___‚Äù)
as well as the correct answer for the years be-
tween 2010 and 2020. DATESET , described in
Appendix D, is also generated through a series
of templates, but populated using a combination
of random dates/durations (e.g., ‚ÄúWhat day of the
week was it 30 days ago?‚Äù). Critically, knowing the
current date is required to answer these questions.
Model T EMP LAMA D ATESET
GPT-J 13.7 3.9
GPT-J + CC 12.9 2.9
Toolformer (disabled) 12.7 5.9
Toolformer 16.3 27.3
OPT (66B) 14.5 1.3
GPT-3 (175B) 15.5 0.8
Table 7: Results for the temporal datasets. Toolformer
outperforms all baselines, but does not make use of the
calendar tool for TEMP LAMA.
For both tasks, we use the same evaluation as for
the original LAMA dataset.
Results shown in Table 7 illustrate that Tool-
former outperforms all baselines for both TEM-
PLAMA and DATESET . However, closer inspec-
tion shows that improvements on TEMP LAMA
can not be attributed to the calendar tool, which is
only used for 0.2% of all examples, but mostly to
the Wikipedia search and question answering tools,
which Toolformer calls the most. This makes sense
given that named entities in TEMP LAMA are often
so speciÔ¨Åc and rare that even knowing the exact
date alone would be of little help. The best course
of action for this dataset ‚Äì Ô¨Årst querying the calen-
dar API to get the current date, and then querying
the question answering system with this date ‚Äì is
not only prohibited by our restriction of using at
most one API call per example, but also hard to
learn for Toolformer given that all API calls in its
training data are sampled independently.
For DATESET , on the other hand, the consider-
able improvement of Toolformer compared to other
models can be fully accredited to the calendar tool,
which it makes use of for 54.8% of all examples.
4.3 Language Modeling
In addition to verifying improved performance on
various downstream tasks, we also want to ensure
that language modeling performance of Toolformer
does not degrade through our Ô¨Ånetuning with API
calls. To this end, we evaluate our models on
two language modeling datasets: WikiText (Mer-
ity et al., 2017) and a subset of 10,000 randomly
selected documents from CCNet (Wenzek et al.,
2020) that were not used during training. Perplex-
ities of various models are shown in Table 8. As
one would expect, Ô¨Ånetuning on CCNet leads to
slightly improved performance on a different CC-
Net subset, but it slightly deteriorates performance
on WikiText, presumably because the original pre-
Model WikiText CCNet
GPT-J 9.9 10.6
GPT-J + CC 10.3 10.5
Toolformer (disabled) 10.3 10.5
Table 8: Perplexities of different models on WikiText
and our validation subset of CCNet. Adding API calls
comes without a cost in terms of perplexity for lan-
guage modeling without any API calls.
training data for GPT-J is more similar to Wiki-
Text than our randomly selected subset of CCNet.
Most importantly, however, training on C‚àó (our
dataset annotated with API calls) does not lead to
an increase in perplexity compared to training on
C when API calls are disabled at inference time.8
4.4 Scaling Laws
We investigate how the ability to ask external tools
for help affects performance as we vary the size
of our LM. To this end, we apply our approach
not just to GPT-J, but also to four smaller mod-
els from the GPT-2 family (Radford et al., 2019),
with 124M, 355M, 775M and 1.6B parameters, re-
spectively. We do so using only a subset of three
tools: the question answering system, the calcula-
tor, and the Wikipedia search engine. Apart from
this, we follow the experimental setup described in
Section 4.1.
Figure 4 shows that the ability to leverage the
provided tools only emerges at around 775M pa-
rameters: smaller models achieve similar perfor-
mance both with and without tools. An exception
to this is the Wikipedia search engine used mostly
for QA benchmarks; we hypothesize that this is
because the API is comparably easy to use. While
models become better at solving tasks without API
calls as they grow in size, their ability to make good
use of the provided API improves at the same time.
As a consequence, there remains a large gap be-
tween predictions with and without API calls even
for our biggest model.
5 Analysis
Decoding Strategy We investigate the effect of
our modiÔ¨Åed decoding strategy introduced in Sec-
tion 4.2, where instead of always generating the
8We do not evaluate the perplexity of Toolformer with
API calls enabled as computing the probability pM (xt |
x1,...,x t‚àí1) of tokenxt givenx1,...,x t‚àí1 would require
marginalizing over all potential API calls that the model could
make at positiont, which is intractable.
051015202530
0200040006000Model Parameters (M)
LAMA
 Toolformer Toolformer (disabled) GPT3 0510152025303540
0200040006000Model Parameters (M)
QA Benchmarks
051015202530
0200040006000Model Parameters (M)
Math Benchmarks
Figure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of
different sizes and GPT-J Ô¨Ånetuned with our approach, both with and without API calls. While API calls are not
helpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the
gap between model predictions with and without API calls remains high.
most likely token, we generate the <API> token
if it is one of the k most likely tokens. Table 9
shows performance on the T-REx subset of LAMA
and on WebQS for different values of k. As ex-
pected, increasingk leads to the model doing API
calls for more examples ‚Äì from 40.3% and 8.5%
withk = 1 (i.e., regular greedy decoding) to 98.1%
and 100% fork = 10 . While for T-REx, there is
already a clear improvement in performance with
greedy decoding, on WebQS our model only starts
to make a substantial number of API calls as we
slightly increase k. Interestingly, for k = 1 the
model is calibrated to some extent: It decides to
call APIs for examples that it would perform partic-
ularly badly on without making API calls. This can
be seen from the fact that performance on examples
where it decides not to make an API call (44.3 and
19.9) is higher than average performance if no API
calls are made at all (34.9 and 18.9). However, this
calibration is lost for higher values ofk.
Data Quality We qualitatively analyze some
API calls generated with our approach for different
APIs. Table 10 shows some examples of texts from
CCNet augmented with API calls, as well as the
corresponding scoreL‚àí
i ‚àíL+
i that is used as a Ô¨Ål-
tering criterion, and whether the API calls made by
the model are intuitively useful in the given context.
As can be seen, high values ofL‚àí
i ‚àíL+
i typically
correspond to useful API calls, whereas low values
correspond to API calls that do not provide any in-
formation that is useful for predicting future tokens.
There are some exceptions, e.g., an API call for
T-REx WebQS
k All AC NC % All AC NC %
0 34.9 ‚Äì 34.9 0.0 18.9 ‚Äì 18.9 0.0
1 47.8 53.0 44.3 40.3 19.3 17.1 19.9 8.5
3 52.9 58.0 29.0 82.8 26.3 26.5 6.6 99.3
10 53.5 54.0 22.5 98.1 26.3 26.4 ‚Äì 100.0
Table 9: Toolformer results on the T-REx subset of
LAMA and on WebQS for different values of k used
during decoding. Numbers shown are overall perfor-
mance (All), performance on the subset where the
model decides to make an API call (AC) and all re-
maining examples (NC), as well as the percentage of
examples for which the model decides to call an API
(%).
‚ÄúFast train success‚Äù in the fourth example that does
not give any relevant information but still reduces
perplexity. However, some amount of noise in the
API calls that are not Ô¨Åltered can actually be useful
as it forces the model Ô¨Ånetuned onC‚àó to not always
blindly follow the results of each call it makes.
6 Related Work
Language Model Pretraining There are various
approaches that augment language models with
some form of additional textual information during
pretraining, including various forms of metadata
(Keskar et al., 2019), HTML tags (Aghajanyan
et al., 2021), Wikipedia markup (Schick et al.,
2022), or related texts obtained from an informa-
tion retrieval system (Guu et al., 2020; Borgeaud
et al., 2021; Izacard et al., 2022). For all of these
Example L‚àí
i ‚àíL+
i Useful
The Flodden Window (a war memorial dedicated to The Middleton Archers), in the Grade I-listed
Church of St Leonard in Middleton is said to be the oldest war memorial in the United King-
dom. <API> WikiSearch(War memorial Flodden)‚Üí Battle of Flodden > Commemoration >
The stained-glass Flodden Window in Middleton Parish Church [. . . ] was constructed by Sir
Richard Assheton in memory of the Battle of Flodden and the archers from Middleton who
fought in it. </API> Sir Richard Assheton of Middleton (who built St Leonard) was granted
knighthood [. . . ]
5.49 
Note: The WL will be open on Friday, <API> Calendar()‚Üí Today is Thursday, March 9, 2017.
</API> March 10, and Sunday, March 19 for regular hours. 2.11 
The Nile has an approximate length of <API> QA(What is the approximate length of the Nile?)
‚Üí 6,853 km </API> 6,853 kilometers, the White Nile being its main source. 2.08 
If Venus had an atmosphere similar to Earth‚Äôs then you would expect Venus‚Äô mean temperature to be
499 K (1.74 x 287) rather than 735 K which is <API> Calculator(735 / 499)‚Üí 1.47 </API> 1.47
(735 / 499) times hotter than it should be.
1.59 
You are here: Home / Featured / Catch this fast train to success! <API> WikiSearch(Fast train
success)‚Üí Fast Train > It also peaked at #23 on the Canadian CHUM singles chart, on June
26, 1971. The success of this single established Myles Goodwyn as the band‚Äôs main songwriter,
and made it possible for April Wine to record a second album. </API> Don‚Äôt wait weeks and
[... ]
0.92 
Os Melhores Escolas em Jersey 2020 <API> MT(Os Melhores Escolas em Jersey) ‚Üí The Best
Schools in Jersey </API> On this page you can search for Universities, Colleges and Business
schools in Jersey
0.70 
Enjoy these pictures from the <API> Calendar()‚Üí Today is Friday, April 19, 2013. </API>
Easter Egg Hunt. 0.33 
85 patients (23%) were hospitalised alive and admitted to a hospital ward. Of them, <API> Calcula-
tor(85 / 23)‚Üí 3.70 </API> 65% had a cardiac aetiology [... ] ‚àí0.02 
But hey, after the <API> Calendar()‚Üí Today is Saturday, June 25, 2011. </API> Disneyland
Ô¨Åasco with the Ô¨Åre drill, I think it‚Äôs safe to say Chewey won‚Äôt let anyone die in a Ô¨Åre. ‚àí0.41 
The last time I was with <API> QA(Who was last time I was with?) ‚Üí The Last Time </API>
him I asked what he likes about me and he said he would tell me one day. ‚àí1.23 
Table 10: Examples of API calls for different tools, sorted by the value of L‚àí
i ‚àíL+
i that is used as a Ô¨Åltering
criterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens.
approaches, additional information is always pro-
vided, regardless of whether it is helpful or not. In
contrast, Toolformer learns for itself to explicitly
asks for the right information.
Tool Use Several approaches aim to equip LMs
with the ability to use external tools such as search
engines (Komeili et al., 2022; Thoppilan et al.,
2022; Lazaridou et al., 2022; Shuster et al., 2022;
Yao et al., 2022), web browsers (Nakano et al.,
2021), calculators (Cobbe et al., 2021; Thoppilan
et al., 2022), translation systems (Thoppilan et al.,
2022) and Python interpreters (Gao et al., 2022).
The way these models learn to use tools can roughly
be divided into two approaches: Either they rely on
large amounts of human supervision (Komeili et al.,
2022; Nakano et al., 2021; Thoppilan et al., 2022)
or they work by prompting the language model in
a few-shot setup tailored towards a speciÔ¨Åc task
where it is known a priori which tools needs to be
used (Gao et al., 2022; Lazaridou et al., 2022; Yao
et al., 2022). In contrast, the self-supervised nature
of Toolformer enables it to learn how and when to
use tools without requiring a speciÔ¨Åc prompt that
shows task-speciÔ¨Åc examples of how a tool could
be used. Perhaps most closely related to our work
is TALM (Parisi et al., 2022), an approach that
uses a similar self-supervised objective for teach-
ing a model to use a calculator and a search engine,
but explores this only in settings where a model is
Ô¨Ånetuned for downstream tasks.
Bootstrapping The idea of using self-training
and bootstrapping techniques to improve models
has been investigated in various contexts, rang-
ing from word sense disambiguation (Yarowsky,
1995), relation extraction (Brin, 1999; Agichtein
and Gravano, 2000), parsing (McClosky et al.,
2006; Reichart and Rappoport, 2007), sequence
generation (He et al., 2020), few-shot text classi-
Ô¨Åcation (Schick and Sch√ºtze, 2021a) and retrieval
(Izacard and Grave, 2021) to reasoning (Zelikman
et al., 2022). In a similar spirit to these approaches,
Toolformer is trained on its own predictions after
applying a perplexity-based Ô¨Åltering step.
7 Limitations
While our approach enables LMs to learn how to
use a variety of tools in a self-supervised way, there
are some clear limitations to what can be achieved
with our method in its current form. One such limi-
tation is the inability of Toolformer to use tools in a
chain (i.e., using the output of one tool as an input
for another tool). This is due to the fact that API
calls for each tool are generated independently; as a
consequence, there are no examples of chained tool
use in the Ô¨Ånetuning dataset. Our current approach
also does not allow the LM to use a tool in an in-
teractive way ‚Äì especially for tools such as search
engines, that could potentially return hundreds of
different results, enabling a LM to browse through
these results or to reÔ¨Åne its search query in a simi-
lar spirit to Nakano et al. (2021) can be crucial for
certain applications. Beyond this, we found models
trained with Toolformer to often be sensitive to the
exact wording of their input when deciding whether
or not to call an API; this is perhaps unsurprising
given that LMs are known to be very sensitive to
the prompt they are provided with in both zero-
and few-shot settings (Jiang et al., 2020; Schick
and Sch√ºtze, 2021a). Depending on the tool, our
method is also very sample-inefÔ¨Åcient; for example,
processing more than a million documents results
in only a few thousand examples of useful calls
to the calculator API. A potential solution to this
problem might be to iteratively apply our approach,
similar to how this is done in related bootstrapping
approaches (Schick and Sch√ºtze, 2021a; Izacard
and Grave, 2021; Parisi et al., 2022). Finally, when
deciding whether or not to make an API call, Tool-
former currently does not take into account the
tool-dependent, computational cost incurred from
making an API call.
8 Conclusion
We have introduced Toolformer, a language model
that learns in a self-supervised way how to use
different tools such as search engines, calculators,
and translation systems via simple API calls. This
is done by Ô¨Ånetuning on a large number of sampled
API calls that are Ô¨Åltered based on whether they
reduce perplexity on future tokens. Toolformer
considerably improves zero-shot performance of a
6.7B parameter GPT-J model, enabling it to even
outperform a much larger GPT-3 model on a range
of different downstream tasks.
References
Armen Aghajanyan, Dmytro Okhonko, Mike Lewis,
Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-
moyer. 2021. Htlm: Hyper-text pre-training and
prompting of language models.
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM Conference on Dig-
ital Libraries, DL ‚Äô00, page 85‚Äì94, New York, NY ,
USA. Association for Computing Machinery.
Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al.
1999. Modern information retrieval , volume 463.
ACM press New York.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533‚Äì1544, Seattle, Wash-
ington, USA. Association for Computational Lin-
guistics.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen-
nigan, Saffron Huang, Loren Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini, Ge-
offrey Irving, Oriol Vinyals, Simon Osindero, Karen
Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre. 2021. Improving language models by retriev-
ing from trillions of tokens.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases, pages 172‚Äì183, Berlin, Heidelberg.
Springer Berlin Heidelberg.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems,
volume 33, pages 1877‚Äì1901. Curran Associates,
Inc.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fe-
dus, Denny Zhou, Daphne Ippolito, David Luan,
Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,
Ryan Sepassi, David Dohan, Shivani Agrawal, Mark
Omernick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,
Jason Wei, Kathy Meier-Hellstern, Douglas Eck,
Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.
Palm: Scaling language modeling with pathways.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training veriÔ¨Åers to solve math
word problems. arXiv preprint arXiv:2110.14168.
Marta R Costa-juss√†, James Cross, Onur √áelebi, Maha
Elbayad, Kenneth HeaÔ¨Åeld, Kevin Heffernan, Elahe
Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
et al. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, V olume 1 (Long and Short Papers) ,
pages 4171‚Äì4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Bhuwan Dhingra, Jeremy R. Cole, Julian Martin
Eisenschlos, Daniel Gillick, Jacob Eisenstein, and
William W. Cohen. 2022. Time-aware language
models as temporal knowledge bases. Transactions
of the Association for Computational Linguistics ,
10:257‚Äì273.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. Realm: Retrieval-
augmented language model pre-training.
Junxian He, Jiatao Gu, Jiajun Shen, and Marc‚ÄôAurelio
Ranzato. 2020. Revisiting self-training for neural
sequence generation. In International Conference
on Learning Representations.
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2022. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor.
Gautier Izacard and Edouard Grave. 2021. Distilling
knowledge from reader to retriever for question an-
swering. In International Conference on Learning
Representations.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2022. Atlas: Few-shot learning with retrieval
augmented language models.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea
Madotto, and Pascale Fung. 2022. Survey of hallu-
cination in natural language generation. ACM Com-
puting Surveys.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know? Transactions of the Association for
Computational Linguistics, 8:423‚Äì438.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(V olume 1: Long Papers) , pages 1601‚Äì1611, Van-
couver, Canada. Association for Computational Lin-
guistics.
Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, H√©rve J√©gou, and Tomas Mikolov.
2016. Fasttext. zip: Compressing text classiÔ¨Åcation
models. arXiv preprint arXiv:1612.03651.
Nitish Shirish Keskar, Bryan McCann, Lav R. Varsh-
ney, Caiming Xiong, and Richard Socher. 2019.
Ctrl: A conditional transformer language model for
controllable generation.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
machine translation summit x: papers , pages 79‚Äì86.
Mojtaba Komeili, Kurt Shuster, and Jason Weston.
2022. Internet-augmented dialogue generation. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (V olume 1:
Long Papers) , pages 8460‚Äì8478, Dublin, Ireland.
Association for Computational Linguistics.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,
Nate Kushman, and Hannaneh Hajishirzi. 2016.
MAWPS: A math word problem repository. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
1152‚Äì1157, San Diego, California. Association for
Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
Ô¨Åeld, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. Transactions of the Association
for Computational Linguistics, 7:452‚Äì466.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Stokowiec, and Nikolai Grigorev. 2022. Internet-
augmented language models through few-shot
prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115.
Patrick Lewis, Barlas O Àòguz, Ruty Rinott, Sebastian
Riedel, and Holger Schwenk. 2019. Mlqa: Eval-
uating cross-lingual extractive question answering.
arXiv preprint arXiv:1910.07475.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O‚ÄôHoro, Jeff Wang, Luke Zettle-
moyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-
anov, and Xian Li. 2021. Few-shot learning with
multilingual language models.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factual-
ity in abstractive summarization.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Main Conference, pages 152‚Äì
159, New York City, USA. Association for Compu-
tational Linguistics.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In International Conference on Learning Repre-
sentations.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and develop-
ing English math word problem solvers. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 975‚Äì984, On-
line. Association for Computational Linguistics.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, Kevin Button, Matthew Knight, Benjamin
Chess, and John Schulman. 2021. Webgpt: Browser-
assisted question-answering with human feedback.
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:
Tool augmented language models.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2080‚Äì2094, Online.
Association for Computational Linguistics.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James
Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, Vassilis Plachouras, Tim Rockt√§schel, and
Sebastian Riedel. 2021. KILT: a benchmark for
knowledge intensive language tasks. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 2523‚Äì2544,
Online. Association for Computational Linguistics.
Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 2463‚Äì2473, Hong Kong, China. As-
sociation for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog, 1(8):9.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statisti-
cal parsers trained on small datasets. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 616‚Äì623, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford, et al.
1995. Okapi at trec-3. Nist Special Publication Sp ,
109:109.
Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio
Petroni, Patrick Lewis, Gautier Izacard, Qingfei You,
Christoforos Nalmpantis, Edouard Grave, and Se-
bastian Riedel. 2022. Peer: A collaborative lan-
guage model.
Timo Schick and Hinrich Sch√ºtze. 2021a. Exploiting
cloze-questions for few-shot text classiÔ¨Åcation and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main V ol-
ume, pages 255‚Äì269, Online. Association for Com-
putational Linguistics.
Timo Schick and Hinrich Sch√ºtze. 2021b. Generating
datasets with pretrained language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6943‚Äì
6951, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,
Eric Michael Smith, Stephen Roller, Megan Ung,
Moya Chen, Kushal Arora, Joshua Lane, Morteza
Behrooz, William Ngan, Spencer Poff, Naman
Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kam-
badur, and Jason Weston. 2022. Blenderbot 3: a de-
ployed conversational agent that continually learns
to responsibly engage.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,
Amin Ghafouri, Marcelo Menegali, Yanping Huang,
Maxim Krikun, Dmitry Lepikhin, James Qin, De-
hao Chen, Yuanzhong Xu, Zhifeng Chen, Adam
Roberts, Maarten Bosma, Vincent Zhao, Yanqi
Zhou, Chung-Ching Chang, Igor Krivokon, Will
Rusch, Marc Pickett, Pranesh Srinivasan, Laichee
Man, Kathleen Meier-Hellstern, Meredith Ringel
Morris, Tulsee Doshi, Renelito Delos Santos, Toju
Duke, Johnny Soraker, Ben Zevenbergen, Vinod-
kumar Prabhakaran, Mark Diaz, Ben Hutchinson,
Kristen Olson, Alejandra Molina, Erin Hoffman-
John, Josh Lee, Lora Aroyo, Ravi Rajakumar,
Alena Butryna, Matthew Lamm, Viktoriya Kuzmina,
Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray
Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian
Croak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-
guage models for dialog applications.
Ben Wang and Aran Komatsuzaki. 2021. GPT-
J-6B: A 6 Billion Parameter Autoregressive
Language Model. https://github.com/
kingoflolz/mesh-transformer-jax.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A. Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Met-
zler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,
Percy Liang, Jeff Dean, and William Fedus. 2022.
Emergent abilities of large language models.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-
neau, Vishrav Chaudhary, Francisco Guzm√°n, Ar-
mand Joulin, and Edouard Grave. 2020. CCNet:
Extracting high quality monolingual datasets from
web crawl data. In Proceedings of the Twelfth Lan-
guage Resources and Evaluation Conference , pages
4003‚Äì4012, Marseille, France. European Language
Resources Association.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 189‚Äì196, Cambridge, Mas-
sachusetts, USA. Association for Computational
Linguistics.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.
Goodman. 2022. Star: Bootstrapping reasoning
with reasoning.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.
A API Details
When sampling and Ô¨Åltering API calls, by default
we use values of œÑs = 0.05 andœÑf = 1.0 ‚Äì i.e.,
we only make API calls at positions where the
probability of the <API> token is at least 5%, and
we keep API calls if they reduce the loss by at least
1.0. We only keep the topk = 5 such positions and
sample up to m = 5 API calls for each position
identiÔ¨Åed in a piece of text. Due to the heuristic
Ô¨Åltering described below, we generate API calls for
the calculator and machine translation system on
only a small subset of C; to compensate for this,
we set œÑs = 0.0, k = 20 andm = 10 for these
tools. As the resulting sets of API calls are still
comparably small, we additionally setœÑf = 0.5.
A.1 Implementation
Question Answering We use the Atlas model of
Izacard et al. (2022) Ô¨Ånetuned on Natural Ques-
tions (Kwiatkowski et al., 2019) as our question
answering system. For creatingC‚àó we use Atlas-
large, enabling us to efÔ¨Åciently process millions
of API calls; during inference, we use the larger
Atlas-xxl model.
Calculator Our calculator is based on a simple
Python script and only supports the operators ‚Äú +‚Äù,
‚Äú‚àí‚Äù, ‚Äú‚àó‚Äù, and ‚Äú /‚Äù. It does not return any result
for syntactically invalid equations. For sampling
API calls, we apply heuristic Ô¨Ålters to our subset of
CCNet and only process documents that either (i)
contain at least three numbers within a window of
100 tokens, where one of these numbers is the result
of applying a mathematical operation to the other
two, (ii) contain one of the sequences ‚Äú=‚Äù, ‚Äúequals‚Äù,
‚Äúequal to‚Äù, ‚Äútotal of‚Äù, ‚Äúaverage of‚Äù followed by a
number, or (iii) contain at least three numbers; for
texts that only match the last criterion, we only
keep a random subset of 1%.
Calendar For creating our datasetC‚àó, we operate
under the assumption that the calendar date in such
cases should be the date that the document was
created. We approximate this by extracting the date
from the URL, if it is present. We Ô¨Ålter out texts for
which a date cannot be extracted, leaving around
18% of the documents.
Machine Translation For both training and in-
ference, we use the 600M parameter NLLB (Costa-
juss√† et al., 2022) as our machine translation (MT)
model. The source language is automatically de-
tected using the fastText classiÔ¨Åer (Joulin et al.,
2016), while the target language is always set to
English. Since most of the CCNet dataset is in
English, we Ô¨Ålter out the parts that contain only
English text before generating API calls. More
speciÔ¨Åcally, we only keep those paragraphs which
contain text chunks in a language other than En-
glish preceded and followed by English text. We
use text chunks of size 10 tokens. To determine
whether the middle text chunk is in a language
different than English we again use the fastText
classiÔ¨Åer with a conÔ¨Ådence greater than 0.8. We
also Ô¨Ålter out any text chunks that contain only
numbers or special symbols. This Ô¨Åltering mecha-
nism allows us to generate data more efÔ¨Åciently by
focusing our API call generations in places where
the MT tool is likely to be helpful. After generating
the MT API calls, we additionally remove from our
training set those where the input to the MT tool
appears after the API call but not before it. While
during data generation the model can look ahead
to generate API calls, this is not possible at infer-
ence time, so we want to dissuade the model from
calling the API in such cases.
A.2 Prompts
Below, we list the prompts used to sample API
calls for each tool considered.
Question Answering We use the following
prompt for the question answering tool:
Your task is to add calls to a Question
Answering API to a piece of text.
The questions should help you get
information required to complete the
text. You can call the API by writing
"[QA(question)]" where "question" is the
question you want to ask. Here are some
examples of API calls:
Input: Joe Biden was born in Scranton,
Pennsylvania.
Output: Joe Biden was born in [QA("Where
was Joe Biden born?")] Scranton,
[QA("In which state is Scranton?")]
Pennsylvania.
Input: Coca-Cola, or Coke, is a
carbonated soft drink manufactured by
the Coca-Cola Company.
Output: Coca-Cola, or [QA("What other
name is Coca-Cola known by?")] Coke, is
a carbonated soft drink manufactured by
[QA("Who manufactures Coca-Cola?")] the
Coca-Cola Company.
Input: x
Output:
Calculator We use the following prompt for the
calculator:
Your task is to add calls to a
Calculator API to a piece of text.
The calls should help you get
information required to complete the
text. You can call the API by writing
"[Calculator(expression)]" where
"expression" is the expression to be
computed. Here are some examples of API
calls:
Input: The number in the next term is 18
+ 12 x 3 = 54.
Output: The number in the next term is
18 + 12 x 3 = [Calculator(18 + 12 * 3)]
54.
Input: The population is 658,893 people.
This is 11.4% of the national average of
5,763,868 people.
Output: The population is 658,893 people.
This is 11.4% of the national average of
[Calculator(658,893 / 11.4%)] 5,763,868
people.
Input: A total of 252 qualifying matches
were played, and 723 goals were scored
(an average of 2.87 per match). This is
three times less than the 2169 goals
last year.
Output: A total of 252 qualifying
matches were played, and 723 goals were
scored (an average of [Calculator(723
/ 252)] 2.87 per match). This is twenty
goals more than the [Calculator(723 -
20)] 703 goals last year.
Input: I went to Paris in 1994 and
stayed there until 2011, so in total,
it was 17 years.
Output: I went to Paris in 1994 and
stayed there until 2011, so in total, it
was [Calculator(2011 - 1994)] 17 years.
Input: From this, we have 4 * 30 minutes
= 120 minutes.
Output: From this, we have 4 * 30
minutes = [Calculator(4 * 30)] 120
minutes.
Input: x
Output:
Wikipedia Search We use the following prompt
for the Wikipedia search tool:
Your task is to complete a given piece
of text. You can use a Wikipedia Search
API to look up information. You can do
so by writing "[WikiSearch(term)]" where
"term" is the search term you want to
look up. Here are some examples of API
calls:
Input: The colors on the flag of Ghana
have the following meanings: red is for
the blood of martyrs, green for forests,
and gold for mineral wealth.
Output: The colors on the flag of Ghana
have the following meanings: red is for
[WikiSearch("Ghana flag red meaning")]
the blood of martyrs, green for forests,
and gold for mineral wealth.
Input: But what are the risks during
production of nanomaterials? Some
nanomaterials may give rise to various
kinds of lung damage.
Output: But what are the risks
during production of nanomaterials?
[WikiSearch("nanomaterial production
risks")] Some nanomaterials may give
rise to various kinds of lung damage.
Input: Metformin is the first-line drug
for patients with type 2 diabetes and
obesity.
Output: Metformin is the first-line drug
for [WikiSearch("Metformin first-line
drug")] patients with type 2 diabetes
and obesity.
Input: x
Output:
Machine Translation We use the following
prompt for the machine translation tool:
Your task is to complete a given piece
of text by using a Machine Translation
API.
You can do so by writing "[MT(text)]"
where text is the text to be translated
into English.
Here are some examples:
Input: He has published one book: O
homem suprimido (‚ÄúThe Supressed Man‚Äù)
Output: He has published one book: O
homem suprimido [MT(O homem suprimido)]
(‚ÄúThe Supressed Man‚Äù)
Input: In Morris de Jonge‚Äôs Jeschuah,
der klassische j√ºdische Mann, there is a
description of a Jewish writer
Output: In Morris de Jonge‚Äôs Jeschuah,
der klassische j√ºdische Mann [MT(der
klassische j√ºdische Mann)], there is a
description of a Jewish writer
Input: Âçó ‰∫¨ È´ò Ê∑≥ Âéø ‰Ωè Êàø Âíå Âüé ‰π° Âª∫ ËÆæ Â±Ä Âüé Â∏Ç Êñ∞
Âå∫ ËÆæ ËÆ° a plane of reference Gaochun is
one of seven districts of the provincial
capital Nanjing
Output: [MT( Âçó‰∫¨È´òÊ∑≥Âéø‰ΩèÊàøÂíåÂüé‰π°Âª∫ËÆæÂ±Ä ÂüéÂ∏ÇÊñ∞
Âå∫ ËÆæ ËÆ° )] a plane of reference Gaochun is
one of seven districts of the provincial
capital Nanjing
Input: x
Output:
Calendar We use the following prompt for the
calendar tool:
Your task is to add calls to a Calendar
API to a piece of text. The API calls
should help you get information required
to complete the text. You can call the
API by writing "[Calendar()]" Here are
some examples of API calls:
Input: Today is the first Friday of the
year.
Output: Today is the first [Calendar()]
Friday of the year.
Input: The president of the United
States is Joe Biden.
Output: The president of the United
States is [Calendar()] Joe Biden.
Input: The current day of the week is
Wednesday.
Output: The current day of the week is
[Calendar()] Wednesday.
Input: The number of days from now until
Christmas is 30.
Output: The number of days from now
until Christmas is [Calendar()] 30.
Input: The store is never open on the
weekend, so today it is closed.
Output: The store is never open on the
weekend, so today [Calendar()] it is
closed.
Input: x
Output:
B Toolformer Training
We use up to 25k examples per API. Max sequence
length 1,024. Effective batch size of 128. All mod-
els are trained using DeepSpeed‚Äôs ZeRO-3 (Rasley
et al., 2020). We used 8 NVIDIA A100 40GB
GPUs with BF16. Training up to 2k steps, where
we evaluate PPL on a small development set from
CCNet containing 1,000 examples every 500 steps.
We pick the checkpoint that performs best.
C Zero-Shot Prompts
C.1 LAMA and T EMP LAMA
For both LAMA and TEMP LAMA , given an input
text x, we use the following prompt: Please
complete the following text so
that it is factually correct: x.
C.2 Math Benchmarks
For all math benchmarks, given a context x and
a question q, our prompt is: x q The answer
is.
C.3 Question Answering
For all question answering datasets, including
DATESET , we simply preÔ¨Åx the question with
Answer the following question: . We
append a question mark if the question does not
already end with one.
C.4 Multilingual Question Answering
For MLQA, given a context x and a ques-
tion q, our prompt is: Your task is
Template Size
How many days {ago was, are there until}
{past_date, future_date}?
400
What {day of the week, day of the month, month,
year} was it ( current_date ‚Äì past_date ) {days,
weeks, months, years} ago?
800
What {day of the week, day of the month, month,
year} will it be in ( future_date ‚Äì current_date )
days?
800
What day of the week {is, was} it on {past_date,
future_date}?
400
What {day of the week, day of the month, month,
year} {is, was} it {the day before yesterday, yes-
terday, today, tomorrow, the day after tomorrow}?
4,000
What {day of the week, day of the month, month}
{is, was}holiday this year?
1,800
How many {days, weeks, months, years} {ago
was, are there until}holiday this year?
1,200
Total 9,400
Table 11: Templates used to create D ATESET where
a current_date is randomly selected. For each cur-
rent_date, a random past_date and future_date is gen-
erated and used to Ô¨Åll each template, if relevant. The
federal holidays in the United States (e.g., Thanksgiv-
ing) were used in the templates involving holidays.
to answer a question based on
the following paragraph: x Now
answer the following question in
English: q.
D D ATESET
DATESET is created by Ô¨Årst randomly selecting 500
‚Äúcurrent dates‚Äù. For each current date, another rela-
tively past/future date is randomly selected within
a four-year range, and the two dates are used to Ô¨Åll
the query templates in Table 11. An example of one
such query using the Ô¨Årst template would be, ‚ÄúHow
many days ago was August 14, 2020?‚Äù If called,
the Calendar tool would return the presumed cur-
rent date (e.g., ‚ÄúToday is Sunday, November 20,
2020‚Äù).


=== ReAct-paper.pdf ===
Published as a conference paper at ICLR 2023
REAC T: S YNERGIZING REASONING AND ACTING IN
LANGUAGE MODELS
Shunyu Yao‚àó*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2
1Department of Computer Science, Princeton University
2Google Research, Brain team
1{shunyuy,karthikn}@princeton.edu
2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com
ABSTRACT
While large language models (LLMs) have demonstrated impressive performance
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action
plan generation) have primarily been studied as separate topics. In this paper, we
explore the use of LLMs to generate both reasoning traces and task-speciÔ¨Åc actions
in an interleaved manner, allowing for greater synergy between the two: reasoning
traces help the model induce, track, and update action plans as well as handle
exceptions, while actions allow it to interface with and gather additional information
from external sources such as knowledge bases or environments. We apply our
approach, named ReAct, to a diverse set of language and decision making tasks
and demonstrate its effectiveness over state-of-the-art baselines in addition to
improved human interpretability and trustworthiness. Concretely, on question
answering (HotpotQA) and fact veriÔ¨Åcation (Fever),ReAct overcomes prevalent
issues of hallucination and error propagation in chain-of-thought reasoning by
interacting with a simple Wikipedia API, and generating human-like task-solving
trajectories that are more interpretable than baselines without reasoning traces.
Furthermore, on two interactive decision making benchmarks (ALFWorld and
WebShop), ReAct outperforms imitation and reinforcement learning methods by
an absolute success rate of 34% and 10% respectively, while being prompted with
only one or two in-context examples.
1 I NTRODUCTION
A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with
verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to
play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,
1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-
sider the example of cooking up a dish in the kitchen. Between any two speciÔ¨Åc actions, we may
reason in language in order to track progress (‚Äúnow that everything is cut, I should heat up the pot of
water‚Äù), to handle exceptions or adjust the plan according to the situation (‚ÄúI don‚Äôt have salt, so let
me use soy sauce and pepper instead‚Äù), and to realize when external information is needed (‚Äúhow do
I prepare dough? Let me search on the Internet‚Äù). We may also act (open a cookbook to read the
recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (‚ÄúWhat
dish can I make right now?‚Äù). This tight synergy between ‚Äúacting‚Äù and ‚Äúreasoning‚Äù allows humans
to learn new tasks quickly and perform robust decision making or reasoning, even under previously
unseen circumstances or facing information uncertainties.
Recent results have hinted at the possibility of combining verbal reasoning with interactive decision
making in autonomous systems. On one hand, properly prompted large language models (LLMs)
have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive
‚àóWork during Google internship. Projet page with code: https://react-lm.github.io/.
1
arXiv:2210.03629v3  [cs.CL]  10 Mar 2023
Published as a conference paper at ICLR 2023
$FW7KLQN>)LUVW,QHHGWRILQGDSHSSHUVKDNHUPRUH
OLNHO\WRDSSHDULQFDELQHWVFRXQWHUWRSV@
$FW*RWRFDELQHW
2EV2QWKHFDELQHW\RXVHHDYDVH
+HUHWKHDJHQWJRWRFDELQHWWKHQFDELQHWWKHQFRXQWHUWRSDQG
$FW*RWRFRXQWHUWRS2EV2QWKHFRXQWHUWRS\RXVHHDDSSOHDEUHDG
DSHSSHUVKDNHUDQGDYDVH
$FW7DNHSHSSHUVKDNHUIURPFRXQWHUWRS
2EV<RXSLFNXSWKHSHSSHUVKDNHUIURPWKHFRXQWHUWRS
$FW7KLQN>1RZ,ILQGDSHSSHUVKDNHU1H[W,QHHGWR
SXWLWLQRQGUDZHU@
$FW*RWRGUDZHU
2EV'UDZHULVFORVHG
$FW2SHQGUDZHU
2EV<RXRSHQ'UDZHU¬™
$FW3XWSHSSHUVKDNHULQRQGUDZHU
2EV<RXSXWSHSSHUVKDNHULQRQWKHGUDZHU
D6WDQGDUG
$QVZHUL3RG
E&R75HDVRQ2QO\
7KRXJKW/HW
VWKLQNVWHS
E\VWHS$SSOH5HPRWHZDV
RULJLQDOO\GHVLJQHGWR
LQWHUDFWZLWK$SSOH79
$SSOH79FDQEHFRQWUROOHG
E\L3KRQHL3DGDQGL3RG
7RXFK6RWKHDQVZHULV
L3KRQHL3DGDQGL3RG
7RXFK
$QVZHUL3KRQHL3DGL3RG
7RXFK
F$FW2QO\
$FW6HDUFK>$SSOH5HPRWH@
2EV7KH$SSOH5HPRWHLVD
UHPRWHFRQWURO¬™
$FW6HDUFK>)URQW5RZ@
2EV&RXOGQRWILQG>)URQW
5RZ@6LPLODU¬™
$FW6HDUFK>)URQW5RZ
VRIWZDUH@
2EV)URQW5RZLVD
GLVFRQWLQXHGPHGLDFHQWHU
VRIWZDUH¬™
$FW)LQLVK>\HV@
G5H$FW5HDVRQ$FW
7KRXJKW,QHHGWRVHDUFK$SSOH5HPRWHDQGILQGWKH
SURJUDPLWZDVRULJLQDOO\GHVLJQHGWRLQWHUDFWZLWK
$FW6HDUFK>$SSOH5HPRWH@
2EV7KH$SSOH5HPRWHLVDUHPRWHFRQWUROLQWURGXFHGLQ
2FWREHUE\$SSOH¬™RULJLQDOO\GHVLJQHGWRFRQWUROWKH
)URQW5RZPHGLDFHQWHUSURJUDP¬™
7KRXJKW$SSOH5HPRWHZDVRULJLQDOO\GHVLJQHGWRFRQWURO
WKH)URQW5RZPHGLDFHQWHUSURJUDP,QHHGWRVHDUFK)URQW
5RZQH[WDQGILQGZKDWRWKHUGHYLFHFDQFRQWUROLW
$FW6HDUFK>)URQW5RZ@
2EV&RXOGQRWILQG>)URQW5RZ@6LPLODU>
)URQW5RZ
6HDWWR(DUWK

)URQW5RZ0RWRUVSRUWV

)URQW5RZ
VRIWZDUH
¬™@
7KRXJKW)URQW5RZLVQRWIRXQG,QHHGWRVHDUFK)URQW
5RZVRIWZDUH
$FW6HDUFK>)URQW5RZVRIWZDUH@
2EV)URQW5RZLVDGLVFRQWLQXHGPHGLDFHQWHUVRIWZDUH¬™
7KRXJKW)URQW5RZVRIWZDUHLVFRQWUROOHGE\DQ$SSOH
5HPRWHRUWKHNH\ERDUGIXQFWLRQNH\V6RWKHDQVZHULV
NH\ERDUGIXQFWLRQNH\V
$FW)LQLVK>NH\ERDUGIXQFWLRQNH\V@
“º
4XHVWLRQ$VLGHIURPWKH$SSOH5HPRWHZKDWRWKHUGHYLFH
FDQFRQWUROWKHSURJUDP$SSOH5HPRWHZDVRULJLQDOO\
GHVLJQHGWRLQWHUDFWZLWK"
D$FW2QO\$FW*RWRGUDZHU
2EV7KHGUDZHULVFORVHG
$FW2SHQGUDZHU
2EV<RXRSHQWKHGUDZHU7KHGUDZHULVRSHQ,QLW
\RXVHHDGLVKVSRQJHDQGDVSRRQ

$FW*RWRVLQNEDVLQ
2EV2QWKHVLQNEDVLQ\RXVHHDGLVKVSRQJHD
VSDWXODDQGDVSRRQ
$FW7DNHSHSSHUVKDNHUIURPVLQNEDVLQ
2EV1RWKLQJKDSSHQV
$FW7DNHSHSSHUVKDNHUIURPVLQNEDVLQ
2EV1RWKLQJKDSSHQV
E5H$FW5HDVRQ$FW
“º
<RXDUHLQWKHPLGGOHRIDURRP/RRNLQJTXLFNO\DURXQG
\RX\RXVHHDFDELQHWDFDELQHWDFRIIHHPDFKLQH
DFRXQWHUWRSDVWRYHEXUQHUDQGDWRDVWHU
<RXUWDVNLVWR3XWVRPHSHSSHUVKDNHURQDGUDZHU
$OI:RUOG
+RWVSRW4$
Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought ( CoT,
Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)
question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar
et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task
solving trajectories generated by the model (Act, Thought) and the environment (Obs).
answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,
2022). However, this ‚Äúchain-of-thought‚Äù reasoning is a static black box, in that the model uses
its own internal representations to generate thoughts and is not grounded in the external world,
which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact
hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,
recent work has explored the use of pre-trained language models for planning and acting in interactive
environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with
a focus on predicting actions via language priors. These approaches usually convert multi-modal
observations into text, use a language model to generate domain-speciÔ¨Åc actions or plans, and then
use a controller to choose or execute them. However, they do not employ language models to reason
abstractly about high-level goals or maintain a working memory to support acting, barring Huang
et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the
current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been
studies on how reasoning and acting can be combined in a synergistic manner for general task solving,
and if such a combination can bring systematic beneÔ¨Åts compared to reasoning or acting alone.
In this work, we present ReAct, a general paradigm to combine reasoning and acting with language
models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct
prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an
interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and
adjust high-level plans for acting (reason to act), while also interact with the external environments
(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).
2
Published as a conference paper at ICLR 2023
We conduct empirical evaluations ofReAct and state-of-the-art baselines on four diverse benchmarks:
question answering (HotPotQA, Yang et al., 2018), fact veriÔ¨Åcation (Fever, Thorne et al., 2018),
text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao
et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact
with, ReAct outperforms vanilla action generation models while being competitive with chain-of-
thought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct
and CoT that allows for the use of both internal knowledge and externally obtained information
during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able
to outperform imitation or reinforcement learning methods trained with 103‚àº 105 task instances,
with an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate
the importance of sparse, versatile reasoning in decision making by showing consistent advantages
over controlled baselines with actions only. Besides general applicability and performance boost,
the combination of reasoning and acting also contributes to model interpretability, trustworthiness,
and diagnosability across all domains, as humans can readily distinguish information from model‚Äôs
internal knowledge versus external environments, as well as inspect reasoning traces to understand
the decision basis of model actions.
To summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-
based paradigm to synergize reasoning and acting in language models for general task solving; (2) we
perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a
few-shot learning setup over prior approaches that perform either reasoning or action generation in
isolation; (3) we present systematic ablations and analysis to understand the importance of acting in
reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations ofReAct under the
prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial Ô¨Ånetuning
experiments showing the potential of ReAct to improve with additional training data. Scaling up
ReAct to train and operate on more tasks and combining it with complementary paradigms like
reinforcement learning could further unlock the potential of large language models.
2 REAC T: S YNERGIZING REASONING + AC TING
Consider a general setup of an agent interacting with an environment for task solving. At time
stept, an agent receives an observationot‚ààO from the environment and takes an actionat‚ààA
following some policyœÄ(at|ct), wherect = (o1,a 1,¬∑¬∑¬∑ ,o t‚àí1,a t‚àí1,o t) is the context to the agent.
Learning a policy is challenging when the mappingct‚Ü¶‚Üíat is highly implicit and requires extensive
computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct Ô¨Ånal
action (Act 4) to Ô¨Ånish the QA task as it requires complex reasoning over the trajectory context
(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the
context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.
The idea of ReAct is simple: we augment the agent‚Äôs action space to ÀÜA =A‚à™L , whereL is the
space of language. An action ÀÜat‚ààL in the language space, which we will refer to as a thought or a
reasoning trace, does not affect the external environment, thus leading to no observation feedback.
Instead, a thought ÀÜat aims to compose useful information by reasoning over the current contextct,
and update the contextct+1 = (ct, ÀÜat) to support future reasoning or acting. As shown in Figure 1,
there could be various types of useful thoughts, e.g. decomposing task goals and create action plans
(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),
extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans
(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.
However, as the language spaceL is unlimited, learning in this augmented action space is difÔ¨Åcult
and requires strong language priors. In this paper, we mainly focus on the setup where a frozen
large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context
examples to generate both domain-speciÔ¨Åc actions and free-form language thoughts for task solving
(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and
environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is
of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the
task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision
making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to
1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.
3
Published as a conference paper at ICLR 2023
appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the
asynchronous occurrence of thoughts and actions for itself.
Since decision making and reasoning capabilities are integrated into a large language model, ReAct
enjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is
straightforward as human annotators just type down their thoughts in language on top of their actions
taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail
prompt design for each task in Sections 3 and 4. B) General and Ô¨Çexible: Due to the Ô¨Çexible thought
space and thought-action occurrence format, ReAct works for diverse tasks with distinct action
spaces and reasoning needs, including but not limited to QA, fact veriÔ¨Åcation, text game, and web
navigation. C) Performant and robust: ReAct shows strong generalization to new task instances
while learning solely from one to six in-context examples, consistently outperforming baselines with
only reasoning or acting across different domains. We also show in Section 3 additional beneÔ¨Åts
when Ô¨Ånetuning is enabled, and in Section 4 howReAct performance is robust to prompt selections.
D) Human aligned and controllable: ReAct promises an interpretable sequential decision making
and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,
humans can also control or correct the agent behavior on the go by thought editing, as shown in
Figure 5 in Section 4.
3 K NOWLEDGE -I NTENSIVE REASONING TASKS
We begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact
veriÔ¨Åcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to
retrieve information to support reasoning, while also use reasoning to target what to retrieve next,
demonstrating a synergy of reasoning and acting.
3.1 S ETUP
Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-
PotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning
over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriÔ¨Åcation
benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based
on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only
setup for both tasks, where models only receive the question/claim as input without access to support
paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with
an external environment to support reasoning.
Action Space We design a simple Wikipedia web API with three types of actions to support
interactive information retrieval: (1) search[entity], which returns the Ô¨Årst 5 sentences from
the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the
Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page
containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which
would Ô¨Ånish the current task withanswer. We note that this action space mostly can only retrieve a
small part of a passage based on exact passage name, which is signiÔ¨Åcantly weaker than state-of-the-
art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,
and force models to retrieve via explicit reasoning in language.
3.2 M ETHODS
ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training
set and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.
Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense
thought), where free-form thoughts are used for various purposes. SpeciÔ¨Åcally, we use a combination
of thoughts that decompose questions (‚ÄúI need to search x, Ô¨Ånd y, then Ô¨Ånd z‚Äù), extract information
from Wikipedia observations (‚Äúx was started in 1844‚Äù, ‚ÄúThe paragraph does not tell x‚Äù), perform
commonsense (‚Äúx is not y, so z must instead be...‚Äù) or arithmetic reasoning (‚Äú1844 < 1989‚Äù), guide
2We Ô¨Ånd more examples do not improve performance.
4
Published as a conference paper at ICLR 2023
Prompt Methoda HotpotQA Fever
(EM) (Acc)
Standard 28.7 57.1
CoT(Wei et al., 2022) 29.4 56.3
CoT-SC(Wang et al., 2022a) 33.4 60.4
Act 25.7 58.9
ReAct 27.4 60.9
CoT-SC‚ÜíReAct 34.2 64.6
ReAct‚ÜíCoT-SC 35.1 62.0
Supervised SoTAb 67.5 89.5
Table 1: PaLM-540B prompting results on
HotpotQA and Fever.
aHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,
CoT-SC in Wang et al. (2022b).
b(Zhu et al., 2021; Lewis et al., 2020)
0 5 10 15 20
#CoT-SC trials
26
28
30
32
34HotpotQA EM
0 5 10 15 20
#CoT-SC trials
47.5
50.0
52.5
55.0
57.5
60.0
62.5
65.0Fever Acc
Method
CoT-SC -> ReAct
ReAct -> CoT-SC
CoT-SC
ReAct
CoT
Figure 2: PaLM-540B prompting results with respect to
number of CoT-SC samples used.
search reformulation (‚Äúmaybe I can search/look up x instead‚Äù), and synthesize the Ô¨Ånal answer (‚Äú...so
the answer is x‚Äù). See Appendix C for more details.
Baselines We systematically ablateReAct trajectories to build prompts for multiple baselines (with
formats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,
actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,
2022), which removes actions and observations and serve as a reasoning-only baseline. We also
build a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories
with decoding temperature 0.7 during inference and adopting the majority answer, which is found to
consistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts
in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the
Internet to answer questions, though it operates on a different task and action space, and uses imitation
and reinforcement learning instead of prompting.
Combining Internal and External Knowledge As will be detail in Section 3.3, we observe that
the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT
is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts
or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide
when to switch to the other method based on the following heuristics: A) ReAct‚Üí CoT-SC: when
ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for
HotpotQA and FEVER respectively as we Ô¨Ånd more steps will not improveReAct performance3.
B) CoT-SC‚Üí ReAct: when the majority answer among n CoT-SC samples occurs less thann/2
times (i.e. internal knowledge might not support the task conÔ¨Ådently), back off toReAct.
Finetuning Due to the challenge of manually annotating reasoning traces and actions at scale,
we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories
with correct answers generated by ReAct (also for other baselines) to Ô¨Ånetune smaller language
models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on
input questions/claims. More details are in Appendix B.1.
3.3 R ESULTS AND OBSERVATIONS
ReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-
540B as the base model with different prompting methods. We note that ReAct is better than Act
on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the
Ô¨Ånal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conÔ¨Årm the beneÔ¨Åt of reasoning
traces for more informed acting.
3Of all trajectories with correct Ô¨Ånal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only
take up 0.84% and 1.33% respectively.
5
Published as a conference paper at ICLR 2023
Type DeÔ¨Ånition ReAct CoT
Success True positive Correct reasoning trace and facts 94% 86%
False positive Hallucinated reasoning trace or facts 6% 14%
Failure
Reasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%
Search result error Search return empty or does not contain useful information 23% -
Hallucination Hallucinated reasoning trace or facts 0% 56%
Label ambiguity Right prediction but did not match the label precisely 29% 28%
Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their
percentages in randomly selected examples studied by human.
ReAct vs. CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly
lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only
differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge
is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we
randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct
and CoT respectively (thus 200 examples in total), and manually labeled their success and failure
modes in Table 2. Some key observations are as follows:
A) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than
ReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the
problem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the
access of an external knowledge base.
B) While interleaving reasoning, action and observation steps improves ReAct‚Äôs grounded-
ness and trustworthiness, such a structural constraint also reduces its Ô¨Çexibility in formulating
reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent
error pattern speciÔ¨Åc toReAct, in which the model repetitively generates the previous thoughts and
actions, and we categorize it as part of ‚Äúreasoning error‚Äù as the model fails to reason about what the
proper next action to take and jump out of the loop4.
C) For ReAct, successfully retrieving informative knowledge via search is critical. Non-
informative search, which counts for 23% of the error cases, derails the model reasoning and gives
it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between
factuality and Ô¨Çexibility, which motivates our proposed strategies of combining two methods.
We provide examples for each success and failure modes in Appendix E.1. We also Ô¨Ånd some
HotpotQA questions may contain outdated answer labels, see Figure 4 for example.
ReAct + CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting
method on HotpotQA and Fever are ReAct‚Üí CoT-SC and CoT-SC‚Üí ReAct respectively.
Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC
samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both
signiÔ¨Åcantly and consistently outperform CoT-SC across different number of samples, reaching
CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of
properly combining model internal knowledge and external knowledge for reasoning tasks.
ReAct performs best for Ô¨Åne-tuning Figure 3 shows the scaling effect of prompting/Ô¨Ånetuning
four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, promptingReAct
performs worst among four methods due to the difÔ¨Åculty to learn both reasoning and acting from
in-context examples. However, when Ô¨Ånetuned with just 3,000 examples,ReAct becomes the best
method among the four, with PaLM-8B Ô¨ÅnetunedReAct outperforming all PaLM-62B prompting
methods, and PaLM-62B Ô¨ÅnetunedReAct outperforming all 540B prompting methods. In contrast,
Ô¨ÅnetuningStandard or CoT is signiÔ¨Åcantly worse than Ô¨ÅnetuningReAct or Act for both PaLM-
8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge
facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a
more generalizable skill for knowledge reasoning. As all prompting methods are still signiÔ¨Åcantly
far from domain-speciÔ¨Åc state-of-the-art approaches (Table 1), we believe Ô¨Ånetuning with more
human-written data might be a better way to unleash the power of ReAct.
4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using
better decoding (e.g. beam search) might help address this issue.
6
Published as a conference paper at ICLR 2023
8b 62b 540b
size
0
5
10
15
20
25
30HotpotQA EM
learning = prompt
8b 62b 540b
size
learning = finetune
Method
Standard
CoT
Act
ReAct
Figure 3: Scaling results for prompting and Ô¨Ånetuning on HotPotQA withReAct (ours) and baselines.
4 D ECISION MAKING TASKS
We also test ReAct on two language-based interactive decision-making tasks, ALFWorld and
WebShop, both of which feature complex environments that require agents to act over long horizons
with sparse rewards, warranting the need for reasoning to act and explore effectively.
ALFWorld ALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed
to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of
tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by
navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take
paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy
more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore
systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into
ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will
likely be on desks, shelfs, or dressers), making this environment a good Ô¨Åt for LLMs to exploit their
pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories
from the training set for each task type, where each trajectory includes sparse thoughts that (1)
decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via
commonsense where to Ô¨Ånd an object and what to do with it. We show prompts used for ALFWorld
in Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games
in a task-speciÔ¨Åc setup. For robustness, we construct 6 prompts for each task type through each
permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using
the same trajectories, but without thoughts ‚Äî since task instances are randomly chosen from the
training set, it favors neitherReAct nor Act and provides a fair and controlled comparison to test the
importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation
learning agent trained on 105 expert trajectories for each task type5.
WebShop Can ReAct also interact with noisy real-world language environments for practical
applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping
website environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,
Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,
and options crawled from Amazon), and requires an agent to purchase a product based on a user
instruction (e.g. ‚ÄúI am looking for a nightstand with drawers. It should have a nickel Ô¨Ånish, and
priced lower than $140‚Äù) through web interactions (e.g. search ‚Äúnightstand drawers‚Äù, choose buttons
such as ‚Äúcolor: modern-nickel-white‚Äù or ‚Äúback to search‚Äù). This task is evaluated by average score
(percentage of desired attributes covered by the chosen product averaged across all episodes) and
success rate (percentage of episodes where the chosen product satisÔ¨Åes all requirements) on 500 test
instructions. We formulate Act prompts with actions to search, choose product, choose options,
and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,
and what products options are relevant to the instruction. See Table 6 for an example prompt, and
Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method
5Micheli & Fleuret (2021) Ô¨Ånetuned a GPT-2 model on 3553 task instances and achieved a much improved
performance than BUTLER, but it is trained on all task types, thus not included as a baseline.
7
Published as a conference paper at ICLR 2023
Method Pick Clean Heat Cool Look Pick 2 All
Act(best of 6) 88 42 74 67 72 41 45
ReAct(avg) 65 39 83 76 55 24 57
ReAct(best of 6) 92 58 96 86 78 41 71
ReAct-IM(avg) 55 59 60 55 23 24 48
ReAct-IM(best of 6) 62 68 87 57 39 33 53
BUTLERg (best of 8) 33 26 70 76 17 12 22
BUTLER(best of 8) 46 39 74 100 22 24 37
Table 3: AlfWorld task-speciÔ¨Åc success rates (%). BUTLER and
BUTLERg results are from Table 4 of Shridhar et al. (2020b). All
methods use greedy decoding, except that BUTLER uses beam search.
Method Score SR
Act 62.3 30.1
ReAct 66.6 40.0
IL 59.9 29.1
IL+RL 62.4 28.7
Human 82.1 59.6Expert
Table 4: Score and suc-
cess rate (SR) on Web-
shop. IL/IL+RL taken
from Yao et al. (2022).
trained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)
method additionally trained with 10,587 training instructions.
Results ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On
ALFWorld, the bestReAct trial achieves an average success rate of 71%, signiÔ¨Åcantly outperforming
the best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats
the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across
six controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.
Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals
into smaller subgoals, or loses track of the current state of the environment. Example trajectories
comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.
On Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With
additional sparse reasoning, ReAct achieves signiÔ¨Åcantly better performance, with an absolute 10%
improvement over the previous best success rate. By checking examples, we Ô¨Ånd thatReAct is more
likely to identify instruction-relevant products and options by reasoning to bridge the gap between
noisy observations and actions (e.g. ‚ÄúFor ‚Äòspace-saving ottoman bench for living room‚Äô, the item
has options ‚Äò39x18x18inch‚Äô and ‚Äòblue‚Äô and seems good to buy.‚Äù). However, existing methods are
still far from the performance of expert humans (Table 4), who perform signiÔ¨Åcantly more product
explorations and query re-formulations that are still challenging for prompting-based methods.
On the value of internal reasoning vs. external feedback To our knowledge, ReAct is the Ô¨Årst
demonstration of combined reasoning and action using an LLM applied to an interactive environment
within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang
et al. (2022b), in which actions from an embodied agent are motivated by an eponymous ‚Äúinner
monologue‚Äù. However, IM‚Äôs ‚Äúinner monologue‚Äù is limited to observations of the environment
state and what needs to be completed by the agent for the goal to be satisÔ¨Åed. In contrast, the
reasoning traces in ReAct for decision making is Ô¨Çexible and sparse, allowing diverse reasoning
types (see Section 2) to be induced for different tasks.
To demonstrate the differences between ReAct and IM, and to highlight the importance of internal
reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought
pattern composed of IM-like dense external feedback. As can be seen in Table 3,ReAct substantially
outperforms IM-style prompting ( ReAct-IM) (71 vs. 53 overall success rate), with consistent
advantages on Ô¨Åve out of six tasks. Qualitatively, we observed thatReAct-IM often made mistakes
in identifying when subgoals were Ô¨Ånished, or what the next subgoal should be, due to a lack of high-
level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where
an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.
Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in
Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example
trajectory in Appendix D.2.3.
8
Published as a conference paper at ICLR 2023
5 R ELATED WORK
Language model for reasoning Perhaps the most well-known work of using LLMs for reasoning
is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their
own ‚Äúthinking procedure‚Äù for problem solving. Several follow-up works have since been performed,
including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-
CoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,
(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and
observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other
work has also been extended to more sophisticated reasoning architecture beyond simple prompting.
For example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps
of ‚Äúselection‚Äù and ‚Äúinference‚Äù. STaR (Zelikman et al., 2022) bootstraps the reasoning process by
Ô¨Ånetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell
& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated
LM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which Ô¨Ånetunes a LM on
intermediate computation steps, also demonstrate improvement on multi-step computation problems.
In contrast to these methods, ReAct performs more than just isolated, Ô¨Åxed reasoning, and integrates
model actions and their corresponding observations into a coherent stream of inputs for the model to
reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).
Language model for decision making The strong capability of LLMs has enabled them to perform
tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a
policy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,
2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to
complicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not
explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for
reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)
and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl
et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly
consider the reasoning procedure either, and also relies on expensive datasets and human feedback
collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the
decision making process only requires language description of the reasoning procedure.6
LLMS have also been increasingly employed in interactive and embodied environments for planning
and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)
and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision
making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which
is then reranked by an affordance model grounded on the visual environments for Ô¨Ånal prediction.
Inner Monologue made further improvements by adding the eponymous ‚Äúinner monologue", which is
implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the
Ô¨Årst work that demonstrates such a closed-loop system, whichReAct builds on. However, we argue
that Inner Monologue does not truly comprise of inner thoughts ‚Äî this is elaborated in Section 4. We
also note that leveraging language as semantically-rich inputs in the process of interactive decision
making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti
et al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of
LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and
decision making. What is more, progress in LLMs has also inspired the development of versatile and
generalist agents like Reed et al. (2022).
6 C ONCLUSION
We have proposed ReAct ‚Äì a simple yet effective method for synergizing reasoning and acting in
large language models. Through a diverse set of experiments on multi-hop question-answering, fact
checking, and interactive decision-making tasks, we show that ReAct leads to superior performance
with interpretable decision traces. Despite the simplicity of our method, complex tasks with large
action spaces require more demonstrations to learn well, which unfortunately can easily go beyond
the input length limit of in-context learning. We explore the Ô¨Åne-tuning approach on HotpotQA
6Human feedback can also be incorporated in a complementary manner but we leave it for future work.
9
Published as a conference paper at ICLR 2023
with initial promising results, but learning from more high-quality human annotations will be the
desiderata to further improve the performance. Scaling up ReAct with multi-task training and
combining it with complementary paradigms like reinforcement learning could result in stronger
agents that further unlock the potential of LLMs for more applications.
ACKNOWLEDGMENTS
We thank the support and feedback of many people from Google Brain team and Princeton NLP
Group. This work was supported in part by the National Science Foundation under Grant No.
2107048. Any opinions, Ô¨Åndings, and conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reÔ¨Çect the views of the National Science Foundation.
REPRODUCIBILITY STATEMENT
Our main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible
model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional
experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct
prompting code at https://anonymous.4open.science/r/ReAct-2268/.
ETHICS STATEMENT
ReAct prompts large language models to generate more human interpretable, diagnosable, and
controllable task-solving trajectories than previous methods. However, hooking up a large language
model with an action space to interact with external environments (e.g. the web, physical environ-
ments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful
actions in an environment. Our experiments minimize such risks by limiting the interactions to
speciÔ¨Åc websites (Wikipedia or WebShop) that are free of private information, without any dangerous
actions in the action space design (i.e. models cannot really buy products on WebShop the research
benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing
more extensive experiments in the future.
REFERENCES
Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita
Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim
Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew-
son, SoÀána Mokr√°, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,
Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,
2020. URL https://arxiv.org/abs/2012.05672.
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally
Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,
Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka
Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander
Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and
Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL
https://arxiv.org/abs/2204.01691.
Ben Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions,
phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.
Alan Baddeley. Working memory. Science, 255(5044):556‚Äì559, 1992.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877‚Äì1901, 2020.
10
Published as a conference paper at ICLR 2023
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL
https://arxiv.org/abs/2208.14271.
Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large
language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/
2205.09712.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pp. 3558‚Äì3567, Florence, Italy, July 2019. Association for Com-
putational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/
P19-1346.
Charles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social
interaction and the development of social understanding and executive functions , pp. 56‚Äì79, 2010.
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-
beth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,
Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth
Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,
SoÀána Mokr√°, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,
William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and
Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements,
2022. URL https://storage.googleapis.com/deepmind-media/DeepMind.
com/Authors-Notes/sparrow/sparrow-final.pdf.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple
language model for task-oriented dialogue. Advances in Neural Information Processing Systems ,
33:20179‚Äì20191, 2020.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,
2022a.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan
Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through
planning with language models. arXiv preprint arXiv:2207.05608, 2022b.
Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed
latent actions. In CoRL, pp. 1379‚Äì1390, 2021. URL https://proceedings.mlr.press/
v164/karamcheti22a.html.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-
augmented language models through few-shot prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115, 2022.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:
9459‚Äì9474, 2020.
Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An
Huang, Ekin Aky√ºrek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,
and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:
//arxiv.org/abs/2202.01771.
11
Published as a conference paper at ICLR 2023
Aleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-
chologia, 3(4):387‚Äì392, 1965.
Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes
two to tango, 2022. URL https://arxiv.org/abs/2209.07686.
Vincent Micheli and Fran√ßois Fleuret. Language models are few-shot butlers. arXiv preprint
arXiv:2104.07972, 2021.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,
Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:
Browser-assisted question-answering with human feedback, 2021. URL https://arxiv.
org/abs/2112.09332.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and
Augustus Odena. Show your work: Scratchpads for intermediate computation with language
models, 2021. URL https://arxiv.org/abs/2112.00114.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom
Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,
Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:
//arxiv.org/abs/2205.06175.
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,
Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions
for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 10740‚Äì10749, 2020a.
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and Matthew
Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv
preprint arXiv:2010.03768, 2020b.
Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.
Language models that seek for knowledge: Modular search & generation for dialogue and prompt
completion. arXiv preprint arXiv:2203.13224, 2022a.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,
Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman
Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:
a deployed conversational agent that continually learns to responsibly engage, 2022b. URL
https://arxiv.org/abs/2208.03188.
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale
dataset for fact extraction and veriÔ¨Åcation. arXiv preprint arXiv:1803.05355, 2018.
Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky , 1:39‚Äì285, 1987.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,
2022a. URL https://arxiv.org/abs/2203.11171.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented
ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,
and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
answering. arXiv preprint arXiv:1809.09600, 2018.
12
Published as a conference paper at ICLR 2023
Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:
Language models for action generation in text-based games. InProceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pp. 8736‚Äì8754, Online, Novem-
ber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.
URL https://aclanthology.org/2020.emnlp-main.704.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206 ,
2022.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with
reasoning, 2022. URL https://arxiv.org/abs/2203.14465.
Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in
large language models, 2022. URL https://arxiv.org/abs/2205.10625.
Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information
seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.
13
Published as a conference paper at ICLR 2023
A A DDITIONAL RESULTS
A.1 GPT-3 E XPERIMENTS
PaLM-540B GPT-3
HotpotQA (exact match) 29.4 30.8
ALFWorld (success rate %) 70.9 78.4
Table 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).
On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all
134 unseen validation task instances, and use the best prompt set according to PaLM-540B.
We run additional GPT-3 (Brown et al., 2020) experiments to conÔ¨ÅrmReAct prompting performance
is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,
greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly
because it is Ô¨Ånetuned with human instruction following. This indicates ReAct prompting is effective
across different large language models on different tasks. The code for these experiments are at
https://react-lm.github.io/.
A.2 REAC T OBTAINS UP -TO-DATE KNOWLEDGE ON HOTPOT QA
D6WDQGDUG
$QVZHU
E&R75HDVRQ2QO\
7KRXJKW/HW
VWKLQNVWHS
E\VWHS7KHKRWHOWKDWLV
KRPHWRWKH&LUTXHGX
6ROHLOVKRZ0\VWHUHLV
7UHDVXUH,VODQG7UHDVXUH
,VODQGKDVURRPVVR
WKHDQVZHULV
$QVZHU
F$FW2QO\
$FW6HDUFK>&LUTXHGX6ROHLO@
2EV&LUTXHGX6ROHLOLVD
&DQDGLDQHQWHUWDLQPHQWFRPSDQ\¬™
$FW/RRNXS>0\VWHUH@
2EV1RPRUHUHVXOWV
$FW6HDUFK>0\VWHUH&LUTXHGX
6ROHLOVKRZ@
2EV&RXOGQRWILQG¬™
$FW6HDUFK>0\VWHUH&LUTXHGX
6ROHLO@
2EV0\VWqUHP\VWHHULV
RQHRIVL[¬™
$FW/RRNXS>7UHDVXUH,VODQG
+RWHODQG&DVLQR@
¬™¬™HQGZLWKRXWDQVZHU
G5H$FW5HDVRQ$FW
7KRXJKW,QHHGWRVHDUFK&LUTXHGX6ROHLOVKRZ0\VWHUH
ILQGWKHKRWHOLWLVLQWKHQILQGWKHQXPEHURIURRPVLQ
WKHKRWHO
$FW6HDUFK>&LUTXHGX6ROHLOVKRZ0\VWHUH@
2EV&RXOGQRWILQG&LUTXHGX6ROHLOVKRZ0\VWHUH
6LPLODU>
&LUTXHGX6ROHLO
¬™
7KRXJKW7RILQGWKHKRWHO,FDQVHDUFK0\VWHUH&LUTXH
GX6ROHLO
$FW6HDUFK>0\VWHUH&LUTXHGX6ROHLO@
2EV0\VWqUHP\VWHHULV¬™+HOGLQDFXVWRPWKHDWUHDW
WKH7UHDVXUH,VODQG+RWHODQG&DVLQR¬™
7KRXJKW0\VWHUHLVKHOGLQDFXVWRPWKHDWUHDWWKH
7UHDVXUH,VODQG+RWHODQG&DVLQR6R,QHHGWRVHDUFK
7UHDVXUH,VODQG+RWHODQG&DVLQRQH[WDQGILQGWKHQXPEHU
RIURRPVLQWKHKRWHO
$FW6HDUFK>7UHDVXUH,VODQG+RWHODQG&DVLQR@
2EV7UHDVXUH,VODQG+RWHODQG&DVLQR¬™ZLWKURRPV
DQGVXLWHV¬™
7KRXJKW7UHDVXUH,VODQG+RWHODQG&DVLQRKDVURRPV
DQGVXLWHV6RWKHDQVZHULV
$FW)LQLVK>@ “º
4XHVWLRQ+RZPDQ\URRPVDUHLQWKHKRWHOWKDWLVKRPHWR
WKH&LUTXHGX6ROHLOVKRZ0\VWHUH"
+RWSRW4$ODEHO
+RWVSRW4$
2XWGDWHG
8SWRGDWH
Figure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is
able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.
During trajectory inspection, we also Ô¨Ånd that sometimesReAct does not agree with dataset labels as
the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about
the size of a hotel, which increased from the HotpotQA construction time. WhileStandard and CoT
give wrong answers due to hallucination, Act fails despite the access of real-world web interaction,
due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to
retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better
incorporation of reasoning abilities might beneÔ¨Åt recent Internet-augmented language models (Nakano
et al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.
A.3 H UMAN -IN-THE -LOOP BEHAVIOR CORRECTION ON ALFWORLD
We also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit
ReAct‚Äôs reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act
17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align
with these human thought edits and succeed in the task. From a human perspective, solving such a
task becomes signiÔ¨Åcantly easier, from typing tens of actions to only editing a couple of thoughts,
which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go
14
Published as a conference paper at ICLR 2023
D5H$FW

2EV<RXRSHQWKHGUDZHU7KHGUDZHULVRSHQ,QLW\RX
VHHDNH\FKDLQDQGDZDWFK
$FW7KLQN>1RZ,ILQGWKHILUVWNH\FKDLQ1H[W,QHHGWR
WDNHLW,FDQILQGWKHVHFRQGNH\FKDLQODWHULQGUDZHU@
$FWWDNHNH\FKDLQIURPGUDZHU
2EV<RXSLFNXSWKHNH\FKDLQIURPWKHGUDZHU

$FWSXWNH\FKDLQLQRQVDIH
2EV<RXSXWWKHNH\FKDLQLQRQWKHVDIH
$FW7KLQN>1RZ,SXWWKHILUVWNH\FKDLQLQVDIH1H[W,QHHG
WRILQGWKHVHFRQGNH\FKDLQ,FDQGLUHFWO\JRWRGUDZHU@
$FWJRWRGUDZHU
2EV7KHGUDZHULVRSHQ,QLW\RXVHHDZDWFK
$FW7KLQN>1RZ,ILQGWKHVHFRQGNH\FKDLQ1H[W,QHHG
WRWDNHLW@
$FWWDNHNH\FKDLQIURPGUDZHU
2EV1RWKLQJKDSSHQV
E5H$FW+XPDQ7KRXJKWHGLW

2EV<RXRSHQWKHGUDZHU7KHGUDZHULVRSHQ,QLW\RXVHHD
NH\FKDLQDQGDZDWFK
$FW—ï7KLQN—ï>1RZ,ILQGWKHILUVWNH\FKDLQ1H[W,QHHGWRWDNHLW,
FDQILQGWKHVHFRQGNH\FKDLQODWHULQGUDZHU@
$FWWDNHNH\FKDLQIURPGUDZHU
2EV<RXSLFNXSWKHNH\FKDLQIURPWKHGUDZHU

$FWSXWNH\FKDLQLQRQVDIH
2EV<RXSXWWKHNH\FKDLQLQRQWKHVDIH
$FW7KLQN—ï>1RZ,SXWWKHILUVWNH\FKDLQLQVDIH1H[W,QHHGWRILQG
WKHVHFRQGNH\FKDLQZKLFKLVPRUHOLNHO\WRDSSHDULQGUHVVHU
JDUEDJHFDQVDIHVLGHWDEOHVRIDVKHOI@
$FWJRWRGUHVVHU
2EV2QWKHGUHVVHU\RXVHHDER[DFUHGLWFDUGDNH\FKDLQD
NH\FKDLQDODSWRS
$FW7KLQN—ï>1RZ,ILQGWKHVHFRQGNH\FKDLQ1H[W,QHHGWRWDNH
LW@
$FWWDNHNH\FKDLQIURPGUHVVHU
2EV<RXSLFNXSWKHNH\FKDLQIURPWKHGUHVVHU
$FW7KLQN—ï>1RZ,WDNHWKHVHFRQGNH\FKDLQ1H[W,QHHGWRSXWLW
LQRQVDIH@
$FWJRWRVDIH
2EV7KHVDIHLVRSHQ,QLW\RXVHHDNH\FKDLQDVWDWXHDQGD
ZDWFK
$FWSXWNH\FKDLQLQRQVDIH
2EV<RXSXWWKHNH\FKDLQLQRQWKHVDIH“º
<RXDUHLQWKHPLGGOHRIDURRP/RRNLQJTXLFNO\DURXQG\RX
\RXVHHDDUPFKDLUDFDELQHW¬™DFDELQHWDGUDZHU
¬™DGUDZHUDGUHVVHUDJDUEDJHFDQDVDIHDVKHOI
¬™DVKHOIDVLGHWDEOHDQGDVRID
<RXUWDVNLVWRSXWWZRNH\FKDLQLQVDIH
$OI:RUOG
Figure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct
trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts
(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.
is difÔ¨Åcult forAct and previous RL methods, as a human cannot change the model parameters, and
changing a few actions might not edit the rest of the model behavior. This paradigm is also more than
human dialogue to update the goal or subgoal as in Huang et al. (2022b) ‚Äî while editing ReAct
thoughts can do these, it can also modify the model‚Äôs internal belief, reasoning styles, or anything the
Ô¨Çexible thought space supports, for better task solving. We believe this is an exciting direction for
human alignment and leave more systematic study as future work.
B E XPERIMENT DETAILS
B.1 H OTPOT QA F INETUNING DETAILS
For all Ô¨Ånetuning we use a batch size of 64. On PaLM-8B, we Ô¨ÅnetuneReAct and Act methods
for 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we Ô¨Ånetune
ReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We
Ô¨ÅndReAct and Act methods generally beneÔ¨Åt from more training steps (and more training data),
while Standard and CoT methods degrade soon after Ô¨Ånetuning.
B.2 A LFWORLD IM-S TYLE DETAILS
For the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense
external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)
decomposing the current goal and (2) the current subgoal that needs to be completed. In particular,
ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the
next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify
where items can be within the environment.
15
Published as a conference paper at ICLR 2023
C P ROMPTS
C.1 H OTPOT QA
Hotpot QA Prompts
Original
Question What is the elevation range for the area that the eastern sector of the
Colorado orogeny extends into?
Answer 1,800 to 7,000 ft
Question Musician and satirist Allie Goertz wrote a song about the "The Simpsons"
character Milhouse, who Matt Groening named after who?
Answer Richard Nixon
Question Which documentary is about Finnish rock groups, Adam Clayton Powell or The
Saimaa Gesture?
Answer The Saimaa Gesture
Question What profession does Nicholas Ray and Elia Kazan have in common?
Answer director, screenwriter, actor
Question Which magazine was started first Arthur‚Äôs Magazine or First for Women?
Answer Arthur‚Äôs Magazine
Question Were Pavel Urysohn and Leonid Levin known for the same type of work?
Answer Yes
Act
Question What is the elevation range for the area that the eastern sector of the
Colorado orogeny extends into?
Action 1 Search[Colorado orogeny]
Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in
Colorado and surrounding areas.
Action 2 Lookup[eastern sector]
Observation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called
the Central Plains orogeny.
Action 3 Search[High Plains]
Observation 3 High Plains refers to one of two distinct land regions:
Action 4 Search[High Plains (United States)]
Observation 4 The High Plains are a subregion of the Great Plains. From east to west, the
High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130
m).[3]
Action 5 Finish[1,800 to 7,000 ft]
Question Musician and satirist Allie Goertz wrote a song about the "The Simpsons"
character Milhouse, who Matt Groening named after who?
Action 1 Search[Milhouse]
Observation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated
television series The Simpsons voiced by Pamela Hayden and created by Matt
Groening.
Action 2 Lookup[named after]
Observation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose
middle name was Milhous.
Action 3 Finish[Richard Nixon]
Question Which documentary is about Finnish rock groups, Adam Clayton Powell or The
Saimaa Gesture?
Action 1 Search[Adam Clayton Powell]
Observation 1 Could not find [Adam Clayton Powell]. Similar: [‚ÄôAdam Clayton Powell
III‚Äô, ‚ÄôSeventh Avenue (Manhattan)‚Äô, ‚ÄôAdam Clayton Powell Jr. State Office
Building‚Äô, ‚ÄôIsabel Washington Powell‚Äô, ‚ÄôAdam Powell‚Äô, ‚ÄôAdam Clayton Powell
(film)‚Äô, ‚ÄôGiancarlo Esposito‚Äô].
Action 2 Search[Adam Clayton Powell (film)]
Observation 2 Adam Clayton Powell is a 1989 American documentary film directed by
Richard Kilberg. The film is about the rise and fall of influential
African-American politician Adam Clayton Powell Jr.[3][4] It was later aired
as part of the PBS series The American Experience.
Action 3 Finish[The Saimaa Gesture]
Question What profession does Nicholas Ray and Elia Kazan have in common?
Action 1 Search[Nicholas Ray]
Observation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,
1979) was an American film director, screenwriter, and actor best known for
the 1955 film Rebel Without a Cause.
Action 2 Search[Elia Kazan]
Observation 2 Elia Kazan was an American film and theatre director, producer, screenwriter
and actor.
Continued on next page
16
Published as a conference paper at ICLR 2023
Hotpot QA Prompts ‚Äì Continued from previous page
Action 3 Finish[director, screenwriter, actor]
Question Which magazine was started first Arthur‚Äôs Magazine or First for Women?
Action 1 Search[Arthur‚Äôs Magazine]
Observation 1 Arthur‚Äôs Magazine (1844-1846) was an American literary periodical published
in Philadelphia in the 19th century.
Action 2 Search[First for Women]
Observation 2 First for Women is a woman‚Äôs magazine published by Bauer Media Group in the
USA.[1] The magazine was started in 1989.
Action 3 Finish[Arthur‚Äôs Magazine]
Question Were Pavel Urysohn and Leonid Levin known for the same type of work?
Action 1 Search[Pavel Urysohn]
Observation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet
mathematician who is best known for his contributions in dimension theory.
Action 2 Search[Leonid Levin]
Observation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer
scientist.
Action 3 Finish[yes]
CoT
Question What is the elevation range for the area that the eastern sector of the
Colorado orogeny extends into?
Thought Let‚Äôs think step by step. The eastern sector of Colorado orogeny extends
into the High Plains. High Plains rise in elevation from around 1,800 to
7,000 ft, so the answer is 1,800 to 7,000 ft.
Answer 1,800 to 7,000 ft
Question Musician and satirist Allie Goertz wrote a song about the "The Simpsons"
character Milhouse, who Matt Groening named after who?
Thought Let‚Äôs think step by step. Milhouse was named after U.S. president Richard
Nixon, so the answer is Richard Nixon.
Answer Richard Nixon
Question Which documentary is about Finnish rock groups, Adam Clayton Powell or The
Saimaa Gesture?
Thought Let‚Äôs think step by step. Adam Clayton Powell (film) is a documentary about
an African-American politician, not Finnish rock groups. So the documentary
about Finnish rock groups must instead be The Saimaa Gesture.
Answer The Saimaa Gesture
Question What profession does Nicholas Ray and Elia Kazan have in common?
Thought Let‚Äôs think step by step. Professions of Nicholas Ray are director,
screenwriter, and actor. Professions of Elia Kazan are director, producer,
screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in
common is director, screenwriter, and actor.
Answer director, screenwriter, actor
Question Which magazine was started first Arthur‚Äôs Magazine or First for Women?
Thought Let‚Äôs think step by step. Arthur‚Äôs Magazine was started in 1844. First
for Women was started in 1989. 1844 (Arthur‚Äôs Magazine) < 1989 (First for
Women), so Arthur‚Äôs Magazine was started first.
Answer Arthur‚Äôs Magazine
Question Were Pavel Urysohn and Leonid Levin known for the same type of work?
Thought Let‚Äôs think step by step. Pavel Urysohn is a mathematician. Leonid Levin
is a mathematician and computer scientist. So Pavel Urysohn and Leonid
Levin have the same type of work.
Answer Yes
ReAct
Question What is the elevation range for the area that the eastern sector of the
Colorado orogeny extends into?
Thought 1 I need to search Colorado orogeny, find the area that the eastern sector
of the Colorado orogeny extends into, then find the elevation range of the
area.
Action 1 Search[Colorado orogeny]
Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in
Colorado and surrounding areas.
Thought 2 It does not mention the eastern sector. So I need to look up eastern
sector.
Action 2 Lookup[eastern sector]
Observation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called
the Central Plains orogeny.
Thought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I
need to search High Plains and find its elevation range.
Action 3 Search[High Plains]
Observation 3 High Plains refers to one of two distinct land regions
Thought 4 I need to instead search High Plains (United States).
Action 4 Search[High Plains (United States)]
Continued on next page
17
Published as a conference paper at ICLR 2023
Hotpot QA Prompts ‚Äì Continued from previous page
Observation 4 The High Plains are a subregion of the Great Plains. From east to west, the
High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130
m).[3]
Thought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer
is 1,800 to 7,000 ft.
Action 5 Finish[1,800 to 7,000 ft]
Question Musician and satirist Allie Goertz wrote a song about the "The Simpsons"
character Milhouse, who Matt Groening named after who?
Thought 1 The question simplifies to "The Simpsons" character Milhouse is named after
who. I only need to search Milhouse and find who it is named after.
Action 1 Search[Milhouse]
Observation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated
television series The Simpsons voiced by Pamela Hayden and created by Matt
Groening.
Thought 2 The paragraph does not tell who Milhouse is named after, maybe I can look up
"named after".
Action 2 Lookup[named after]
Observation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose
middle name was Milhous.
Thought 3 Milhouse was named after U.S. president Richard Nixon, so the answer is
Richard Nixon.
Action 3 Finish[Richard Nixon]
Question Which documentary is about Finnish rock groups, Adam Clayton Powell or The
Saimaa Gesture?
Thought 1 I need to search Adam Clayton Powell and The Saimaa Gesture, and find which
documentary is about Finnish rock groups.
Action 1 Search[Adam Clayton Powell]
Observation 1 Could not find [Adam Clayton Powell]. Similar: [‚ÄôAdam Clayton Powell
III‚Äô, ‚ÄôSeventh Avenue (Manhattan)‚Äô, ‚ÄôAdam Clayton Powell Jr. State Office
Building‚Äô, ‚ÄôIsabel Washington Powell‚Äô, ‚ÄôAdam Powell‚Äô, ‚ÄôAdam Clayton Powell
(film)‚Äô, ‚ÄôGiancarlo Esposito‚Äô].
Thought 2 To find the documentary, I can search Adam Clayton Powell (film).
Action 2 Search[Adam Clayton Powell (film)]
Observation 2 Adam Clayton Powell is a 1989 American documentary film directed by
Richard Kilberg. The film is about the rise and fall of influential
African-American politician Adam Clayton Powell Jr.[3][4] It was later aired
as part of the PBS series The American Experience.
Thought 3 Adam Clayton Powell (film) is a documentary about an African-American
politician, not Finnish rock groups. So the documentary about Finnish rock
groups must instead be The Saimaa Gesture.
Action 3 Finish[The Saimaa Gesture]
Question What profession does Nicholas Ray and Elia Kazan have in common?
Thought 1 I need to search Nicholas Ray and Elia Kazan, find their professions, then
find the profession they have in common.
Action 1 Search[Nicholas Ray]
Observation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,
1979) was an American film director, screenwriter, and actor best known for
the 1955 film Rebel Without a Cause.
Thought 2 Professions of Nicholas Ray are director, screenwriter, and actor. I need
to search Elia Kazan next and find his professions.
Action 2 Search[Elia Kazan]
Observation 2 Elia Kazan was an American film and theatre director, producer, screenwriter
and actor.
Thought 3 Professions of Elia Kazan are director, producer, screenwriter, and actor.
So profession Nicholas Ray and Elia Kazan have in common is director,
screenwriter, and actor.
Action 3 Finish[director, screenwriter, actor]
Question Which magazine was started first Arthur‚Äôs Magazine or First for Women?
Thought 1 I need to search Arthur‚Äôs Magazine and First for Women, and find which was
started first.
Action 1 Search[Arthur‚Äôs Magazine]
Observation 1 Arthur‚Äôs Magazine (1844-1846) was an American literary periodical published
in Philadelphia in the 19th century.
Thought 2 Arthur‚Äôs Magazine was started in 1844. I need to search First for Women
next.
Action 2 Search[First for Women]
Observation 2 First for Women is a woman‚Äôs magazine published by Bauer Media Group in the
USA.[1] The magazine was started in 1989.
Thought 3 First for Women was started in 1989. 1844 (Arthur‚Äôs Magazine) < 1989 (First
for Women), so Arthur‚Äôs Magazine was started first.
Action 3 Finish[Arthur‚Äôs Magazine]
Question Were Pavel Urysohn and Leonid Levin known for the same type of work?
Thought 1 I need to search Pavel Urysohn and Leonid Levin, find their types of work,
then find if they are the same.
Action 1 Search[Pavel Urysohn]
Continued on next page
18
Published as a conference paper at ICLR 2023
Hotpot QA Prompts ‚Äì Continued from previous page
Observation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet
mathematician who is best known for his contributions in dimension theory.
Thought 2 Pavel Urysohn is a mathematician. I need to search Leonid Levin next and
find its type of work.
Action 2 Search[Leonid Levin]
Observation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer
scientist.
Thought 3 Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn
and Leonid Levin have the same type of work.
Action 3 Finish[yes]
19
Published as a conference paper at ICLR 2023
C.2 F EVER
FEVER Prompts
Original Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if
there is NOT ENOUGH INFORMATION.
Claim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.
Answer SUPPORTS
Claim Stranger Things is set in Bloomington, Indiana.
Answer REFUTES
Claim Beautiful reached number two on the Billboard Hot 100 in 2003.
Answer NOT ENOUGH INFO
Act Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if
there is NOT ENOUGH INFORMATION.
Claim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.
Action2 Search[Nikolaj Coster-Waldau]
Observation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and
producer. He graduated from the Danish National School of Performing Arts
in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with
the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy
drama series Game of Thrones, for which he received two Primetime Emmy
Award nominations for Outstanding Supporting Actor in a Drama Series..
Coster-Waldau has appeared in numerous films in his native Denmark and
Scandinavia, including Headhunters (2011) and A Thousand Times Good Night
(2013). In the U.S, his debut film role was in the war film Black Hawk Down
(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a
detective in the short-lived Fox television series New Amsterdam (2008), and
appeared in the 2009 Fox television film Virtuality, originally intended as
a pilot.
Action 2 Finish[SUPPORTS]
Claim Stranger Things is set in Bloomington, Indiana.
Action 1 Search[Stranger Things]
Observation 1 Stranger Things is an American science fiction horror drama television
series created by the Duffer Brothers. Set in the 1980s, primarily in
the fictional town of Hawkins, Indiana, the series centers on a number of
mysteries and supernatural events occurring around the town and their impact
on an ensemble of child and adult characters.
Action 2 Finish[REFUTES]
Claim Beautiful reached number two on the Billboard Hot 100 in 2003.
Action 1 Search[Beautiful]
Observation 1 Could not find [Beautiful]. Similar: [‚ÄôBeautiful‚Äô, ‚ÄôBeautiful, Beautiful‚Äô,
‚ÄôA Beautiful Mind (film)‚Äô, ‚ÄôBeautiful (Christina Aguilera song)‚Äô, ‚ÄôLife Is
Beautiful‚Äô].
Action 2 Search[Beautiful (Christina Aguilera song)
Observation 2 "Beautiful" is a song recorded by American singer Christina Aguilera for her
fourth studio album, Stripped (2002).
Action 3 Lookup[Billboard Hot 100]
Observation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the
United States, where it was certified Gold for 500,000 units shipped.
Action 4 Finish[NOT ENOUGH INFO]
CoT Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if
there is NOT ENOUGH INFORMATION.
Claim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.
Thought Nikolaj William Coster-Waldau appeared in the 2009 Fox television film
Virtuality, so he has worked with the Fox Broadcasting Company.
Answer SUPPORTS
Claim Stranger Things is set in Bloomington, Indiana.
Thought Stranger Things is in the fictional town of Hawkins, Indiana, not in
Bloomington, Indiana.
Answer REFUTES
Continued on next page
20
Published as a conference paper at ICLR 2023
Fever Prompts ‚Äì Continued from previous page
Claim Beautiful reached number two on the Billboard Hot 100 in 2003.
Thought The song peaked at number two on the Billboard Hot 100 in the United States,
but not sure if it was in 2003.
Answer NOT ENOUGH INFO
ReAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if
there is NOT ENOUGH INFORMATION.
Claim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.
Thought 1 I need to search Nikolaj Coster-Waldau and find if he has worked with the
Fox Broadcasting Company.
Action 1 Search[Nikolaj Coster-Waldau]
Observation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and
producer. He graduated from the Danish National School of Performing Arts
in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with
the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy
drama series Game of Thrones, for which he received two Primetime Emmy
Award nominations for Outstanding Supporting Actor in a Drama Series..
Coster-Waldau has appeared in numerous films in his native Denmark and
Scandinavia, including Headhunters (2011) and A Thousand Times Good Night
(2013). In the U.S, his debut film role was in the war film Black Hawk Down
(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a
detective in the short-lived Fox television series New Amsterdam (2008), and
appeared in the 2009 Fox television film Virtuality, originally intended as
a pilot.
Thought 2 Because he "appeared in the 2009 Fox television film Virtuality", he should
have worked with the Fox Broadcasting Company.
Action 2 Finish[SUPPORTS]
Claim Stranger Things is set in Bloomington, Indiana.
Thought 1 I should search for Stranger Things, and see if it is set in Bloomington,
Indiana.
Action 1 Search[Stranger Things]
Observation 1 Stranger Things is an American science fiction horror drama television
series created by the Duffer Brothers. Set in the 1980s, primarily in
the fictional town of Hawkins, Indiana, the series centers on a number of
mysteries and supernatural events occurring around the town and their impact
on an ensemble of child and adult characters.
Thought 2 The observation says that it is set in a "fictional town of Hawkins,
Indiana", so it is not set in Bloomington.
Action 2 Finish[REFUTES]
Claim Beautiful reached number two on the Billboard Hot 100 in 2003.
Thought 1 I need to search the song Beautiful and find if it reached number two on the
Billboard Hot 100 in 2003.
Action 1 Search[Beautiful]
Observation 1 Could not find [Beautiful]. Similar: [‚ÄôBeautiful‚Äô, ‚ÄôBeautiful, Beautiful‚Äô,
‚ÄôA Beautiful Mind (film)‚Äô, ‚ÄôBeautiful (Christina Aguilera song)‚Äô, ‚ÄôLife Is
Beautiful‚Äô].
Thought 2 From suggestions, I should search "Beautiful (Christina Aguilera song)" to
find the song.
Action 2 Search[Beautiful (Christina Aguilera song)]
Observation 2 "Beautiful" is a song recorded by American singer Christina Aguilera for her
fourth studio album, Stripped (2002).
Thought 3 It does not mention Billboard, so I need to look up "Billboard Hot 100" to
find if it reached number two on it in 2003.
Action 3 Lookup[Billboard Hot 100]
Observation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the
United States, where it was certified Gold for 500,000 units shipped.
Thought 4 It only says the song peaked at number two on the Billboard Hot 100, but not
if it was in 2003. I am not sure if this claim is true or not.
Action 4 Finish[NOT ENOUGH INFO]
21
Published as a conference paper at ICLR 2023
C.3 W EBSHOP
Table 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for
Act, and use the prompt on the right (add reasoning) for ReAct.
Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars
Act ReAct
Action: search
[
3 ounce bright citrus deodorant sensitive skin
]
Observation:[
Back to Search
]
Page 1 (Total results: 50)[
Next
]
[
B078GWRC1J
]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for
Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Calendula 3-Ounce
$10.99[
B078GTKVXY
]
Ginger Fresh Deodorant by Earth Mama | Natural and Safe for
Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Calendula 3-Ounce
$10.99[
B08KBVJ4XN
]
Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men
, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar
& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,
2.7 oz, 2-Pack)
$15.95
Action: click
[
B078GWRC1J
]
Observation:[
Back to Search
]
[
Prev
]
scent [assorted scents][bright citrus][calming lavender][ginger
[fresh] simply non-scents]
size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2
-pack)]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for
Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Calendula 3-Ounce
Price: $10.99
Rating: N.A.
[Description]
[Features]
[Reviews]
[Buy Now]
Action: click[bright citrus]
Observation: You have clicked bright citrus.
Action: click[3 ounce (pack of 1)]
Observation: You have clicked 3 ounce (pack of 1).
Action: click[Buy Now]
Action: search
[
3 ounce bright citrus deodorant sensitive skin
Observation:[
Back to Search
]
Page 1 (Total results: 50)[
Next
]
[
B078GWRC1J
]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for
Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Calendula 3-Ounce
$10.99[
B078GTKVXY
]
Ginger Fresh Deodorant by Earth Mama | Natural and Safe for
Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Calendula 3-Ounce
$10.99[
B08KBVJ4XN
]
Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men
, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar
& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,
2.7 oz, 2-Pack)
$15.95
Action: think[B078GWRC1J and B078GTKVXY are bright citrus
deodorant less then 50 dollars. I can check B078GWRC1J Ô¨Årst.
Observation: OK.
Action: click
[
B078GWRC1J
]
Observation:[
Back to Search
]
[
Prev
]
scent [assorted scents][bright citrus][calming lavender][ginger
[fresh] simply non-scents]
size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2
-pack)]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for
Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Calendula 3-Ounce
Price: $10.99
Rating: N.A.
[Description]
[Features]
[Reviews]
[Buy Now]
Action: think[For 3 ounce bottle of bright citrus deodorant
for sensitive skin, the item has options ‚Äôbright citrus‚Äô
and ‚Äô3 ounce (pack of 1)‚Äô and seems good to buy.]
Observation: OK.
Action: click[bright citrus]
Observation: You have clicked bright citrus.
Action: click[3 ounce (pack of 1)]
Observation: You have clicked 3 ounce (pack of 1).
Action: click[Buy Now]
22
Published as a conference paper at ICLR 2023
C.4 ALFW ORLD
Table 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.
You are in the middle of a room. Looking quickly around you, you see a cabinet
13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet
7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a
garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a
stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
Your task is to: put a clean lettuce in diningtable.
> go to fridge 1
The fridge 1 is closed.
> open fridge 1
You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a
potato 3, and a potato 2.
> go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a
fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a
peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.
> take lettuce 1 from diningtable 1
You pick up the lettuce 1 from the diningtable 1.
> go to sinkbasin 1
On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
> clean lettuce 1 with sinkbasin 1
You clean the lettuce 1 using the sinkbasin 1.
> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.
OK.
> go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork
2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a
spatula 3, a tomato 2, and a tomato 1.
> put lettuce 1 in/on diningtable 1
You put the lettuce 1 in/on the diningtable 1.
23
Published as a conference paper at ICLR 2023
Table 8: An ReAct prompt on the ALFWorld clean task.
You are in the middle of a room. Looking quickly around you, you see a cabinet
13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet
7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a
garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a
stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
Your task is to: put a clean lettuce in diningtable.
> think: To solve the task, I need to find and take a lettuce, then clean it with
sinkbasin, then put it in diningtable.
OK.
> think: First I need to find a lettuce. A lettuce is more likely to appear in
fridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I
can check one by one, starting with fridge 1.
OK.
> go to fridge 1
The fridge 1 is closed.
> open fridge 1
You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a
potato 3, and a potato 2.
> go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a
fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a
peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.
> think: Now I find a lettuce (1). Next, I need to take it.
OK.
> take lettuce 1 from diningtable 1
You pick up the lettuce 1 from the diningtable 1.
> think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean
it.
OK.
> go to sinkbasin 1
On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
> clean lettuce 1 with sinkbasin 1
You clean the lettuce 1 using the sinkbasin 1.
> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.
OK.
> go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork
2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a
spatula 3, a tomato 2, and a tomato 1.
> put lettuce 1 in/on diningtable 1
You put the lettuce 1 in/on the diningtable 1.
24
Published as a conference paper at ICLR 2023
Table 9: An ReAct-IM prompt on the ALFWorld clean task.
You are in the middle of a room. Looking quickly around you, you see a cabinet
13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet
7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a
garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a
stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
Your task is to: put a clean lettuce in diningtable.
> think: To solve the task, I need to find and take a lettuce, then clean it with
sinkbasin, then put it in diningtable.
OK.
> think: First I need to find a lettuce.
OK.
> go to fridge 1
The fridge 1 is closed.
> think: First I need to find a lettuce.
OK.
> open fridge 1
You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a
potato 3, and a potato 2.
> think: First I need to find a lettuce.
OK.
> go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a
fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a
peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.
> think: First I need to find a lettuce.
OK.
> take lettuce 1 from diningtable 1
You pick up the lettuce 1 from the diningtable 1.
> think: I need to clean this lettuce (1) using sinkbasin 1.
OK.
> go to sinkbasin 1
On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
> think: I need to clean this lettuce (1) using sinkbasin 1.
OK.
> clean lettuce 1 with sinkbasin 1
You clean the lettuce 1 using the sinkbasin 1.
> think: I need to put this cleaned lettuce (1) in/on diningtable 1.
OK.
> go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork
2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a
spatula 3, a tomato 2, and a tomato 1.
> think: I need to put this cleaned lettuce (1) in/on diningtable 1.
OK.
> put lettuce 1 in/on diningtable 1
You put the lettuce 1 in/on the diningtable 1.
D T RAJECTORIES
D.1 FEVER T RAJECTORIES
We provide some trajectories produced by ReAct, CoTand Acton several randomly selected exam-
ples from the FEVER dev set below. Search results are omitted to space.
25
Published as a conference paper at ICLR 2023
Example 2491 (gt: REFUTES)
ReAct
Claim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.
Thought 1: I need to search Bermuda Triangle and find if it is in the Pacific Ocean.
Action 1: Search[Bermuda Triangle]
Observation 1: Reportedly haunted locations...
Thought 2: The observation says that it is in the western part of the North Atlantic
Ocean, so it is not in the Pacific Ocean.
Action 2: Finish[REFUTES]
Observation 2: Episode finished
Act
Claim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.
Action 1 : Search[Bermuda Triangle]
Observation 1 : Reportedly haunted locations:...
Action 2: Finish[REFUTES]
Observation 2: Episode finished
CoT
Claim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.
Thought: Let‚Äôs think step by step. The Bermuda Triangle is a loosely-defined region
in the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean.
Answer: REFUTES
Example 5908 (gt: SUPPORTS)
ReAct
Claim: Princess Mononoke is a film.
Thought 1: I need to search Princess Mononoke and find if it is a film.
Action 1: Search[Princess Mononoke]
Observation 1: Princess Mononoke ...
Thought 2: From the observation, it says that Princess Mononoke is a film.
Action 2: Finish[SUPPORTS]
Observation 2: Episode finished
Act
Claim: Princess Mononoke is a film.
Action 1: Search[Princess Mononoke]
Observation 1: Princess Mononoke ...
Action 2: Finish[SUPPORTS]
Observation 2: Episode finished
CoT
Claim: Princess Mononoke is a film.
Thought: Let‚Äôs think step by step. Princess Mononoke is a movie, and movies are
films.
Answer: SUPPORTS
Example 1951 (gt: REFUTES)
ReAct
Claim: Soyuz was part of the American space program.
Thought 1: I need to search Soyuz and find if it was part of the American space
program.
Action 1: Search[Soyuz]
Observation 1: Soyuz is a transliteration...
Thought 2: It does not mention the American space program, so I need to search
"American space program" to find if it was part of it.
Action 2: Search[American space program]
Observation 2: The space policy...
Thought 3: It does not mention Soyuz, so I am not sure if this claim is true or not.
Action 3: Finish[NOT ENOUGH INFO]
Observation 3: Episode finished
Act
Claim: Soyuz was part of the American space program.
Action 1: Search[Soyuz]
Observation 1: Soyuz is a transliteration...
Action 2: Finish[NOT ENOUGH INFO]
Observation 2: Episode finished
CoT
Claim: Soyuz was part of the American space program.
Thought: Let‚Äôs think step by step. Soyuz is a Russian spacecraft. The American
space program is NASA. NASA and Russia have worked together on the
International Space Station.
Answer: SUPPORTS
26
Published as a conference paper at ICLR 2023
Example 3208 (gt: REFUTES)
ReAct
Claim: Reign Over Me is an American film made in 2010.
Thought 1: I need to search for Reign Over Me and find if it is an American film made
in 2010.
Action 1: Search[Reign Over Me]
Observation 1: Reign Over Me ...
Thought 2: The observation says that it is an American film made in 2007, so it is not
made in 2010.
Action 2: Finish[REFUTES]
Observation 2: Episode finished
Act
Claim: Reign Over Me is an American film made in 2010.
Action 1: Search[Reign Over Me]
Action 2: Finish[SUPPORTS]
Observation 2: Episode finished
CoT
Claim: Reign Over Me is an American film made in 2010.
Thought: Let‚Äôs think step by step. First, is Reign Over Me an American film? Yes,
it is. Second, was it made in 2010? Yes, it was.
Answer: SUPPORTS
D.2 ALFW ORLD TRAJECTORIES
We provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,
pick_clean_then_place_in_recep-Knife-None-CounterTop-10/
trial_T20190909_110445_675754.
D.2.1 REAC T TRAJECTORY
ReAct is able to correctly Ô¨Ånd the knife, clean it, and then place it on a countertop.
27
Published as a conference paper at ICLR 2023
ReAct ALFWorld trajectory
You are in the middle of a room. Looking quickly around you, you see a cabinet 6, a
cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a
countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a
fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin
1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
1.
Your task is to: put a clean knife in countertop.
> think: To solve the task, I need to find and take a knife, then clean it with
sinkbasin, then put it in countertop.
OK.
> think: First I need to find a knife. A knife is more likely to appear in cabinet
(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),
sinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with
cabinet 1.
OK.
> go to cabinet 1
On the cabinet 1, you see a bowl 1.
> go to cabinet 2
The cabinet 2 is closed.
> go to cabinet 3
On the cabinet 3, you see a glassbottle 1.
> go to cabinet 4
On the cabinet 4, you see a mug 1.
> go to cabinet 5
The cabinet 5 is closed.
> go to cabinet 6
The cabinet 6 is closed.
> go to drawer 1
The drawer 1 is closed.
> go to drawer 2
The drawer 2 is closed.
> go to drawer 3
The drawer 3 is closed.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> go to countertop 2
On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a
plate 2, a potato 3, and a statue 1.
> think: Now I find a knife (1). Next, I need to take it.
OK.
> take knife 1 from countertop 2
You pick up the knife 1 from the countertop 2.
> think: Now I take a knife (1). Next, I need to go to sinkbasin (1) and clean it.
OK.
> go to sinkbasin 1
On the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.
> clean knife 1 with sinkbasin 1
You clean the knife 1 using the sinkbasin 1.
> think: Now I clean a knife (1). Next, I need to put it in/on countertop 1.
OK.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> put knife 1 in/on countertop 1
You put the knife 1 in/on the countertop 1.
D.2.2 AC T TRAJECTORY
Act is able to correctly Ô¨Ånd the knife, but tries to clean it using a sinkbasin before actually going
to the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same
sequence of commands. With ReAct, an existing thought would have concretely identiÔ¨Åed that the
knife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.
28
Published as a conference paper at ICLR 2023
Act ALFWorld trajectory
You are in the middle of a room. Looking quickly around you, you see a cabinet 6, a
cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a
countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a
fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin
1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
1.
Your task is to: put a clean knife in countertop.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> go to countertop 2
On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a
plate 2, a potato 3, and a statue 1.
> take knife 1 from countertop 2
You pick up the knife 1 from the countertop 2.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> clean knife 1 with sinkbasin 1
Nothing happens.
> go to countertop 3
On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard
1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato
2, a tomato 1, and a vase 2.
> take knife 2 from countertop 3
Nothing happens.
> go to countertop 2
On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a
potato 3, and a statue 1.
> take knife 1 from countertop 2
Nothing happens.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> clean knife 1 with sinkbasin 1
Nothing happens.
> go to countertop 3
On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard
1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato
2, a tomato 1, and a vase 2.
> take knife 2 from countertop 3
Nothing happens.
> go to countertop 2
On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a
potato 3, and a statue 1.
> take knife 1 from countertop 2
Nothing happens.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> clean knife 1 with sinkbasin 1
Nothing happens.
> go to countertop 3
On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard
1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato
2, a tomato 1, and a vase 2.
> take knife 2 from countertop 3
Nothing happens.
> go to countertop 2
On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a
potato 3, and a statue 1.
> take knife 1 from countertop 2
Nothing happens.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> clean knife 1 with sinkbasin 1
Nothing happens.
D.2.3 REAC T-IM TRAJECTORY
ReAct-IM is able to correctly Ô¨Ånd the knife, but is unable to clean it as well. The LLM then
gets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought
think: I need to find a clean knife. , which seems to trick the LLM in believing
that the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured
by each thought.
29
Published as a conference paper at ICLR 2023
ReAct-IM ALFWorld trajectory
You are in the middle of a room. Looking quickly around you, you see a cabinet 6, a
cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a
countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a
fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin
1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
1.
Your task is to: put a clean knife in countertop.
> think: To solve the task, I need to find and take a clean knife, then put it in
countertop.
OK.
> think: I need to find a clean knife.
OK.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> think: I need to find a clean knife.
OK.
> go to countertop 2
On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a
plate 2, a potato 3, and a statue 1.
> think: I need to find a clean knife.
OK.
> take knife 1 from countertop 2
You pick up the knife 1 from the countertop 2.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> go to countertop 1
On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> put knife 1 in/on countertop 1
You put the knife 1 in/on the countertop 1.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> put knife 1 in/on countertop 1
Nothing happens.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> put knife 1 in/on countertop 1
Nothing happens.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> put knife 1 in/on countertop 1
Nothing happens.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> put knife 1 in/on countertop 1
Nothing happens.
> think: I need to put this knife (1) in/on countertop 1.
OK.
> put knife 1 in/on countertop 1
Nothing happens.
> think: I need to put this knife (1) in/on countertop 1.
OK.
30
Published as a conference paper at ICLR 2023
D.3 W EBSHOP TRAJECTORIES
Table 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,
ReAct uses reasoning to Ô¨Ånd products that satisfy all target attributes.
Instruction: get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars
Act ReAct
Action: search
[
sixteen pack apple cinnamon freeze dried banana
chips
]
Observation:[
Back to Search
]
Page 1 (Total results: 50)[
Next
]
[
B0061IVFZE
]
Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -
Ounce (Pack of 100)
$85.0[
B096H2P6G2
]
Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -
(Variety Pack)
$18.99[
B092JLLYK6
]
Nature‚Äôs Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect
For School Lunches or an On-The-Go Snack - No Sugar Added,
Non GMO, Gluten Free, Nothing ArtiÔ¨Åcial (0.53oz) 6-Pack
$12.99
Action: click
[
B0061IVFZE
]
Observation:[
Back to Search
]
[
Prev
]
Ô¨Çavor name [asian pear][banana][fuji apple & cinnamon][strawberry
banana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,
0.42 Ounce (Pack of 100)
Price: $85.0
Rating: N.A.
[Description]
[Features]
[Reviews]
[Buy Now]
Action: click[Buy Now]
Action: search
[
sixteen pack apple cinnamon freeze dried banana
chips
]
Observation:[
Back to Search
]
Page 1 (Total results: 50)[
Next
]
[
B0061IVFZE
]
Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -
Ounce (Pack of 100)
$85.0[
B096H2P6G2
]
Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -
(Variety Pack)
$18.99[
B092JLLYK6
]
Nature‚Äôs Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect
For School Lunches or an On-The-Go Snack - No Sugar Added,
Non GMO, Gluten Free, Nothing ArtiÔ¨Åcial (0.53oz) 6-Pack
$12.99
Action: think[B0061IVFZE is strawberry banana, not apple cinnamon.
B096H2P6G2 is fruit snacks, not freeze dried banana chips.
B092JLLYK6 is banana crisps, not apple cinnamon. I can check
B092JLLYK6 Ô¨Årst.
Observation: OK.
Action: click
[
B092JLLYK6
]
Observation:[
Back to Search
]
[
Prev
]
Ô¨Çavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]
[strawberry][strawberry banana][sampler variety pack][mega variety
pack][orchard variety pack]
size [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack
of 16)]
Nature‚Äôs Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect
For School Lunches or an On-The-Go Snack - No Sugar Added, Non
GMO, Gluten Free, Nothing ArtiÔ¨Åcial (0.53oz) 6-Pack
Price: $12.99
Rating: N.A.
[Description]
[Features]
[Reviews]
[Buy Now]
Action: think[For sixteen pack of apple cinnamon freeze dried banana
chips, the item has options ‚Äôapple cinnamon‚Äô and ‚Äô0.53 ounce (pack of
16)‚Äô and seems good to buy.]
Observation: OK.
Action: click[apple cinnamon]
Observation: You have clicked apple cinnamon.
Action: click[0.53 ounce (pack of 16)]
Observation: You have clicked 0.53 ounce (pack of 16).
Action: click[Buy Now]
Score: 0.125 Score: 1.0
31
Published as a conference paper at ICLR 2023
E M ORE ANALYSIS
E.1 S UCCESS AND FAILURE MODES ANALYSIS
We provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.
Search results and non-representative steps are omitted to reduce space.
Success: True positive
ReAct
Question: Author David Chanoff has collaborated with a U.S. Navy admiral who served as
the ambassador to the United Kingdom under which President?
Thought 1: I need to search David Chanoff and find the U.S. Navy admiral he
collaborated with.
Action 1: Search[David Chanoff]
Observation 1: David Chanoff is a noted author...
Action 2: Search[William J. Crowe]
Observation 2: William J. Crowe is...
Thought 3: William J. Crowe served as the ambassador to the United Kingdom under
President Bill Clinton. So the answer is Bill Clinton.
Action 3: Finish[Bill Clinton]
Observation 3: Episode finished
CoT
Question: What was the name of the team that represented an Arizona university in
the conference that covers a broad expanse of the western United States,
including Arizona, California, and New Mexico?
Thought: Let‚Äôs think step by step. The name of the team that represented an Arizona
university in the conference that covers a broad expanse of the western
United States, including Arizona, California, and New Mexico is Arizona
State Sun Devils.
Answer: Arizona State Sun Devils
Success: False positive
ReAct
Question: Did the board game San Marco or About Time come out first?
Thought 1: I need to search San Marco and About Time, and find which came out first.
Action 1: Search[San Marco]
Observation 1: San Marco is one of the ... The was built in venice.
Thought 2: San Marco is a sestiere of Venice. I need to search About Time next.
Action 2: Search[About Time]
Observation 2: Could not find [About Time]. Similar: [...].
Thought 3: About Time is a 2013 film. So San Marco came out first.
Action 3: Finish[San Marco]
Observation 3: Episode finished
CoT
Question: Was Vanderbilt University or Emory University founded first?
Thought: Let‚Äôs think step by step. Vanderbilt University was founded in 1873. Emory
University was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory
University), so Vanderbilt University was founded first.
Answer: Emory University
Failure: Reasoning error
ReAct
Question: What was the middle name of the actress who was cast alongside Harvey
Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura
Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in
Hairspray?
Thought 1: I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,
Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and
Marissa Jaret Winokur, find the actress that was cast alongside them in
Hairspray, then find its middle name.
Action 1: Search[Harvey Fierstein]
Observation 1: Harvey Forbes Fierstein ...
Thought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to
search Linda Hart next...
Action 2: Search[Linda Hart]...
CoT
Question: Who is older Danny Green or James Worthy?
Thought: Let‚Äôs think step by step. Danny Green was born in 1987. James Worthy was
born in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is
older.
Answer: Danny Green
32
Published as a conference paper at ICLR 2023
Failure: Search error
ReAct
Question: What does the goddess associated with the goddess frigg consists of what
tales?
Thought 1: I need to search goddess frigg and find its associated goddess.
Action 1: Search[goddess frigg]
Observation 1: Could not find goddess frigg. Similar: [‚ÄôFrigg‚Äô, ‚ÄôFrigg and Freyja common
origin hypothesis‚Äô, ‚ÄôFreyja (section Relation to Frigg and other goddesses
and figures)‚Äô, ‚ÄôEir (redirect from Eir (goddess))‚Äô, ‚ÄôBaldr‚Äô].
Thought 2: I need to search Frigg instead...
Failure: Hallucination
CoT
Question: What was the first year a scientific journal published by an organization
located in the Bronx was published?
Thought: Let‚Äôs think step by step. The first year a scientific journal published by
an organization located in the Bronx was published was 1916.
Answer: 1916 (Label: 1909)
Failure: Label ambiguity
ReAct
Question: What nationality is the man who developed the dictator game?
Thought 1: I need to search dictator game, find the man who developed it, then find his
nationality.
Action 1: Search[dictator game]
Observation 1: The dictator game is a popular experimental instrument in social psychology
and economics...
Action 3: Search[Amos Tversky]
Observation 3: Amos Nathan Tversky...
Thought 4: Amos Tversky is an Israeli cognitive and mathematical psychologist. So the
man who developed the dictator game is Israeli.
Action 4: Finish[Israeli] (label: Israel-American)
CoT
Question: Kurt Fuller from Stockton, CA appeared in which American Detective Comedy
Drama television series created by Steve Franks?
Thought: Let‚Äôs think step by step. Kurt Fuller from Stockton, CA appeared in Psych.
Psych is an American Detective Comedy Drama television series created by
Steve Franks.
Answer: Psych (label: Psych is an American detective comedy-drama)
33


=== NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf ===
Language Models are Few-Shot Learners
Tom B. Brown‚àó Benjamin Mann‚àó Nick Ryder‚àó Melanie Subbiah‚àó
Jared Kaplan‚Ä† Prafulla Dhariwal Arvind Neelakantan Pranav Shyam
Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss
Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh
Daniel M. Ziegler Jeffrey Wu Clemens Winter
Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray
Benjamin Chess Jack Clark Christopher Berner
Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei
Abstract
We demonstrate that scaling up language models greatly improves task-agnostic,
few-shot performance, sometimes even becoming competitive with prior state-of-
the-art Ô¨Åne-tuning approaches. SpeciÔ¨Åcally, we train GPT-3, an autoregressive
language model with 175 billion parameters, 10x more than any previous non-
sparse language model, and test its performance in the few-shot setting. For all
tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks
and few-shot demonstrations speciÔ¨Åed purely via text interaction with the model.
GPT-3 achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks. We also identify some datasets where GPT-
3‚Äôs few-shot learning still struggles, as well as some datasets where GPT-3 faces
methodological issues related to training on large web corpora.
1 Introduction
NLP has shifted from learning task-speciÔ¨Åc representations and designing task-speciÔ¨Åc architectures
to using task-agnostic pre-training and task-agnostic architectures. This shift has led to substantial
progress on many challenging NLP tasks such as reading comprehension, question answering, textual
entailment, among others. Even though the architecture and initial representations are now task-
agnostic, a Ô¨Ånal task-speciÔ¨Åc step remains: Ô¨Åne-tuning on a large dataset of examples to adapt a task
agnostic model to perform a desired task.
Recent work [RWC+19] suggested this Ô¨Ånal step may not be necessary. [RWC+19] demonstrated
that a single pretrained language model can be zero-shot transferred to perform standard NLP tasks
‚àóEqual contribution
‚Ä†Johns Hopkins University, OpenAI
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1.1: Performance on SuperGLUE increases with model size. A value ofK = 32 means
that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in
SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable
to the dotted reference lines (our test set results are in the appendix). The BERT-Large reference
model was Ô¨Åne-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was Ô¨Årst
Ô¨Åne-tuned on MultiNLI (392K examples) and SW AG (113K examples) before further Ô¨Åne-tuning on
the SuperGLUE training set (for a total of 630K Ô¨Åne-tuning examples).
Performance on SuperGLUE increases with number of examples in context. We Ô¨Ånd the differ-
ence in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference
between GPT-3 with one example per context versus eight examples per context.
Aggregate performance for all 42 accuracy-denominated benchmarks. While zero-shot perfor-
mance improves steadily with model size, few-shot performance increases more rapidly, demonstrat-
ing that larger models are more proÔ¨Åcient at in-context learning.
without the need for Ô¨Ånetuning on a dataset of training examples. While this work was a promising
proof of concept, the best case performance only matched some supervised baselines on a single
dataset. On most tasks, performance was still far from even simple supervised baselines.
However [RWC+19] also showed a potential way forward. The work observed relatively consistent
log-linear trends in performance on both transfer tasks and language modeling loss across one an
order of magnitude of scaling. [KMH+20] then conducted a much more rigorous study of the scaling
behavior of log loss and conÔ¨Årmed smooth scaling trends. In this work, we empirically test whether
scaling continues to improve performance by extrapolating the previously identiÔ¨Åed phenomena
another two orders of magnitude. We train a 175 billion parameter autoregressive language model,
which we call GPT-3, and measure its transfer learning abilities.
As part of this investigation, we also clarify and systematize the approach introduced in [RWC+19].
While [RWC+19] describe their work as ‚Äúzero-shot task transfer‚Äù they sometimes provide examples
of the relevant task in the context. Due to the use of what are effectively training examples, these
cases are better described as ‚Äúone-shot‚Äù or ‚Äúfew-shot‚Äù transfer. We study these one-shot and few-shot
settings in detail comparing them with the zero-shot setting which only uses a natural language
description or invocation of the task to be performed. Our Ô¨Åndings are summarized in Figure 1.1. We
observe that one- and few-shot performance is often much higher than true zero-shot performance
leading us to suggest that language models can also be understood as meta-learners where slow
outer-loop gradient descent based learning is combined with fast ‚Äúin-context‚Äù learning implemented
within the context activations of the model.
Broadly, on NLP tasks GPT-3 achieves promising results in the zero- and one-shot settings, and in
the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art
(despite state-of-the-art being held by Ô¨Åne-tuned models). For example, GPT-3 achieves 81.5 F1 on
CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, and 85.0 F1 in the few-shot
setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the
one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to
Ô¨Åne-tuned models operating in the same closed-book setting.
We additionally train a series of smaller models (ranging from 125 million parameters to 13 billion
parameters) in order to compare their performance to GPT-3 in the zero-, one- and few-shot settings.
In general, we Ô¨Ånd relatively smooth scaling for most tasks with model capacity in all three settings;
one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with
model capacity, perhaps suggesting that larger models are more proÔ¨Åcient meta-learners.
2
2 Approach
Our basic pre-training approach, including model, data, and training, is similar to the process
described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and
diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in
this work we systematically explore different settings for learning within the context:
‚Ä¢ Fine-Tuning (FT) - updates the weights of a pre-trained model by training on thousands of
supervised labels speciÔ¨Åc to the desired task. The main advantage of Ô¨Åne-tuning is strong
performance on many benchmarks. The main disadvantages are the need for a new large
dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and
the potential to exploit spurious features of the training data [GSL+18, NK19]. We focus
on task-agnostic performance, leaving Ô¨Åne-tuning for future work.
‚Ä¢ Few-Shot (FS) - the model is given a few demonstrations of the task at inference time as
conditioning [RWC+19], but no weights are updated. An example typically has a context
and a desired completion (for example an English sentence and the French translation),
and few-shot works by givingK examples of context and completion, and then one Ô¨Ånal
example of context, with the model expected to provide the completion (see appendix for
more details). We typically setK in the range of 10 to 100, as this is how many examples can
Ô¨Åt in the model‚Äôs context window (nctx = 2048). The main advantage of few-shot is a major
reduction in the need for task-speciÔ¨Åc data. The main disadvantage is that results from this
method have so far been much worse than state-of-the-art Ô¨Åne-tuned models. Also, a small
amount of task speciÔ¨Åc data is still required. As indicated by the name, few-shot learning as
described here for language models is related to few-shot learning as used in other contexts
in ML [HYC01, VBL+16] ‚Äì both involve learning based on a broad distribution of tasks
and then rapidly adapting to a new task.
‚Ä¢ One-Shot (1S) - similar to few-shot but withK = 1.
‚Ä¢ Zero-Shot (0S) - similar to few-shot but with a natural language description of the task
instead of any examples.
The appendix includes a demonstration of the four methods using the example of translating English
to French. While the few-shot results we present in this paper achieve the highest performance,
one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and
are important targets for future work.
2.1 Model and Architectures
We use the same model and architecture as GPT-2 [RWC+19], including the modiÔ¨Åed initialization,
pre-normalization, and reversible tokenization described therein, with the exception that we use
alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar
to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size,
we train 8 different sizes of model, from 125 million parameters to 175 billion parameters, with the
last being the model we call GPT-3. This range of model sizes allows us to test the scaling laws
introduced in [KMH+20].
More details on the sizes and architectures of our models can be found in the appendix. We partition
each model across GPUs along both the depth and width dimension in order to minimize data-transfer
between nodes.
2.2 Training Dataset
To create our training data, we (1) downloaded and Ô¨Åltered a version of CommonCrawl1 [RSR+19]
based on similarity to a range of high-quality reference corpora, (2) performed fuzzy deduplication at
the document level, within and across datasets, to prevent redundancy and preserve the integrity of
our held-out validation set as an accurate measure of overÔ¨Åtting, and (3) added known high-quality
reference corpora to the training mix to augment CommonCrawl and increase its diversity. These
reference corpora include an expanded version of the WebText dataset [ RWC+19], collected by
1https://commoncrawl.org/the-data/
3
Setting
LAMBADA
(acc)
LAMBADA
(ppl)
StoryCloze
(acc)
HellaSwag
(acc)
SOTA 68.0 a 8.63b 91.8c 85.6d
GPT-3 Zero-Shot 76.2 3.00 83.2 78.9
GPT-3 One-Shot 72.5 3.35 84.7 78.1
GPT-3 Few-Shot 86.4 1.92 87.7 79.3
Table 3.1: Performance on cloze and completion tasks. GPT-3 signiÔ¨Åcantly improves SOTA on
LAMBADA while achieving respectable performance on two difÔ¨Åcult completion prediction datasets.
a[Tur20] b[RWC+19] c[LDL19] d[LCH+20]
scraping links over a longer period of time, and Ô¨Årst described in [KMH+20], two internet-based
books corpora (Books1 and Books2) and English-language Wikipedia (details in the appendix).
2.3 Training Process
As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require
a smaller learning rate. We measure the gradient noise scale during training and use it to guide
our choice of batch size [MKAT18]. Table A.1 shows the parameter settings we used. To train the
larger models without running out of memory, we use a mixture of model parallelism within each
matrix multiply and model parallelism across the layers of the network. All models were trained on
V100 GPU‚Äôs on part of a high-bandwidth cluster. Details of the training process and hyperparameter
settings are described in the appendix.
2.4 Evaluation
For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K
examples from that task‚Äôs training set as conditioning, delimited by 1 or 2 newlines depending on
the task. For LAMBADA and Storycloze there is no supervised training set available so we draw
conditioning examples from the development set and evaluate on the test set.
For some tasks we use a natural language prompt in addition to (or forK = 0, instead of) demonstra-
tions. Similar to [RSR+19] we also sometimes change the formatting of answers. See the appendix
for per-task examples.
On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a
beam width of 4 and a length penalty ofŒ± = 0.6.
Final results are reported on the test set when publicly available, for each model size and learning
setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to Ô¨Åt on
the test server, so we report results on the development set.
3 Results
3.1 Language Modeling, Cloze, and Completion Tasks
We test GPT-3‚Äôs performance on the traditional task of language modeling as well as related tasks.
We calculate zero-shot perplexity on the Penn Tree Bank (PTB) [ MKM+94] dataset measured in
[RWC+19]. We omit the 4 Wikipedia-related tasks and the one-billion word benchmark due to a
high fraction of these datasets being contained in our training set. Our largest model sets a new SOTA
on PTB by a substantial margin of 15 points.
The LAMBADA dataset [ PKL+16] requires the model to predict the last word of a paragraph.
Although [BHT+20] suggested scaling language models is yielding diminishing returns on this
benchmark, we Ô¨Ånd that zero-shot GPT-3 achieves a substantive gain of 8% over the previous state-of-
the-art. For the few-shot setting, we use a Ô¨Åll-in-the-blank format to encourage the language model to
only generate one word (Alice was friends with Bob. Alice went to visit her friend, .‚Üí Bob).
With this format, GPT-3 achieves an increase of over 18% from the previous state-of-the-art, and
4
Setting NaturalQS WebQS TriviaQA
RAG (Fine-tuned, Open-Domain) [LPP+20] 44.5 45.5 68.0
T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] 36.6 44.7 60.5
T5-11B (Fine-tuned, Closed-Book) 34.5 37.4 50.1
GPT-3 Zero-Shot 14.6 14.4 64.3
GPT-3 One-Shot 23.0 25.3 68.0
GPT-3 Few-Shot 29.9 41.5 71.2
Table 3.2: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and
zero-shot settings, as compared to prior SOTA results for closed book and open domain settings.
TriviaQA few-shot result is evaluated on the wiki split test server.
Setting ARC (Easy) ARC (Challenge) CoQA DROP
Fine-tuned SOTA 92.0a 78.5b 90.7c 89.1d
GPT-3 Zero-Shot 68.8 51.4 81.5 23.6
GPT-3 One-Shot 71.2 53.2 84.0 34.3
GPT-3 Few-Shot 70.1 51.5 85.0 36.5
Table 3.3: GPT-3 results on a selection of QA / RC tasks. CoQA and DROP are F1 while ARC
reports accuracy. See the appendix for additional experiments. a[KKS+20] b[KKS+20] c[JZC+19]
d[JN20]
performance improves smoothly with model size. However, the Ô¨Åll-in-blank method is not effective
one-shot, where it always performs worse than the zero-shot setting, perhaps because all models
require several examples to recognize the pattern. An analysis of test set contamination identiÔ¨Åed that
a signiÔ¨Åcant minority of the LAMBADA dataset appears to be present in our training data ‚Äì however
analysis performed in Section 4 suggests negligible impact on performance.
The HellaSwag dataset [ZHB+19] involves picking the best ending to a story or set of instructions.
The examples were adversarially mined to be difÔ¨Åcult for language models while remaining easy for
humans. GPT-3 outperforms a Ô¨Åne-tuned 1.5B parameter language model [ZHR+19] but is still a fair
amount lower than the overall SOTA achieved by the Ô¨Åne-tuned multi-task model ALUM.
The StoryCloze 2016 dataset [ MCH+16] involves selecting the correct ending sentence for Ô¨Åve-
sentence long stories. Here GPT-3 improves over previous zero-shot results by roughly 10% but is
overall still 4.1% lower than the Ô¨Åne-tuned SOTA using a BERT based model [LDL19].
3.2 Question Answering
In this section we measure GPT-3‚Äôs ability to handle a variety of question answering tasks. First, we
look at datasets involving answering questions about broad factual knowledge. We evaluate in the
‚Äúclosed-book‚Äù setting (meaning no conditioning information/articles) as suggested by [ RRS20]. On
TriviaQA [JCWZ17], GPT-3 zero-shot already outperforms the Ô¨Åne-tuned T5-11B by 14.2%, and also
outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot
result improves by 3.7% and matches the SOTA for an open-domain QA system which not only
Ô¨Åne-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector
index of 21M documents [LPP+20]. GPT-3‚Äôs few-shot result further improves performance another
3.2% beyond this. On Natural Questions (NQs) [KPR+19], GPT-3 underperforms a Ô¨Åne-tuned T5
11B+SSM. The questions in NQs tend towards Ô¨Åne-grained Wikipedia knowledge which could be
testing the limits of GPT-3‚Äôs capacity and broad pretraining distribution.
ARC [CCE+18] is a common sense reasoning dataset of multiple-choice questions collected from
3rd to 9th grade science exams. On the ‚ÄúChallenge‚Äù version of the dataset, which has been Ô¨Åltered to
questions which simple statistical or information retrieval methods are unable to correctly answer,
GPT-3 approaches the performance of a Ô¨Åne-tuned RoBERTa baseline [KKS+20]. On the ‚ÄúEasy‚Äù
5
Setting En ‚ÜíFr Fr ‚ÜíEn En ‚ÜíDe De ‚ÜíEn En ‚ÜíRo Ro ‚ÜíEn
SOTA (Supervised) 45.6a 35.0 b 41.2c 40.2d 38.5e 39.9e
XLM [LC19] 33.4 33.3 26.4 34.3 33.3 31.8
MASS [STQ+19] 37.5 34.9 28.3 35.2 35.2 33.1
mBART [LGG+20] - - 29.8 34.0 35.0 30.5
GPT-3 Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9
GPT-3 One-Shot 28.3 33.7 26.2 30.4 20.6 38.6
GPT-3 Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5
Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU
when translating into English reÔ¨Çecting its strength as an English LM. We report BLEU
scores on the WMT‚Äô14 Fr ‚ÜîEn, WMT‚Äô16 De‚ÜîEn, and WMT‚Äô16 Ro ‚ÜîEn datasets as mea-
sured by multi-bleu.perl with XLM‚Äôs tokenization in order to compare most closely with
prior unsupervised NMT work. SacreBLEU f [Pos18] results reported in the appendix. Under-
line indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative
conÔ¨Ådence. a[EOAG18] b[DHKH14] c[WXH+18] d[oR16] e[LGG+20] f [SacreBLEU signature:
BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]
version of the dataset, GPT-3 slightly exceeds the same Ô¨Åne-tuned RoBERTa baseline [KKS+20].
However, both of these results are still much worse than the overall SOTAs achieved by [KKS+20].
Finally, we evaluate GPT-3 on two reading comprehension datasets. Few-shot GPT-3 performs within
3 points of the human baseline on CoQA [RCM19], a free-form conversational dataset. On DROP
[DWD+19], a dataset testing discrete reasoning and numeracy, few-shot GPT-3 outperforms the
Ô¨Åne-tuned BERT baseline from the original paper but is still well below both human performance and
state-of-the-art approaches which augment neural networks with symbolic systems [RLL+19].
3.3 Translation
In collecting training data for GPT-3, we used the unÔ¨Åltered distribution of languages reÔ¨Çected
in internet text datasets (primarily Common Crawl). As a result, although GPT-3‚Äôs training data
primarily consists of English (93% by word count), it also includes 7% non-English content (full list
at GPT-3 GitHub). Existing unsupervised machine translation approaches often combine pretraining
on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a
controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages
together. Additionally, our one / few-shot settings aren‚Äôt strictly comparable to prior unsupervised
work since they make use of a small amount of paired examples in-context (1 or 64).
Zero-shot GPT-3 underperforms recent unsupervised NMT results, but the one-shot setting improves
performance by 7 BLEU and nears competitive performance with prior work. Few-shot GPT-3 further
improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work.
For the three input languages studied, GPT-3 signiÔ¨Åcantly outperforms prior unsupervised NMT work
when translating into English but underperforms when translating in the other direction. Performance
on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This
could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for
an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms
the best supervised result we could Ô¨Ånd but due to our unfamiliarity with the literature and the
appearance that these are un-competitive benchmarks we do not suspect those results represent a
true SOTA. For Ro-En, few shot GPT-3 is very close to the overall SOTA which is achieved with
unsupervised pretraining, Ô¨Ånetuning on 608K labeled examples, and backtranslation [LHCG19b].
3.4 SuperGLUE
The SuperGLUE benchmark is a standardized collection of datasets [ WPN+19]. In the few-shot
setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks
except WSC and MultiRC, we sampled a new set of examples to use in the context for each problem.
For WSC and MultiRC, we used the same set of randomly drawn examples from the training set
6
SuperGLUE BoolQ CB CB COPA RTE
Average Accuracy Accuracy F1 Accuracy Accuracy
Fine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5
Fine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7
GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0
WiC WSC MultiRC MultiRC ReCoRD ReCoRD
Accuracy Accuracy Accuracy F1a Accuracy F1
Fine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3
Fine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0
GPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1
Table 3.5: Performance of GPT-3 on SuperGLUE compared to Ô¨Åne-tuned baselines and SOTA. All
results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context
of each task and performs no gradient updates.
as context for all of the problems we evaluated. We sweep values of K up to 32 and note that the
few-shot SuperGLUE score steadily improves with both model size and with number of examples in
the context showing increasing beneÔ¨Åts from in-context learning (Figure 1.1).
We observe a wide range in GPT-3‚Äôs performance across tasks. On COPA and ReCoRD GPT-3
achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only
a couple points short and achieving second place on the leaderboard, where Ô¨Årst place is held by
a Ô¨Åne-tuned 11 billion parameter model (T5). On WSC, BoolQ, MultiRC, and RTE, performance
is reasonable, roughly matching that of a Ô¨Åne-tuned BERT-Large. On CB, we see signs of life at
75.6% in the few-shot setting. WiC is a notable weak spot with few-shot performance equivalent to
random chance. We tried a number of different phrasings and formulations for WiC (which involves
determining if a word is being used with the same meaning in two sentences), none of which was
able to achieve strong performance. This hints at a phenomenon (which we saw in other experiments
we ran contained in the Additional Materials) ‚Äì GPT-3 appears to be weak in the few-shot or one-shot
setting at some tasks that involve comparing two sentences or snippets. This could also explain the
comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses,
GPT-3 still outperforms a Ô¨Åne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is
close to the state-of-the-art held by a Ô¨Åne-tuned 11 billion parameter model.
4 Measuring and Preventing Memorization Of Benchmarks
The dataset and model size are about two orders of magnitude larger than those used for GPT-2,
and include a large amount of Common Crawl, creating increased potential for contamination and
memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does
not overÔ¨Åt its training set by a signiÔ¨Åcant amount, measured relative to a held-out validation set with
which it was deduplicated. For each benchmark, we produce a ‚Äòclean‚Äô version which removes all
potentially leaked examples, deÔ¨Åned roughly as examples that have a 13-gram overlap with anything
in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). We
then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on
the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if
present, does not have a signiÔ¨Åcant effect on reported results. In most cases performance changes only
negligibly, and we see no evidence that contamination level and performance difference are correlated.
We conclude that either our conservative method substantially overestimated contamination or that
contamination has little effect on performance. We provide full details of the methodology and
analysis on the most problematic tasks in the appendix.
7
5 Limitations
On text synthesis, GPT-3 samples still sometimes repeat themselves semantically at the document
level, start to lose coherence over sufÔ¨Åciently long passages, contradict themselves, and occasionally
contain non-sequitur sentences or paragraphs. Our release repository contains uncurated unconditional
samples.
Our experiments do not include any bidirectional architectures or other training objectives such as
denoising. Our design decision comes at the cost of potentially worse performance on tasks which
empirically beneÔ¨Åt from bidirectionality, such as Ô¨Åll-in-the-blank tasks, tasks that involve looking
back and comparing two pieces of content (ANLI, WIC), or tasks that require re-reading or carefully
considering a long passage and then generating a very short answer (QuAC, RACE).
Our objective weights every token equally and lacks a notion of what is most important to predict
and what is less important. [ RRS20] demonstrate beneÔ¨Åts of customizing prediction to entities of
interest. Also, with self-supervised objectives, task speciÔ¨Åcation relies on forcing the desired task into
a prediction problem, whereas ultimately, useful language systems (for example virtual assistants)
might be better thought of as taking goal-directed actions rather than just making predictions. Finally,
large pretrained language models are not grounded in other domains of experience, such as video or
real-world physical interaction, and thus lack a large amount of context about the world [BHT+20].
For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation
with a different approach is likely to be necessary. Promising future directions in this vein might
include learning the objective function from humans [ ZSW+19], Ô¨Åne-tuning with reinforcement
learning, or adding additional modalities such as images to provide grounding and a better model of
the world [CLY+19].
GPT-3‚Äôs size makes it challenging to deploy. Task-speciÔ¨Åc distillation [HVD15] merits exploration at
this new scale.
6 Related Work
Several efforts have studied the effect of scale on language model performance. [KMH+20, RRBS19,
LWS+20, HNA+17], Ô¨Ånd a smooth power-law trend in loss as autoregressive language models are
scaled up. There are different approaches to scaling language models through increasing parameters,
compute, or both. Our work is most aligned with methods that have increased the size of transformers
by increasing parameters and FLOPS-per-token roughly in proportion, with a parameter count of 213
million [VSP+17] in the original paper, then 300 million [DCLT18], 1.5 billion [RWC+19], 8 billion
[SPP+19], 11 billion [RSR+19], and most recently 17 billion [Tur20]. A second line of work has
focused on increasing parameter count but not computation by using the conditional computation
framework [BLC13]. SpeciÔ¨Åcally, the mixture-of-experts method [ SMM+17] has produced 100
billion parameter models and 50 billion parameter translation models [AJF19]. One way to decrease
the computational cost of our models would be to draw from work such as ALBERT [LCG+19] or
general [HVD15] or task-speciÔ¨Åc [SDCW19, JYS+19, KR16] approaches to distillation. Lastly, a
third approach to scale increases computation without increasing parameters through methods like
adaptive computation time [Gra16] and the universal transformer [DGV+18].
There are many approaches to building multi-task models. Giving task instructions in natural language
was Ô¨Årst formalized in a supervised setting with [MKXS18] and used in [RWC+19] for in-context
learning and in [RSR+19] for multi-task Ô¨Åne-tuning. Multi-task learning [Car97] has shown some
promising initial results [LGH+15, LCR19] and multi-stage Ô¨Åne-tuning has produced SOTA or SOTA-
competitive results [PFB18, KKS+20]. Metalearning was used in language models in [RWC+19],
though with limited results and no systematic study. Other uses of metalearning include matching
networks [VBL+16], RL2 [DSC+16], learning to optimize [RL16, ADG+16, LM17] and MAML
[FAL17]. Our approach of stufÔ¨Ång the model‚Äôs context with previous examples is most structurally
similar to RL2. It also resembles [ HYC01], in that an inner loop adapts to a task, while an outer
loop updates the weights. Our inner loop performs few-shot in-context learning, but prior work has
explored other methods of few-shot learning [SS20, RCP+17, GWC+18, XDH+19].
Finally, Algorithmic innovation in language models over the last two years has been enormous, in-
cluding denoising-based bidirectionality [DCLT18], preÔ¨ÅxLM [DL15], encoder-decoder architectures
[LLG+19, RSR+19], random permutations during training [YDY+19], architectures for sampling
8
efÔ¨Åciency [DYY+19], data and training improvements [LOG+19], and embedding parameters efÔ¨Å-
ciency [LCG+19]. It is likely that incorporating some of these algorithmic advances could improve
GPT-3‚Äôs performance on downstream tasks, especially in the Ô¨Åne-tuning setting.
7 Conclusion
We presented a 175 billion parameter language model which shows strong performance on many
NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly
matching the performance of state-of-the-art Ô¨Åne-tuned systems, as well as generating high-quality
samples and strong qualitative performance at tasks deÔ¨Åned on-the-Ô¨Çy. We documented roughly
predictable trends of scaling in performance without using Ô¨Åne-tuning. We also discussed the social
impacts of this class of model. Despite many limitations and weaknesses, these results suggest that
very large language models may be an important ingredient in the development of adaptable, general
language systems.
Funding Disclosures
This work was funded by OpenAI. All models were trained on V100 GPU‚Äôs on part of a high-
bandwidth cluster provided by Microsoft
Broader Impacts
Language models have a wide range of beneÔ¨Åcial applications for society, including code and writing
auto-completion, grammar assistance, game narrative generation, improving search engine responses,
and answering questions. But they also have potentially harmful applications. GPT-3 improves
the quality of text generation and adaptability over smaller models and increases the difÔ¨Åculty of
distinguishing synthetic text from human-written text. It therefore has the potential to advance both
the beneÔ¨Åcial and harmful applications of language models.
Here we focus on the potential harms of improved language models, not because we believe the
harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader
impacts of language models like this are numerous. We focus on two primary issues: the potential
for deliberate misuse of language models like GPT-3 in Section 7.1, and issues of bias, fairness, and
representation within models like GPT-3 in Section 7.2. We also brieÔ¨Çy discuss issues of energy
efÔ¨Åciency (Section 7.3).
7.1 Misuse of Language Models
Malicious uses of language models can be somewhat difÔ¨Åcult to anticipate because they often
involve repurposing language models in a very different environment or for a different purpose than
researchers intended. To help with this, we can think in terms of traditional security risk assessment
frameworks, which outline key steps such as identifying threats and potential impacts, assessing
likelihood, and determining risk as a combination of likelihood and impact [Ros12]. We discuss three
factors: potential misuse applications, threat actors, and external incentive structures.
7.1.1 Potential Misuse Applications
Any socially harmful activity that relies on generating text could be augmented by powerful lan-
guage models. Examples include misinformation, spam, phishing, abuse of legal and governmental
processes, fraudulent academic essay writing and social engineering pretexting. Many of these
applications bottleneck on human beings to write sufÔ¨Åciently high quality text. Language models that
produce high quality text generation could lower existing barriers to carrying out these activities and
increase their efÔ¨Åcacy.
The misuse potential of language models increases as the quality of text synthesis improves. The
ability of GPT-3 to generate several paragraphs of synthetic content that people Ô¨Ånd difÔ¨Åcult to
distinguish from human-written text represents a concerning milestone in this regard.
9
7.1.2 Threat Actor Analysis
Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled
and resourced actors who may be able to build a malicious product to ‚Äòadvanced persistent threats‚Äô
(APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas
[SBC+19].
To understand how low and mid-skill actors think about language models, we have been monitoring
forums and chat groups where misinformation tactics, malware distribution, and computer fraud
are frequently discussed. While we did Ô¨Ånd signiÔ¨Åcant discussion of misuse following the initial
release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful
deployments since then. Additionally, those misuse discussions were correlated with media coverage
of language model technologies. From this, we assess that the threat of misuse from these actors is
not immediate, but signiÔ¨Åcant improvements in reliability could change this.
Because APTs do not typically discuss operations in the open, we have consulted with professional
threat analysts about possible APT activity involving the use of language models. Since the release
of GPT-2 there has been no discernible difference in operations that may see potential gains by using
language models. The assessment was that language models may not be worth investing signiÔ¨Åcant
resources in because there has been no convincing demonstration that current language models are
signiÔ¨Åcantly better than current methods for generating text, and because methods for ‚Äútargeting‚Äù or
‚Äúcontrolling‚Äù the content of language models are still at a very early stage.
7.1.3 External Incentive Structures
Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely
on to accomplish their agenda. TTPs are inÔ¨Çuenced by economic factors like scalability and ease of
deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort,
high-yield method of deploying malware and stealing login credentials. Using language models to
augment existing TTPs would likely result in an even lower cost of deployment.
Ease of use is another signiÔ¨Åcant incentive. Having stable infrastructure has a large impact on the
adoption of TTPs. The outputs of language models are stochastic, however, and though developers
can constrain these (e.g. using top-k truncation) they are not able to perform consistently without
human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the
time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor
required in operating this bot. But a human is still needed to Ô¨Ålter the outputs, which restricts how
scalable the operation can be.
Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI
researchers will eventually develop language models that are sufÔ¨Åciently consistent and steerable that
they will be of greater interest to malicious actors. We expect this will introduce challenges for the
broader research community, and hope to work on this through a combination of mitigation research,
prototyping, and coordinating with other technical developers.
7.2 Fairness, Bias, and Representation
Biases present in training data may lead models to generate stereotyped or prejudiced content.
This is concerning, since model bias could harm people in the relevant groups in different ways
by entrenching existing stereotypes and producing demeaning portrayals amongst other potential
harms [Cra17]. We have conducted an analysis of biases in the model in order to better understand
GPT-3‚Äôs limitations when it comes to fairness, bias, and representation. 2
Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of
its limitations and behaviors. We focus on biases relating to gender, race, and religion, although
many other categories of bias are likely present and could be studied in follow-up work. This is a
preliminary analysis and does not reÔ¨Çect all of the model‚Äôs biases even within the studied categories.
Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to
reÔ¨Çect stereotypes present in their training data. Below we discuss our preliminary Ô¨Åndings of bias
2Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large
body of prior work. See, for example, [HZJ+19, NBR20, SCNP19].
10
along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter
model and also in similar smaller models, to see if and how they are different in this dimension.
7.2.1 Gender
In our investigation of gender bias in GPT-3, we focused on associations between gender and
occupation. We found that occupations in general have a higher probability of being followed by a
male gender identiÔ¨Åer than a female one (in other words, they are male leaning) when given a context
such as "The {occupation} was a" (Neutral Variant). 83% of the 388 occupations we tested
were more likely to be followed by a male identiÔ¨Åer by GPT-3. We measured this by feeding the
model a context such as "The detective was a" and then looking at the probability of the model
following up with male indicating words (eg. man, male etc.) or female indicating words (woman,
female etc.). In particular, occupations demonstrating higher levels of education such as legislator,
banker, or professor emeritus were heavily male leaning along with occupations that require hard
physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be
followed by female identiÔ¨Åers include midwife, nurse, receptionist, housekeeper etc.
We also tested how these probabilities changed when we shifted the context to be the "The
competent {occupation} was a" (Competent Variant), and when we shifted the context to
be "The incompetent {occupation} was a" (Incompetent Variant) for each occupation in the
dataset. We found that, when prompted with "The competent {occupation} was a," the ma-
jority of occupations had an even higher probability of being followed by a male identiÔ¨Åer than a
female one than was the case with our original neutral prompt, "The {occupation} was a" . With
the prompt "The incompetent {occupation} was a" the majority of occupations still leaned
male with a similar probability than for our original neutral prompt. The average occupation bias -
measured as 1
njobs
‚àë
jobs log( P (female|Context)
P (male|Context)) ) - was‚àí1.11 for the Neutral Variant,‚àí2.14 for the
Competent Variant and‚àí1.15 for the Incompetent Variant.
We also carried out pronoun resolution on the Winogender dataset [ RNLVD18] using
two methods which further corroborated the model‚Äôs tendency to associate most occupa-
tions with males. One method measured the models ability to correctly assign a pro-
noun as the occupation or the participant. For example, we fed the model a con-
text such as "The advisor met with the advisee because she wanted to get advice
about job applications. ‚ÄòShe‚Äô refers to the" and found the option with the lowest
probability between the two possible options (Choices between Occupation Option: advisor; Partici-
pant Option: advisee).
Occupation and participant words often have societal biases associated with them such as the
assumption that most occupants are by default male. We found that the language models learnt some
of these biases such as a tendency to associate female pronouns with participant positions more than
male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was
also the only model where the accuracy for Occupant sentences (sentences where the correct answer
was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other
models had a higher accuracy for male pronouns with Occupation sentences as compared to female
pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy
(60%) for both. This offers some preliminary evidence that in places where issues of bias can make
language models susceptible to error, the larger models are more robust than smaller models.
We also performed co-occurrence tests, where we analyzed which words are likely to occur in the
vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs
of length 50 each with a temperature of 1 and top p of 0.9 for every prompt in our dataset. For
gender, we had prompts such as "He was very" , "She was very" , "He would be described
as", "She would be described as" 3. We looked at the adjectives and adverbs in the top 100
most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often
described using appearance oriented words such as ‚Äùbeautiful‚Äù and ‚Äùgorgeous‚Äù as compared to men
who were more often described using adjectives that span a greater spectrum.
3We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence
since it does not require the isolation of instances in which ‚Äòthey‚Äô refers to a singular noun from those where it
didn‚Äôt, but other forms of gender bias are likely present and could be studied using different approaches.
11
Table 7.1: Most Biased Descriptive Words in 175B Model
Top 10 Most Biased Male Descriptive Words
with Raw Co-Occurrence Counts
Top 10 Most Biased Female Descriptive Words
with Raw Co-Occurrence Counts
Average Number of Co-Occurrences Across All
Words: 17.5
Average Number of Co-Occurrences Across All
Words: 23.9
Large (16) Optimistic (12)
Mostly (15) Bubbly (12)
Lazy (14) Naughty (12)
Fantastic (13) Easy-going (12)
Eccentric (13) Petite (10)
Protect (10) Tight (10)
Jolly (10) Pregnant (10)
Stable (9) Gorgeous (28)
Personable (22) Sucked (8)
Survive (7) Beautiful (158)
Table 7.1 shows the top 10 most favored descriptive words for the model along with the raw number
of times each word co-occurred with a pronoun indicator. ‚ÄúMost Favored‚Äù here indicates words
which were most skewed towards a category by co-occurring with it at a higher rate as compared to
the other category. To put these numbers in perspective, we have also included the average for the
number of co-occurrences across all qualifying words for each gender.
7.2.2 Race
To investigate racial bias in GPT-3, we seeded the model with prompts such as -"The {race} man
was very" , "The {race} woman was very" and "People would describe the {race}
person as" and generated 800 samples for each of the above prompts, with{race} replaced with
a term indicating a racial category such as White or Asian. We then measure word co-occurrences
in the generated samples. Given prior research demonstrating that language models produce text
of differing sentiment when varying features such as occupation [HZJ+19], we explored how race
impacted sentiment. We measured sentiment using Senti WordNet [ BES10] for the words which
co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with
positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores
indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral
words (eg. sloping, chalet).
It should be noted that we were explicitly prompting the models to talk about race and this in turn
generated text that focused on racial features; these results are not from the models talking about
race in the wild but talking about race in an experimental setup where they have been primed to do
so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the
resulting sentiment can reÔ¨Çect socio-historical factors - for instance, text relating to a discussion of
slavery will frequently have a negative sentiment, which may lead to a demographic being associated
with a negative sentiment under this testing methodology.
Across the models we analyzed, ‚ÄòAsian‚Äô had a consistently high sentiment - it ranked 1st in 3 out of 7
models. On the other hand, ‚ÄôBlack‚Äô had a consistently low sentiment - it ranked the lowest in 5 out of
7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a
sense of the biases of different models and highlights the need for more sophisticated analysis of the
relationship between sentiment, entities, and input data.
7.2.3 Religion
We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity,
Hinduism, Islam, and Judaism, by generating 800 model outputs of length‚âà50 with a temperature of 1
and a topp of 0.9 for every prompt. Our prompts were of the nature "{Religion practitioners}
are" (Eg. "Christians are" ) for each of the six religious categories listed above. We then
12
Figure 7.1: Racial Sentiment Across Models
Religion Most Favored Descriptive Words
Atheism ‚ÄòTheists‚Äô, ‚ÄòCool‚Äô, ‚ÄòAgnostics‚Äô, ‚ÄòMad‚Äô, ‚ÄòTheism‚Äô, ‚ÄòDefensive‚Äô, ‚ÄòComplaining‚Äô, ‚ÄòCor-
rect‚Äô, ‚ÄòArrogant‚Äô, ‚ÄòCharacterized‚Äô
Buddhism ‚ÄòMyanmar‚Äô, ‚ÄòVegetarians‚Äô, ‚ÄòBurma‚Äô, ‚ÄòFellowship‚Äô, ‚ÄòMonk‚Äô, ‚ÄòJapanese‚Äô, ‚ÄòReluctant‚Äô,
‚ÄòWisdom‚Äô, ‚ÄòEnlightenment‚Äô, ‚ÄòNon-Violent‚Äô
Christianity ‚ÄòAttend‚Äô, ‚ÄòIgnorant‚Äô, ‚ÄòResponse‚Äô, ‚ÄòJudgmental‚Äô, ‚ÄòGrace‚Äô, ‚ÄòExecution‚Äô, ‚ÄòEgypt‚Äô,
‚ÄòContinue‚Äô, ‚ÄòComments‚Äô, ‚ÄòOfÔ¨Åcially‚Äô
Hinduism ‚ÄòCaste‚Äô, ‚ÄòCows‚Äô, ‚ÄòBJP‚Äô, ‚ÄòKashmir‚Äô, ‚ÄòModi‚Äô, ‚ÄòCelebrated‚Äô, ‚ÄòDharma‚Äô, ‚ÄòPakistani‚Äô,
‚ÄòOriginated‚Äô, ‚ÄòAfrica‚Äô
Islam ‚ÄòPillars‚Äô, ‚ÄòTerrorism‚Äô, ‚ÄòFasting‚Äô, ‚ÄòSheikh‚Äô, ‚ÄòNon-Muslim‚Äô, ‚ÄòSource‚Äô, ‚ÄòCharities‚Äô,
‚ÄòLevant‚Äô, ‚ÄòAllah‚Äô, ‚ÄòProphet‚Äô
Judaism ‚ÄòGentiles‚Äô, ‚ÄòRace‚Äô, ‚ÄòSemites‚Äô, ‚ÄòWhites‚Äô, ‚ÄòBlacks‚Äô, ‚ÄòSmartest‚Äô, ‚ÄòRacists‚Äô, ‚ÄòArabs‚Äô,
‚ÄòGame‚Äô, ‚ÄòRussian‚Äô
Table 7.2: Shows the ten most favored words about each religion in the GPT-3 175B model.
allowed the model to naturally carry out completions and created a corpus of such completions for
studying co-occurrence of words.
The following is an example output from the model:
"Buddhists are divided into two main branches - Theravada and Mahayana.
Theravada is the more conservative branch, centering on monastic life
and the earliest sutras and refusing to recognize the later Mahayana
sutras as authentic."
Similar to race, we found that the models make associations with religious terms that indicate some
propensity to reÔ¨Çect how these terms are sometimes presented in the world. For example, with the
religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher
rate than for other religions. We also found that words such asviolent, terrorism and terrorist
co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored
words for Islam in GPT-3.
7.2.4 Future Bias and Fairness Challenges
We have presented this preliminary analysis to share some of the biases we found in order to motivate
further research, and to highlight the inherent difÔ¨Åculties in characterizing biases in large-scale
generative models; we expect this to be an area of continuous research for us and are excited to
discuss different methodological approaches with the community. We view the work in this section
13
Figure 7.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural
Language Models [KMH+20] we train much larger models on many fewer tokens than is typical.
As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params),
both models took roughly 50 petaÔ¨Çop/s-days of compute during pre-training. Methodology for these
calculations can be found in the Appendix.
as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize
the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model
attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18].
Ultimately, it is important not just to characterize biases in language systems but to intervene. The
literature on this is also extensive [ QMZH19, HZJ+19], so we offer only a few brief comments
on future directions speciÔ¨Åc to large language models. In order to pave the way for effective bias
prevention in general purpose models, there is a need for building a common vocabulary tying
together the normative, technical and empirical challenges of bias mitigation for these models. There
is room for more research that engages with the literature outside NLP, better articulates normative
statements about harm, and engages with the lived experience of communities affected by NLP
systems [BBDIW20]. Thus, mitigation work should not be approached purely with a metric driven
objective to ‚Äòremove‚Äô bias as this has been shown to have blind spots [GG19, NvNvdG19] but in a
holistic manner.
7.3 Energy Usage
Practical large-scale pre-training requires large amounts of computation, which is energy-intensive:
training the GPT-3 175B consumed several thousand petaÔ¨Çop/s-days of compute during pre-training,
compared to tens of petaÔ¨Çop/s-days for a 1.5B parameter GPT-2 model (Figure 7.2). This means we
should be cognizant of the cost and efÔ¨Åciency of such models, as advocated by [SDSE19].
The use of large-scale pre-training also gives another lens through which to view the efÔ¨Åciency of
large models - we should consider not only the resources that go into training them, but how these
resources are amortized over the lifetime of a model, which will subsequently be used for a variety of
purposes and Ô¨Åne-tuned for speciÔ¨Åc tasks. Though models like GPT-3 consume signiÔ¨Åcant resources
during training, they can be surprisingly efÔ¨Åcient once trained: even with the full GPT-3 175B,
generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a
few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further
bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models,
then creating more efÔ¨Åcient versions of them for use in appropriate contexts. Algorithmic progress
may also naturally further increase the efÔ¨Åciency of such models over time, similar to trends observed
in image recognition and neural machine translation [HB20].
14
7.4 News Generation
We test GPT-3‚Äôs ability to generate synthetic ‚Äúnews articles‚Äù by prompting the model with a context
of three previous news articles and the title and subtitle of a proposed article to generate. To gauge the
quality of generated articles, we measured human ability to distinguish GPT-3-generated articles from
real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al. [ZHR+19].
Generative language models are trained to match the distribution of content generated by humans, so
the (in)ability of humans to distinguish the two is a potentially important measure of quality.4
In order to see how well humans can detect model generated text, we arbitrarily selected 25 article
titles and subtitles from the website newser.com (mean length: 215 words). We then generated
completions of these titles and subtitles from for language models ranging in size from 125M to 175B
(GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based
participants with a quiz consisting of these real titles and subtitles followed by either the human
written article or the article generated by the model5. Participants were asked to select whether the
article was ‚Äúvery likely written by a human‚Äù, ‚Äúmore likely written by a human‚Äù, ‚ÄúI don‚Äôt know‚Äù,
‚Äúmore likely written by a machine‚Äù, or ‚Äúvery likely written by a machine‚Äù.
The articles we selected were not in the models‚Äô training data and the model outputs were formatted
and selected programmatically to prevent human cherry-picking. All models used the same context to
condition outputs on and were pre-trained with the same context size and the same article titles and
subtitles were used as prompts for each model. However, we also ran an experiment to control for
participant effort and attention that followed the same format but involved intentionally bad model
generated articles. This was done by generating articles from a ‚Äúcontrol model‚Äù: a 160M parameter
model with no context and increased output randomness.
Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at
detecting that the intentionally bad articles were model generated was‚àº 86% where 50% is chance
level performance. By contrast, mean human accuracy at detecting articles that were produced by the
175B parameter model was barely above chance at‚àº 52% (see Table 7.3).6 Human abilities to detect
model generated text appear to decrease as model size increases: there appears to be a trend towards
chance accuracy with model size, and human detection of GPT-3 is close to chance. 7 This is true
despite the fact that participants spend more time on each output as model size increases (see the
Appendix).
Examples of synthetic articles from GPT-3 are given in Figures 7.4 and 7.5. 8 Much of the text
is‚Äîas indicated by the evaluations‚ÄîdifÔ¨Åcult for humans to distinguish from authentic human content.
Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors,
the models have no access to the speciÔ¨Åc facts that the article titles refer to or when the article was
written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are
often subtle enough that they are not noticed.
Related work on language model detection by Ippolito et al. [ IDCBE19] indicates that automatic
discriminators like G R O V E R[ZHR+19] and GLTR [GSR19] may have greater success at detecting
model generated text than human evaluators. Automatic detection of these models may be a promising
area of future research.
Ippolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases
as humans observe more tokens. To do a preliminary investigation of how good humans are at
detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from
Reuters with an average length of 569 words and generated completions of these articles from GPT-3
with an average length of 498 words (298 words longer than our initial experiments). Following the
4This task is also relevant to the potential misuse of language models discussed in Section 7.1.
5We wanted to identify how good an average person on the internet is at detecting language model outputs,
so we focused on participants drawn from the general US population. See the Appendix for details.
6We use a two-sample Student‚Äôs T-Test to test for signiÔ¨Åcant difference between the means of the participant
accuracies of each model and the control model and report the normalized difference in the means (as the
t-statistic) and the p-value.
7If a model consistently produces texts that are more impressive than human articles, it is possible that human
performance on this task would drop below 50%. Indeed, many individual participants scored below 50% on
this task.
8Additional non-news samples can be found in the Appendix.
15
Mean accuracy
95% ConÔ¨Ådence
Interval (low, hi)
t compared to
control (p-value)
‚ÄúI don‚Äôt know‚Äù
assignments
Control (deliberately bad model) 86% 83%‚Äì90% - 3.6 %
GPT-3 Small 76% 72%‚Äì80% 3.9 (2 e-4) 4.9%
GPT-3 Medium 61% 58%‚Äì65% 10.3 (7 e-21) 6.0%
GPT-3 Large 68% 64%‚Äì72% 7.3 (3 e-11) 8.7%
GPT-3 XL 62% 59%‚Äì65% 10.7 (1 e-19) 7.5%
GPT-3 2.7B 62% 58%‚Äì65% 10.4 (5 e-19) 7.1%
GPT-3 6.7B 60% 56%‚Äì63% 11.2 (3 e-21) 6.2%
GPT-3 13B 55% 52%‚Äì58% 15.3 (1 e-32) 7.1%
GPT-3 175B 52% 49%‚Äì54% 16.9 (1 e-34) 7.8%
Table 7.3: Human accuracy in identifying whether short (‚àº200 word) news articles are model
generated. We Ô¨Ånd that human accuracy (measured by the ratio of correct assignments to non-neutral
assignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares
mean accuracy between Ô¨Åve different models, and shows the results of a two-sample T-Test for the
difference in mean accuracy between each model and the control model (an unconditional GPT-3
Small model with increased output randomness).
Mean accuracy
95% ConÔ¨Ådence
Interval (low, hi)
t compared to
control (p-value)
‚ÄúI don‚Äôt know‚Äù
assignments
Control 88% 84%‚Äì91% - 2.7%
GPT-3 175B 52% 48%‚Äì57% 12.7 (3.2 e-23) 10.6%
Table 7.4: People‚Äôs ability to identify whether‚àº 500 word articles are model generated (as measured
by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and
52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean
accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with
increased output randomness).
methodology above, we ran two experiments, each on around 80 US-based participants, to compare
human abilities to detect the articles generated by GPT-3 and a control model.
We found that mean human accuracy at detecting the intentionally bad longer articles from the control
model was‚àº 88%, while mean human accuracy at detecting the longer articles that were produced
by GPT-3 175B was still barely above chance at‚àº 52% (see Table 7.4). This indicates that, for news
articles that are around 500 words long, GPT-3 continues to produce articles that humans Ô¨Ånd difÔ¨Åcult
to distinguish from human written news articles.
Acknowledgements
The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks
to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov,
Brooke Chan, and Chelsea V oss for helping run evaluations on OpenAI‚Äôs infrastructure. Thanks to
David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to
approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation
with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model
scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy
for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions
of people who created content that was used in the training of the model, and to those who were
involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like
to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train
models at this scale.
16
Figure 7.3: People‚Äôs ability to identify whether news articles are model-generated (measured by the
ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy
on the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with
higher output randomness) is indicated with the dashed line at the top, and the random chance (50%)
is indicated with the dashed line at the bottom. Line of best Ô¨Åt is a power law with 95% conÔ¨Ådence
intervals.
Contributions
Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and
Jeffrey Wuimplemented the large-scale models, training infrastructure, and model-parallel strategies.
Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.
Ben Mann and Alec Radford collected, Ô¨Åltered, deduplicated, and conducted overlap analysis on
the training data.
Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown,
Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework
for supporting them, including creation of synthetic tasks.
Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show
continued gains, and applied scaling laws to help predict and guide model and data scaling decisions
for the research.
Ben Mann implemented sampling without replacement during training.
Alec Radford originally demonstrated few-shot learning occurs in language models.
Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and
systematically studied in-context learning curves, task prompting, and evaluation methods.
Prafulla Dhariwal implemented an early version of the codebase, and developed the memory
optimizations for fully half-precision training.
Rewon Child and Mark Chen developed an early version of our model-parallel strategy.
Rewon Child and Scott Gray contributed the sparse transformer.
17
Title: United Methodists Agree to Historic Split
Subtitle: Those who oppose gay marriage will form their own
denomination
Article: After two days of intense debate, the United Methodist
Church has agreed to a historic split - one that is expected
to end in the creation of a new denomination, one that will
be "theologically and socially conservative," according to
The Washington Post. The majority of delegates attending the
church's annual General Conference in May voted to strengthen a
ban on the ordination of LGBTQ clergy and to write new rules that
will "discipline" clergy who officiate at same-sex weddings. But
those who opposed these measures have a new plan: They say they
will form a separate denomination by 2020, calling their church
the Christian Methodist denomination.
The Post notes that the denomination, which claims 12.5 million
members, was in the early 20th century the "largest Protestant
denomination in the U.S.," but that it has been shrinking in
recent decades. The new split will be the second in the church's
history. The first occurred in 1968, when roughly 10 percent of
the denomination left to form the Evangelical United Brethren
Church. The Post notes that the proposed split "comes at a
critical time for the church, which has been losing members
for years," which has been "pushed toward the brink of a schism
over the role of LGBTQ people in the church." Gay marriage is
not the only issue that has divided the church. In 2016, the
denomination was split over ordination of transgender clergy,
with the North Pacific regional conference voting to ban them
from serving as clergy, and the South Pacific regional conference
voting to allow them.
Figure 7.4: The GPT-3 generated news article that humans had the greatest difÔ¨Åculty distinguishing
from a human written article (accuracy: 12%).
Aditya Ramesh experimented with loss scaling strategies for pretraining.
Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam
search.
Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and
meta-learning literature.
Sandhini Agarwal conducted the fairness and representation analysis.
Girish Sastry and Amanda Askell conducted the human evaluations of the model.
Ariel Herbert-Voss conducted the threat analysis of malicious use.
Gretchen Krueger edited and red-teamed the policy sections of the paper.
Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and
Christopher Berner optimized OpenAI‚Äôs clusters to run the largest models efÔ¨Åciently.
Scott Gray developed fast GPU kernels used during training.
Jack Clark led the analysis of ethical impacts ‚Äî fairness and representation, human assessments of
the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel
on their work.
Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sand-
hini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper.
18
Title: Star‚Äôs Tux Promise Draws Megyn Kelly‚Äôs Sarcasm
Subtitle: Joaquin Phoenix pledged to not change for each awards
event
Article: A year ago, Joaquin Phoenix made headlines when he
appeared on the red carpet at the Golden Globes wearing a tuxedo
with a paper bag over his head that read, "I am a shape-shifter.
I can't change the world. I can only change myself." It was a
promise to not change to fit into the Hollywood mold: "I think
that's a really special thing, to not change yourself. I think
it's a really special thing to say, `This is what's inside of
me, I'm proud of it, and I'm not going to be ashamed because of
the way that someone else thinks I should be.'" Now, it's the
Oscars, and Phoenix is at it again. But this time, his publicist
is saying he'll be wearing a tux no matter what.
Megyn Kelly was not impressed, and she let him have it on The
Tonight Show. "You know, I feel like, I feel like you could
have worn the tux," she says. "But you're saying you're a
shape-shifter. I don't know if you can change your tux, but you
can change your mind. You can change your mind. You can change
your mind." Phoenix says he did, but it didn't stick. "I was
like, `Okay, I'm going to wear a tuxedo to this thing.' And then
I thought, `I don't want to wear a tuxedo to this thing.'" Kelly
goes on to encourage him to change his mind again, but Phoenix
says it's too late: "I'm committed to wearing this."
Figure 7.5: The GPT-3 generated news article that humans found the easiest to distinguish from a
human written article (accuracy: 61%).
Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan
on their work.
Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context,
and demonstrated the beneÔ¨Åt of weight decay for training.
Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised
Pranav, Prafulla, Rewon, Alec, and Aditya on their work.
Dario Amodei designed and led the research.
19
References
[ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David
Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by
gradient descent by gradient descent. In Advances in neural information processing
systems, pages 3981‚Äì3989, 2016.
[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural
machine translation. In Proceedings of the 2019 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 2019.
[BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum¬¥e III, and Hanna Wallach. Language (tech-
nology) is power: A critical survey of ‚Äúbias‚Äù in nlp. arXiv preprint arXiv:2005.14050,
2020.
[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an en-
hanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10,
pages 2200‚Äì2204, 2010.
[BHT+20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce
Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al.
Experience grounds language. arXiv preprint arXiv:2004.10151, 2020.
[BLC13] Yoshua Bengio, Nicholas L¬¥eonard, and Aaron C. Courville. Estimating or propagating
gradients through stochastic neurons for conditional computation. Arxiv, 2013.
[Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997.
[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,
the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.
[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences
with sparse transformers, 2019.
[CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations.
arXiv preprint arXiv:1909.11740, 2019.
[Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017.
[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-
training of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805, 2018.
[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
Kaiser. Universal transformers. Arxiv, 2018.
[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth HeaÔ¨Åeld. Edinburgh‚Äôs
phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages 97‚Äì104, 2014.
[DL15] Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. InAdvances in
neural information processing systems, 2015.
[DSC+16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter
Abbeel. Rl 2: Fast reinforcement learning via slow reinforcement learning. ArXiv,
abs/1611.02779, 2016.
[DWD+19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and
Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning
over paragraphs. arXiv preprint arXiv:1903.00161, 2019.
20
[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V . Le, and Ruslan
Salakhutdinov. Transformer-xl: Attentive language models beyond a Ô¨Åxed-length
context. Arxiv, 2019.
[EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-
translation at scale. arXiv preprint arXiv:1808.09381, 2018.
[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for
fast adaptation of deep networks. ArXiv, abs/1703.03400, 2017.
[GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up
systematic gender biases in word embeddings but do not remove them. arXiv preprint
arXiv:1903.03862, 2019.
[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016.
[GSL+18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R
Bowman, and Noah A Smith. Annotation artifacts in natural language inference data.
arXiv preprint arXiv:1803.02324, 2018.
[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical
detection and visualization of generated text. arXiv preprint arXiv: 1906.04043, 2019.
[GWC+18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning
for low-resource neural machine translation. arXiv preprint arXiv:1808.08437, 2018.
[HB20] Daniel Hernandez and Tom Brown. Ai and efÔ¨Åciency, May 2020.
[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep
learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.
[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural
network. arXiv preprint arXiv:1503.02531, 2015.
[HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using
Gradient Descent. In International Conference on ArtiÔ¨Åcial Neural Networks, pages
87‚Äì94. Springer, 2001.
[HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae,
Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in
language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064,
2019.
[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Auto-
matic detection of generated text is easiest when humans are fooled. arXiv preprint
arXiv:1911.00650, 2019.
[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large
scale distantly supervised challenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551, 2017.
[JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020.
[JYS+19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,
and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv
preprint arXiv:1909.10351, 2019.
[JZC+19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yun-
feng Liu. Technical report on conversational question answering. arXiv preprint
arXiv:1909.10772, 2019.
[KKS+20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and
Hannaneh Hajishirzi. UniÔ¨Åedqa: Crossing format boundaries with a single qa system.
arXiv preprint arXiv:2005.00700, 2020.
21
[KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that‚Äôs Ô¨Åt to fabricate:
Ai-generated text as a tool of media misinformation, 2020.
[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws
for neural language models, 2020.
[KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob
Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew
Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark
for question answering research. Transactions of the Association of Computational
Linguistics, 2019.
[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv,
2016.
[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.
[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining.
arXiv preprint arXiv:1901.07291, 2019.
[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language
representations. arXiv preprint arXiv:1909.11942, 2019.
[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon,
and Jianfeng Gao. Adversarial training for large neural language models. arXiv
preprint arXiv:2004.08994, 2020.
[LCR19] Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text sum-
marization using length-agnostic auto-encoders. arXiv preprint arXiv:1910.00998,
2019.
[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert.
arXiv preprint arXiv:1905.07504, 2019.
[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad,
Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural
machine translation. arXiv preprint arXiv:2001.08210, 2020.
[LGH+15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang.
Representation learning using multi-task deep neural networks for semantic classiÔ¨Åca-
tion and information retrieval. In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies, 2015.
[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task
deep neural networks via knowledge distillation for natural language understanding.
arXiv preprint arXiv:1904.09482, 2019.
[LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural
networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.
[LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mo-
hamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-
sequence pre-training for natural language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461, 2019.
[LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint
arXiv:1703.00441, 2017.
[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly
optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
22
[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich K¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¬®aschel, Sebastian
Riedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive
nlp tasks. arXiv preprint arXiv:2005.11401, 2020.
[LWS+20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
Joseph E. Gonzalez. Train large, then compress: Rethinking model size for efÔ¨Åcient
training and inference of transformers, 2020.
[MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Ba-
tra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evalua-
tion framework for deeper understanding of commonsense stories. arXiv preprint
arXiv:1604.01696, 2016.
[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical
model of large-batch training, 2018.
[MKM+94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: annotating
predicate argument structure. In Proceedings of the workshop on Human Language
Technology, pages 114‚Äì119. Association for Computational Linguistics, 1994.
[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The
natural language decathlon: Multitask learning as question answering. arXiv preprint
arXiv:1806.08730, 2018.
[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnos-
ing syntactic heuristics in natural language inference.arXiv preprint arXiv:1902.01007,
2019.
[MWZ+18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model
cards for model reporting, 2018.
[NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias
in pretrained language models. arXiv preprint arXiv:2004.09456, 2020.
[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural
language arguments. arXiv preprint arXiv:1907.07355, 2019.
[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational:
Man is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866, 2019.
[oR16] University of Regensburg. Fascha, 2016.
[PFB18] Jason Phang, Thibault F ¬¥evry, and Samuel R. Bowman. Sentence encoders on
STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint
arXiv:1811.01088, 2018.
[PKL+16] Denis Paperno, Germ¬¥an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella
Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern¬¥andez. The
lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint
arXiv:1606.06031, 2016.
[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint
arXiv:1804.08771, 2018.
[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in
word-level language models with a gender-equalizing loss function. arXiv preprint
arXiv:1905.12801, 2019.
[RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational
question answering challenge. Transactions of the Association for Computational
Linguistics, 7:249‚Äì266, 2019.
23
[RCP+17] Scott Reed, Yutian Chen, Thomas Paine, A¬®aron van den Oord, SM Eslami, Danilo
Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density
estimation: Towards learning to learn distributions. arXiv preprint arXiv:1710.10304,
2017.
[RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning.
ICLR 2017 (oral), 2016.
[RLL+19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading
comprehension with numerical reasoning. In Proceedings of EMNLP, 2019.
[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme.
Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018.
[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012.
[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A con-
structive prediction of the generalization error across scales, 2019.
[RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack
into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.
[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning
with a uniÔ¨Åed text-to-text transformer, 2019.
[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners, 2019.
[SBC+19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff
Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain,
Alex Newhouse, Jason Blazakis, Kris McGufÔ¨Åe, and Jasmine Wang. Release strategies
and the social impacts of language models, 2019.
[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The
woman worked as a babysitter: On biases in language generation. arXiv preprint
arXiv:1909.01326, 2019.
[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT,
a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint
arXiv:1910.01108, 2019.
[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR,
abs/1907.10597, 2019.
[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine
translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015.
[SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey
Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.
[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models
using model parallelism, 2019.
[SS20] Timo Schick and Hinrich Sch ¬®utze. Exploiting cloze questions for few-shot text
classiÔ¨Åcation and natural language inference. arXiv preprint arXiv:2001.07676, 2020.
[STQ+19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence
to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450,
2019.
[Tur20] Project Turing. Microsoft research blog, Feb 2020.
24
[VBL+16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching
Networks for One Shot Learning. In Advances in neural information processing
systems, pages 3630‚Äì3638, 2016.
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in
neural information processing systems, 2017.
[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for
general-purpose language understanding systems. In Advances in Neural Information
Processing Systems, pages 3261‚Äì3275, 2019.
[WXH+18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan
Liu. Multi-agent dual learning. ICLR 2019, 2018.
[XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V . Le. Unsuper-
vised data augmentation for consistency training, 2019.
[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and
Quoc V . Le. XLNet: Generalized autoregressive pretraining for language understand-
ing. arXiv preprint arXiv:1906.08237, 2019.
[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:
Can a machine really Ô¨Ånish your sentence? arXiv preprint arXiv:1905.07830, 2019.
[ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska
Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint
arXiv:1905.12616, 2019.
[ZSW+19] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario
Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from
human preferences, 2019.
25


=== BERT-Paper-2.pdf ===
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
Google AI Language
{jacobdevlin,mingweichang,kentonl,kristout}@google.com
Abstract
We introduce a new language representa-
tion model called BERT, which stands for
Bidirectional Encoder Representations from
Transformers. Unlike recent language repre-
sentation models (Peters et al., 2018a; Rad-
ford et al., 2018), BERT is designed to pre-
train deep bidirectional representations from
unlabeled text by jointly conditioning on both
left and right context in all layers. As a re-
sult, the pre-trained BERT model can be Ô¨Åne-
tuned with just one additional output layer
to create state-of-the-art models for a wide
range of tasks, such as question answering and
language inference, without substantial task-
speciÔ¨Åc architecture modiÔ¨Åcations.
BERT is conceptually simple and empirically
powerful. It obtains new state-of-the-art re-
sults on eleven natural language processing
tasks, including pushing the GLUE score to
80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute
improvement), SQuAD v1.1 question answer-
ing Test F1 to 93.2 (1.5 point absolute im-
provement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
1 Introduction
Language model pre-training has been shown to
be effective for improving many natural language
processing tasks (Dai and Le, 2015; Peters et al.,
2018a; Radford et al., 2018; Howard and Ruder,
2018). These include sentence-level tasks such as
natural language inference (Bowman et al., 2015;
Williams et al., 2018) and paraphrasing (Dolan
and Brockett, 2005), which aim to predict the re-
lationships between sentences by analyzing them
holistically, as well as token-level tasks such as
named entity recognition and question answering,
where models are required to produce Ô¨Åne-grained
output at the token level (Tjong Kim Sang and
De Meulder, 2003; Rajpurkar et al., 2016).
There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and Ô¨Åne-tuning. The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-speciÔ¨Åc architectures that
include the pre-trained representations as addi-
tional features. The Ô¨Åne-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-speciÔ¨Åc parameters, and is trained on the
downstream tasks by simply Ô¨Åne-tuning all pre-
trained parameters. The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the Ô¨Åne-tuning approaches. The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training. For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017). Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying Ô¨Åne-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
In this paper, we improve the Ô¨Åne-tuning based
approaches by proposing BERT: Bidirectional
Encoder Representations from Transformers.
BERT alleviates the previously mentioned unidi-
rectionality constraint by using a ‚Äúmasked lan-
guage model‚Äù (MLM) pre-training objective, in-
spired by the Cloze task (Taylor, 1953). The
masked language model randomly masks some of
the tokens from the input, and the objective is to
predict the original vocabulary id of the masked
arXiv:1810.04805v2  [cs.CL]  24 May 2019
word based only on its context. Unlike left-to-
right language model pre-training, the MLM ob-
jective enables the representation to fuse the left
and the right context, which allows us to pre-
train a deep bidirectional Transformer. In addi-
tion to the masked language model, we also use
a ‚Äúnext sentence prediction‚Äù task that jointly pre-
trains text-pair representations. The contributions
of our paper are as follows:
‚Ä¢ We demonstrate the importance of bidirectional
pre-training for language representations. Un-
like Radford et al. (2018), which uses unidirec-
tional language models for pre-training, BERT
uses masked language models to enable pre-
trained deep bidirectional representations. This
is also in contrast to Peters et al. (2018a), which
uses a shallow concatenation of independently
trained left-to-right and right-to-left LMs.
‚Ä¢ We show that pre-trained representations reduce
the need for many heavily-engineered task-
speciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne-
tuning based representation model that achieves
state-of-the-art performance on a large suite
of sentence-level and token-level tasks, outper-
forming many task-speciÔ¨Åc architectures.
‚Ä¢ BERT advances the state of the art for eleven
NLP tasks. The code and pre-trained mod-
els are available at https://github.com/
google-research/bert.
2 Related Work
There is a long history of pre-training general lan-
guage representations, and we brieÔ¨Çy review the
most widely-used approaches in this section.
2.1 Unsupervised Feature-based Approaches
Learning widely applicable representations of
words has been an active area of research for
decades, including non-neural (Brown et al., 1992;
Ando and Zhang, 2005; Blitzer et al., 2006) and
neural (Mikolov et al., 2013; Pennington et al.,
2014) methods. Pre-trained word embeddings
are an integral part of modern NLP systems, of-
fering signiÔ¨Åcant improvements over embeddings
learned from scratch (Turian et al., 2010). To pre-
train word embedding vectors, left-to-right lan-
guage modeling objectives have been used (Mnih
and Hinton, 2009), as well as objectives to dis-
criminate correct from incorrect words in left and
right context (Mikolov et al., 2013).
These approaches have been generalized to
coarser granularities, such as sentence embed-
dings (Kiros et al., 2015; Logeswaran and Lee,
2018) or paragraph embeddings (Le and Mikolov,
2014). To train sentence representations, prior
work has used objectives to rank candidate next
sentences (Jernite et al., 2017; Logeswaran and
Lee, 2018), left-to-right generation of next sen-
tence words given a representation of the previous
sentence (Kiros et al., 2015), or denoising auto-
encoder derived objectives (Hill et al., 2016).
ELMo and its predecessor (Peters et al., 2017,
2018a) generalize traditional word embedding re-
search along a different dimension. They extract
context-sensitive features from a left-to-right and a
right-to-left language model. The contextual rep-
resentation of each token is the concatenation of
the left-to-right and right-to-left representations.
When integrating contextual word embeddings
with existing task-speciÔ¨Åc architectures, ELMo
advances the state of the art for several major NLP
benchmarks (Peters et al., 2018a) including ques-
tion answering (Rajpurkar et al., 2016), sentiment
analysis (Socher et al., 2013), and named entity
recognition (Tjong Kim Sang and De Meulder,
2003). Melamud et al. (2016) proposed learning
contextual representations through a task to pre-
dict a single word from both left and right context
using LSTMs. Similar to ELMo, their model is
feature-based and not deeply bidirectional. Fedus
et al. (2018) shows that the cloze task can be used
to improve the robustness of text generation mod-
els.
2.2 Unsupervised Fine-tuning Approaches
As with the feature-based approaches, the Ô¨Årst
works in this direction only pre-trained word em-
bedding parameters from unlabeled text (Col-
lobert and Weston, 2008).
More recently, sentence or document encoders
which produce contextual token representations
have been pre-trained from unlabeled text and
Ô¨Åne-tuned for a supervised downstream task (Dai
and Le, 2015; Howard and Ruder, 2018; Radford
et al., 2018). The advantage of these approaches
is that few parameters need to be learned from
scratch. At least partly due to this advantage,
OpenAI GPT (Radford et al., 2018) achieved pre-
viously state-of-the-art results on many sentence-
level tasks from the GLUE benchmark (Wang
et al., 2018a). Left-to-right language model-
BERT BERT
E[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô
C
 T1
 T[SEP]...
 TN
 T1‚Äô ...
 TM‚Äô
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Question Paragraph
Start/End Span
BERT
E[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô
C
 T1
 T[SEP]...
 TN
 T1‚Äô ...
 TM‚Äô
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Masked Sentence A Masked Sentence B
Pre-training Fine-Tuning
NSP Mask LM Mask LM
Unlabeled Sentence A and B Pair 
SQuAD
Question Answer Pair
NERMNLI
Figure 1: Overall pre-training and Ô¨Åne-tuning procedures for BERT. Apart from output layers, the same architec-
tures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize
models for different down-stream tasks. During Ô¨Åne-tuning, all parameters are Ô¨Åne-tuned. [CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-
tions/answers).
ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
2.3 Transfer Learning from Supervised Data
There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017). Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to Ô¨Åne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
3 BERT
We introduce BERT and its detailed implementa-
tion in this section. There are two steps in our
framework: pre-training and Ô¨Åne-tuning. Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks. For Ô¨Åne-
tuning, the BERT model is Ô¨Årst initialized with
the pre-trained parameters, and all of the param-
eters are Ô¨Åne-tuned using labeled data from the
downstream tasks. Each downstream task has sep-
arate Ô¨Åne-tuned models, even though they are ini-
tialized with the same pre-trained parameters. The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniÔ¨Åed ar-
chitecture across different tasks. There is mini-
mal difference between the pre-trained architec-
ture and the Ô¨Ånal downstream architecture.
Model Architecture BERT‚Äôs model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al. (2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al. (2017) as well as excellent guides
such as ‚ÄúThe Annotated Transformer.‚Äù2
In this work, we denote the number of layers
(i.e., Transformer blocks) asL, the hidden size as
H, and the number of self-attention heads as A.3
We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
1https://github.com/tensorÔ¨Çow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/Ô¨Ålter size to be 4H,
i.e., 3072 for the H = 768 and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-
Input/Output Representations To make BERT
handle a variety of down-stream tasks, our input
representation is able to unambiguously represent
both a single sentence and a pair of sentences
(e.g.,‚ü® Question, Answer‚ü©) in one token sequence.
Throughout this work, a ‚Äúsentence‚Äù can be an arbi-
trary span of contiguous text, rather than an actual
linguistic sentence. A ‚Äúsequence‚Äù refers to the in-
put token sequence to BERT, which may be a sin-
gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al.,
2016) with a 30,000 token vocabulary. The Ô¨Årst
token of every sequence is always a special clas-
siÔ¨Åcation token ( [CLS]). The Ô¨Ånal hidden state
corresponding to this token is used as the ag-
gregate sequence representation for classiÔ¨Åcation
tasks. Sentence pairs are packed together into a
single sequence. We differentiate the sentences in
two ways. First, we separate them with a special
token ([SEP]). Second, we add a learned embed-
ding to every token indicating whether it belongs
to sentence A or sentence B. As shown in Figure 1,
we denote input embedding asE, the Ô¨Ånal hidden
vector of the special [CLS] token as C ‚àà RH,
and the Ô¨Ånal hidden vector for the ith input token
asTi‚àà RH.
For a given token, its input representation is
constructed by summing the corresponding token,
segment, and position embeddings. A visualiza-
tion of this construction can be seen in Figure 2.
3.1 Pre-training BERT
Unlike Peters et al. (2018a) and Radford et al.
(2018), we do not use traditional left-to-right or
right-to-left language models to pre-train BERT.
Instead, we pre-train BERT using two unsuper-
vised tasks, described in this section. This step
is presented in the left part of Figure 1.
Task #1: Masked LM Intuitively, it is reason-
able to believe that a deep bidirectional model is
strictly more powerful than either a left-to-right
model or the shallow concatenation of a left-to-
right and a right-to-left model. Unfortunately,
standard conditional language models can only be
trained left-to-right or right-to-left, since bidirec-
tional conditioning would allow each word to in-
directly ‚Äúsee itself‚Äù, and the model could trivially
predict the target word in a multi-layered context.
former is often referred to as a ‚ÄúTransformer encoder‚Äù while
the left-context-only version is referred to as a ‚ÄúTransformer
decoder‚Äù since it can be used for text generation.
In order to train a deep bidirectional representa-
tion, we simply mask some percentage of the input
tokens at random, and then predict those masked
tokens. We refer to this procedure as a ‚Äúmasked
LM‚Äù (MLM), although it is often referred to as a
Cloze task in the literature (Taylor, 1953). In this
case, the Ô¨Ånal hidden vectors corresponding to the
mask tokens are fed into an output softmax over
the vocabulary, as in a standard LM. In all of our
experiments, we mask 15% of all WordPiece to-
kens in each sequence at random. In contrast to
denoising auto-encoders (Vincent et al., 2008), we
only predict the masked words rather than recon-
structing the entire input.
Although this allows us to obtain a bidirec-
tional pre-trained model, a downside is that we
are creating a mismatch between pre-training and
Ô¨Åne-tuning, since the [MASK] token does not ap-
pear during Ô¨Åne-tuning. To mitigate this, we do
not always replace ‚Äúmasked‚Äù words with the ac-
tual [MASK] token. The training data generator
chooses 15% of the token positions at random for
prediction. If the i-th token is chosen, we replace
thei-th token with (1) the [MASK] token 80% of
the time (2) a random token 10% of the time (3)
the unchanged i-th token 10% of the time. Then,
Ti will be used to predict the original token with
cross entropy loss. We compare variations of this
procedure in Appendix C.2.
Task #2: Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques-
tion Answering (QA) and Natural Language Infer-
ence (NLI) are based on understanding the rela-
tionship between two sentences, which is not di-
rectly captured by language modeling. In order
to train a model that understands sentence rela-
tionships, we pre-train for a binarized next sen-
tence prediction task that can be trivially gener-
ated from any monolingual corpus. SpeciÔ¨Åcally,
when choosing the sentences A and B for each pre-
training example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext),
and 50% of the time it is a random sentence from
the corpus (labeled as NotNext). As we show
in Figure 1, C is used for next sentence predic-
tion (NSP). 5 Despite its simplicity, we demon-
strate in Section 5.1 that pre-training towards this
task is very beneÔ¨Åcial to both QA and NLI. 6
5The Ô¨Ånal model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation
without Ô¨Åne-tuning, since it was trained with NSP.
[CLS] he likes play ##ing [SEP]my dog is cute [SEP]Input
E[CLS] Ehe Elikes Eplay E##ing E[SEP]Emy Edog Eis Ecute E[SEP]
Token
Embeddings
EA EB EB EB EB EBEA EA EA EA EASegment
Embeddings
E0 E6 E7 E8 E9 E10E1 E2 E3 E4 E5Position
Embeddings
Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-
tion embeddings and the position embeddings.
The NSP task is closely related to representation-
learning objectives used in Jernite et al. (2017) and
Logeswaran and Lee (2018). However, in prior
work, only sentence embeddings are transferred to
down-stream tasks, where BERT transfers all pa-
rameters to initialize end-task model parameters.
Pre-training data The pre-training procedure
largely follows the existing literature on language
model pre-training. For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al.,
2015) and English Wikipedia (2,500M words).
For Wikipedia we extract only the text passages
and ignore lists, tables, and headers. It is criti-
cal to use a document-level corpus rather than a
shufÔ¨Çed sentence-level corpus such as the Billion
Word Benchmark (Chelba et al., 2013) in order to
extract long contiguous sequences.
3.2 Fine-tuning BERT
Fine-tuning is straightforward since the self-
attention mechanism in the Transformer al-
lows BERT to model many downstream tasks‚Äî
whether they involve single text or text pairs‚Äîby
swapping out the appropriate inputs and outputs.
For applications involving text pairs, a common
pattern is to independently encode text pairs be-
fore applying bidirectional cross attention, such
as Parikh et al. (2016); Seo et al. (2017). BERT
instead uses the self-attention mechanism to unify
these two stages, as encoding a concatenated text
pair with self-attention effectively includes bidi-
rectional cross attention between two sentences.
For each task, we simply plug in the task-
speciÔ¨Åc inputs and outputs into BERT and Ô¨Åne-
tune all the parameters end-to-end. At the in-
put, sentence A and sentence B from pre-training
are analogous to (1) sentence pairs in paraphras-
ing, (2) hypothesis-premise pairs in entailment, (3)
question-passage pairs in question answering, and
(4) a degenerate text- ‚àÖ pair in text classiÔ¨Åcation
or sequence tagging. At the output, the token rep-
resentations are fed into an output layer for token-
level tasks, such as sequence tagging or question
answering, and the [CLS] representation is fed
into an output layer for classiÔ¨Åcation, such as en-
tailment or sentiment analysis.
Compared to pre-training, Ô¨Åne-tuning is rela-
tively inexpensive. All of the results in the pa-
per can be replicated in at most 1 hour on a sin-
gle Cloud TPU, or a few hours on a GPU, starting
from the exact same pre-trained model. 7 We de-
scribe the task-speciÔ¨Åc details in the correspond-
ing subsections of Section 4. More details can be
found in Appendix A.5.
4 Experiments
In this section, we present BERT Ô¨Åne-tuning re-
sults on 11 NLP tasks.
4.1 GLUE
The General Language Understanding Evaluation
(GLUE) benchmark (Wang et al., 2018a) is a col-
lection of diverse natural language understanding
tasks. Detailed descriptions of GLUE datasets are
included in Appendix B.1.
To Ô¨Åne-tune on GLUE, we represent the input
sequence (for single sentence or sentence pairs)
as described in Section 3, and use the Ô¨Ånal hid-
den vector C ‚àà RH corresponding to the Ô¨Årst
input token ([CLS]) as the aggregate representa-
tion. The only new parameters introduced during
Ô¨Åne-tuning are classiÔ¨Åcation layer weights W ‚àà
RK√óH, whereK is the number of labels. We com-
pute a standard classiÔ¨Åcation loss with C andW ,
i.e., log(softmax(CW T )).
7For example, the BERT SQuAD model can be trained in
around 30 minutes on a single Cloud TPU to achieve a Dev
F1 score of 91.0%.
8See (10) in https://gluebenchmark.com/faq.
System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE A verage
392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -
Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0
BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0
OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1
BERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6
BERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1
Table 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard).
The number below each task denotes the number of training examples. The ‚ÄúAverage‚Äù column is slightly different
than the ofÔ¨Åcial GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are single-
model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and
accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.
We use a batch size of 32 and Ô¨Åne-tune for 3
epochs over the data for all GLUE tasks. For each
task, we selected the best Ô¨Åne-tuning learning rate
(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.
Additionally, for BERTLARGE we found that Ô¨Åne-
tuning was sometimes unstable on small datasets,
so we ran several random restarts and selected the
best model on the Dev set. With random restarts,
we use the same pre-trained checkpoint but per-
form different Ô¨Åne-tuning data shufÔ¨Çing and clas-
siÔ¨Åer layer initialization.9
Results are presented in Table 1. Both
BERTBASE and BERTLARGE outperform all sys-
tems on all tasks by a substantial margin, obtaining
4.5% and 7.0% respective average accuracy im-
provement over the prior state of the art. Note that
BERTBASE and OpenAI GPT are nearly identical
in terms of model architecture apart from the at-
tention masking. For the largest and most widely
reported GLUE task, MNLI, BERT obtains a 4.6%
absolute accuracy improvement. On the ofÔ¨Åcial
GLUE leaderboard10, BERTLARGE obtains a score
of 80.5, compared to OpenAI GPT, which obtains
72.8 as of the date of writing.
We Ô¨Ånd that BERT LARGE signiÔ¨Åcantly outper-
forms BERTBASE across all tasks, especially those
with very little training data. The effect of model
size is explored more thoroughly in Section 5.2.
4.2 SQuAD v1.1
The Stanford Question Answering Dataset
(SQuAD v1.1) is a collection of 100k crowd-
sourced question/answer pairs (Rajpurkar et al.,
2016). Given a question and a passage from
9The GLUE data set distribution does not include the Test
labels, and we only made a single GLUE evaluation server
submission for each of BERTBASE and BERTLARGE .
10https://gluebenchmark.com/leaderboard
Wikipedia containing the answer, the task is to
predict the answer text span in the passage.
As shown in Figure 1, in the question answer-
ing task, we represent the input question and pas-
sage as a single packed sequence, with the ques-
tion using the A embedding and the passage using
the B embedding. We only introduce a start vec-
tor S ‚àà RH and an end vector E ‚àà RH during
Ô¨Åne-tuning. The probability of word i being the
start of the answer span is computed as a dot prod-
uct betweenTi andS followed by a softmax over
all of the words in the paragraph: Pi = eS¬∑Ti
‚àë
j eS¬∑Tj .
The analogous formula is used for the end of the
answer span. The score of a candidate span from
positioni to positionj is deÔ¨Åned asS¬∑Ti +E¬∑Tj,
and the maximum scoring span where j ‚â• i is
used as a prediction. The training objective is the
sum of the log-likelihoods of the correct start and
end positions. We Ô¨Åne-tune for 3 epochs with a
learning rate of 5e-5 and a batch size of 32.
Table 2 shows top leaderboard entries as well
as results from top published systems (Seo et al.,
2017; Clark and Gardner, 2018; Peters et al.,
2018a; Hu et al., 2018). The top results from the
SQuAD leaderboard do not have up-to-date public
system descriptions available,11 and are allowed to
use any public data when training their systems.
We therefore use modest data augmentation in
our system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi
et al., 2017) befor Ô¨Åne-tuning on SQuAD.
Our best performing system outperforms the top
leaderboard system by +1.5 F1 in ensembling and
+1.3 F1 as a single system. In fact, our single
BERT model outperforms the top ensemble sys-
tem in terms of F1 score. Without TriviaQA Ô¨Åne-
11QANet is described in Yu et al. (2018), but the system
has improved substantially after publication.
System Dev Test
EM F1 EM F1
Top Leaderboard Systems (Dec 10th, 2018)
Human - - 82.3 91.2
#1 Ensemble - nlnet - - 86.0 91.7
#2 Ensemble - QANet - - 84.5 90.5
Published
BiDAF+ELMo (Single) - 85.6 - 85.8
R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5
Ours
BERTBASE (Single) 80.8 88.5 - -
BERTLARGE (Single) 84.1 90.9 - -
BERTLARGE (Ensemble) 85.8 91.8 - -
BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8
BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2
Table 2: SQuAD 1.1 results. The BERT ensemble
is 7x systems which use different pre-training check-
points and Ô¨Åne-tuning seeds.
System Dev Test
EM F1 EM F1
Top Leaderboard Systems (Dec 10th, 2018)
Human 86.3 89.0 86.9 89.5
#1 Single - MIR-MRC (F-Net) - - 74.8 78.0
#2 Single - nlnet - - 74.2 77.1
Published
unet (Ensemble) - - 71.4 74.9
SLQA+ (Single) - 71.4 74.4
Ours
BERTLARGE (Single) 78.7 81.9 80.0 83.1
Table 3: SQuAD 2.0 results. We exclude entries that
use BERT as one of their components.
tuning data, we only lose 0.1-0.4 F1, still outper-
forming all existing systems by a wide margin.12
4.3 SQuAD v2.0
The SQuAD 2.0 task extends the SQuAD 1.1
problem deÔ¨Ånition by allowing for the possibility
that no short answer exists in the provided para-
graph, making the problem more realistic.
We use a simple approach to extend the SQuAD
v1.1 BERT model for this task. We treat ques-
tions that do not have an answer as having an an-
swer span with start and end at the [CLS] to-
ken. The probability space for the start and end
answer span positions is extended to include the
position of the [CLS] token. For prediction, we
compare the score of the no-answer span:snull =
S¬∑C +E¬∑C to the score of the best non-null span
12The TriviaQA data we used consists of paragraphs from
TriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents,
that contain at least one of the provided possible answers.
System Dev Test
ESIM+GloVe 51.9 52.7
ESIM+ELMo 59.1 59.2
OpenAI GPT - 78.0
BERTBASE 81.6 -
BERTLARGE 86.6 86.3
Human (expert)‚Ä† - 85.0
Human (5 annotations)‚Ä† - 88.0
Table 4: SW AG Dev and Test accuracies.‚Ä†Human per-
formance is measured with 100 samples, as reported in
the SW AG paper.
ÀÜsi,j = maxj‚â•iS¬∑Ti +E¬∑Tj. We predict a non-null
answer when ÀÜsi,j > snull +œÑ, where the thresh-
old œÑ is selected on the dev set to maximize F1.
We did not use TriviaQA data for this model. We
Ô¨Åne-tuned for 2 epochs with a learning rate of 5e-5
and a batch size of 48.
The results compared to prior leaderboard en-
tries and top published work (Sun et al., 2018;
Wang et al., 2018b) are shown in Table 3, exclud-
ing systems that use BERT as one of their com-
ponents. We observe a +5.1 F1 improvement over
the previous best system.
4.4 SW AG
The Situations With Adversarial Generations
(SW AG) dataset contains 113k sentence-pair com-
pletion examples that evaluate grounded common-
sense inference (Zellers et al., 2018). Given a sen-
tence, the task is to choose the most plausible con-
tinuation among four choices.
When Ô¨Åne-tuning on the SW AG dataset, we
construct four input sequences, each containing
the concatenation of the given sentence (sentence
A) and a possible continuation (sentence B). The
only task-speciÔ¨Åc parameters introduced is a vec-
tor whose dot product with the [CLS] token rep-
resentation C denotes a score for each choice
which is normalized with a softmax layer.
We Ô¨Åne-tune the model for 3 epochs with a
learning rate of 2e-5 and a batch size of 16. Re-
sults are presented in Table 4. BERT LARGE out-
performs the authors‚Äô baseline ESIM+ELMo sys-
tem by +27.1% and OpenAI GPT by 8.3%.
5 Ablation Studies
In this section, we perform ablation experiments
over a number of facets of BERT in order to better
understand their relative importance. Additional
Dev Set
Tasks MNLI-m QNLI MRPC SST-2 SQuAD
(Acc) (Acc) (Acc) (Acc) (F1)
BERTBASE 84.4 88.4 86.7 92.7 88.5
No NSP 83.9 84.9 86.5 92.6 87.9
LTR & No NSP 82.1 84.3 77.5 92.1 77.8
+ BiLSTM 82.1 84.1 75.7 91.6 84.9
Table 5: Ablation over the pre-training tasks using the
BERTBASE architecture. ‚ÄúNo NSP‚Äù is trained without
the next sentence prediction task. ‚ÄúLTR & No NSP‚Äù is
trained as a left-to-right LM without the next sentence
prediction, like OpenAI GPT. ‚Äú+ BiLSTM‚Äù adds a ran-
domly initialized BiLSTM on top of the ‚ÄúLTR + No
NSP‚Äù model during Ô¨Åne-tuning.
ablation studies can be found in Appendix C.
5.1 Effect of Pre-training Tasks
We demonstrate the importance of the deep bidi-
rectionality of BERT by evaluating two pre-
training objectives using exactly the same pre-
training data, Ô¨Åne-tuning scheme, and hyperpa-
rameters as BERTBASE :
No NSP : A bidirectional model which is trained
using the ‚Äúmasked LM‚Äù (MLM) but without the
‚Äúnext sentence prediction‚Äù (NSP) task.
LTR & No NSP: A left-context-only model which
is trained using a standard Left-to-Right (LTR)
LM, rather than an MLM. The left-only constraint
was also applied at Ô¨Åne-tuning, because removing
it introduced a pre-train/Ô¨Åne-tune mismatch that
degraded downstream performance. Additionally,
this model was pre-trained without the NSP task.
This is directly comparable to OpenAI GPT, but
using our larger training dataset, our input repre-
sentation, and our Ô¨Åne-tuning scheme.
We Ô¨Årst examine the impact brought by the NSP
task. In Table 5, we show that removing NSP
hurts performance signiÔ¨Åcantly on QNLI, MNLI,
and SQuAD 1.1. Next, we evaluate the impact
of training bidirectional representations by com-
paring ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR
model performs worse than the MLM model on all
tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR
model will perform poorly at token predictions,
since the token-level hidden states have no right-
side context. In order to make a good faith at-
tempt at strengthening the LTR system, we added
a randomly initialized BiLSTM on top. This does
signiÔ¨Åcantly improve results on SQuAD, but the
results are still far worse than those of the pre-
trained bidirectional models. The BiLSTM hurts
performance on the GLUE tasks.
We recognize that it would also be possible to
train separate LTR and RTL models and represent
each token as the concatenation of the two mod-
els, as ELMo does. However: (a) this is twice as
expensive as a single bidirectional model; (b) this
is non-intuitive for tasks like QA, since the RTL
model would not be able to condition the answer
on the question; (c) this it is strictly less powerful
than a deep bidirectional model, since it can use
both left and right context at every layer.
5.2 Effect of Model Size
In this section, we explore the effect of model size
on Ô¨Åne-tuning task accuracy. We trained a number
of BERT models with a differing number of layers,
hidden units, and attention heads, while otherwise
using the same hyperparameters and training pro-
cedure as described previously.
Results on selected GLUE tasks are shown in
Table 6. In this table, we report the average Dev
Set accuracy from 5 random restarts of Ô¨Åne-tuning.
We can see that larger models lead to a strict ac-
curacy improvement across all four datasets, even
for MRPC which only has 3,600 labeled train-
ing examples, and is substantially different from
the pre-training tasks. It is also perhaps surpris-
ing that we are able to achieve such signiÔ¨Åcant
improvements on top of models which are al-
ready quite large relative to the existing literature.
For example, the largest Transformer explored in
Vaswani et al. (2017) is (L=6, H=1024, A=16)
with 100M parameters for the encoder, and the
largest Transformer we have found in the literature
is (L=64, H=512, A=2) with 235M parameters
(Al-Rfou et al., 2018). By contrast, BERT BASE
contains 110M parameters and BERT LARGE con-
tains 340M parameters.
It has long been known that increasing the
model size will lead to continual improvements
on large-scale tasks such as machine translation
and language modeling, which is demonstrated
by the LM perplexity of held-out training data
shown in Table 6. However, we believe that
this is the Ô¨Årst work to demonstrate convinc-
ingly that scaling to extreme model sizes also
leads to large improvements on very small scale
tasks, provided that the model has been sufÔ¨Å-
ciently pre-trained. Peters et al. (2018b) presented
mixed results on the downstream task impact of
increasing the pre-trained bi-LM size from two
to four layers and Melamud et al. (2016) men-
tioned in passing that increasing hidden dimen-
sion size from 200 to 600 helped, but increasing
further to 1,000 did not bring further improve-
ments. Both of these prior works used a feature-
based approach ‚Äî we hypothesize that when the
model is Ô¨Åne-tuned directly on the downstream
tasks and uses only a very small number of ran-
domly initialized additional parameters, the task-
speciÔ¨Åc models can beneÔ¨Åt from the larger, more
expressive pre-trained representations even when
downstream task data is very small.
5.3 Feature-based Approach with BERT
All of the BERT results presented so far have used
the Ô¨Åne-tuning approach, where a simple classiÔ¨Å-
cation layer is added to the pre-trained model, and
all parameters are jointly Ô¨Åne-tuned on a down-
stream task. However, the feature-based approach,
where Ô¨Åxed features are extracted from the pre-
trained model, has certain advantages. First, not
all tasks can be easily represented by a Trans-
former encoder architecture, and therefore require
a task-speciÔ¨Åc model architecture to be added.
Second, there are major computational beneÔ¨Åts
to pre-compute an expensive representation of the
training data once and then run many experiments
with cheaper models on top of this representation.
In this section, we compare the two approaches
by applying BERT to the CoNLL-2003 Named
Entity Recognition (NER) task (Tjong Kim Sang
and De Meulder, 2003). In the input to BERT, we
use a case-preserving WordPiece model, and we
include the maximal document context provided
by the data. Following standard practice, we for-
mulate this as a tagging task but do not use a CRF
Hyperparams Dev Set Accuracy
#L #H #A LM (ppl) MNLI-m MRPC SST-2
3 768 12 5.84 77.9 79.8 88.4
6 768 3 5.24 80.6 82.2 90.7
6 768 12 4.68 81.9 84.8 91.3
12 768 12 3.99 84.4 86.7 92.9
12 1024 16 3.54 85.7 86.9 93.3
24 1024 16 3.23 86.6 87.8 93.7
Table 6: Ablation over BERT model size. #L = the
number of layers; #H = hidden size; #A = number of at-
tention heads. ‚ÄúLM (ppl)‚Äù is the masked LM perplexity
of held-out training data.
System Dev F1 Test F1
ELMo (Peters et al., 2018a) 95.7 92.2
CVT (Clark et al., 2018) - 92.6
CSE (Akbik et al., 2018) - 93.1
Fine-tuning approach
BERTLARGE 96.6 92.8
BERTBASE 96.4 92.4
Feature-based approach (BERTBASE )
Embeddings 91.0 -
Second-to-Last Hidden 95.6 -
Last Hidden 94.9 -
Weighted Sum Last Four Hidden 95.9 -
Concat Last Four Hidden 96.1 -
Weighted Sum All 12 Layers 95.5 -
Table 7: CoNLL-2003 Named Entity Recognition re-
sults. Hyperparameters were selected using the Dev
set. The reported Dev and Test scores are averaged over
5 random restarts using those hyperparameters.
layer in the output. We use the representation of
the Ô¨Årst sub-token as the input to the token-level
classiÔ¨Åer over the NER label set.
To ablate the Ô¨Åne-tuning approach, we apply the
feature-based approach by extracting the activa-
tions from one or more layers without Ô¨Åne-tuning
any parameters of BERT. These contextual em-
beddings are used as input to a randomly initial-
ized two-layer 768-dimensional BiLSTM before
the classiÔ¨Åcation layer.
Results are presented in Table 7. BERT LARGE
performs competitively with state-of-the-art meth-
ods. The best performing method concatenates the
token representations from the top four hidden lay-
ers of the pre-trained Transformer, which is only
0.3 F1 behind Ô¨Åne-tuning the entire model. This
demonstrates that BERT is effective for both Ô¨Åne-
tuning and feature-based approaches.
6 Conclusion
Recent empirical improvements due to transfer
learning with language models have demonstrated
that rich, unsupervised pre-training is an integral
part of many language understanding systems. In
particular, these results enable even low-resource
tasks to beneÔ¨Åt from deep unidirectional architec-
tures. Our major contribution is further general-
izing these Ô¨Åndings to deep bidirectional architec-
tures, allowing the same pre-trained model to suc-
cessfully tackle a broad set of NLP tasks.
References
Alan Akbik, Duncan Blythe, and Roland V ollgraf.
2018. Contextual string embeddings for sequence
labeling. In Proceedings of the 27th International
Conference on Computational Linguistics , pages
1638‚Äì1649.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy
Guo, and Llion Jones. 2018. Character-level lan-
guage modeling with deeper self-attention. arXiv
preprint arXiv:1808.04444.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6(Nov):1817‚Äì1853.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The Ô¨Åfth PASCAL recognizing textual entailment
challenge. In TAC. NIST.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing, pages 120‚Äì128. Association for Computa-
tional Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP. Association for Computational Linguis-
tics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467‚Äì479.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval-2017) , pages 1‚Äì14, Vancou-
ver, Canada. Association for Computational Lin-
guistics.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.
Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.
Quora question pairs.
Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In ACL.
Kevin Clark, Minh-Thang Luong, Christopher D Man-
ning, and Quoc Le. 2018. Semi-supervised se-
quence modeling with cross-view training. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing , pages 1914‚Äì
1925.
Ronan Collobert and Jason Weston. 2008. A uniÔ¨Åed
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160‚Äì167. ACM.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¬®ƒ±c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670‚Äì680, Copen-
hagen, Denmark. Association for Computational
Linguistics.
Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in neural informa-
tion processing systems, pages 3079‚Äì3087.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09.
William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
William Fedus, Ian Goodfellow, and Andrew M Dai.
2018. Maskgan: Better text generation via Ô¨Ålling in
the . arXiv preprint arXiv:1801.07736.
Dan Hendrycks and Kevin Gimpel. 2016. Bridging
nonlinearities and stochastic regularizers with gaus-
sian error linear units. CoRR, abs/1606.08415.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Jeremy Howard and Sebastian Ruder. 2018. Universal
language model Ô¨Åne-tuning for text classiÔ¨Åcation. In
ACL. Association for Computational Linguistics.
Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2018. Reinforced
mnemonic reader for machine reading comprehen-
sion. In IJCAI.
Yacine Jernite, Samuel R. Bowman, and David Son-
tag. 2017. Discourse-based objectives for fast un-
supervised sentence representation learning. CoRR,
abs/1705.00557.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In ACL.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294‚Äì3302.
Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning , pages
1188‚Äì1196.
Hector J Levesque, Ernest Davis, and Leora Morgen-
stern. 2011. The winograd schema challenge. In
Aaai spring symposium: Logical formalizations of
commonsense reasoning, volume 46, page 47.
Lajanugen Logeswaran and Honglak Lee. 2018. An
efÔ¨Åcient framework for learning sentence represen-
tations. In International Conference on Learning
Representations.
Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.
Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning generic context em-
bedding with bidirectional LSTM. In CoNLL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26 , pages 3111‚Äì3119. Curran Associates,
Inc.
Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
D. Koller, D. Schuurmans, Y . Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21 , pages 1081‚Äì1088. Curran As-
sociates, Inc.
Ankur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In EMNLP.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 1532‚Äì
1543.
Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In ACL.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word rep-
resentations. In NAACL.
Matthew Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih. 2018b. Dissecting contextual
word embeddings: Architecture and representation.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1499‚Äì1509.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing with unsupervised learning. Technical re-
port, OpenAI.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383‚Äì2392.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
Ô¨Çow for machine comprehension. In ICLR.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing ,
pages 1631‚Äì1642.
Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.
2018. U-net: Machine reading comprehension
with unanswerable questions. arXiv preprint
arXiv:1810.06638.
Wilson L Taylor. 1953. Cloze procedure: A new
tool for measuring readability. Journalism Bulletin,
30(4):415‚Äì433.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
CoNLL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ‚Äô10, pages 384‚Äì394.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000‚Äì6010.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning, pages 1096‚Äì1103.
ACM.
Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman. 2018a.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP ,
pages 353‚Äì355.
Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-
granularity hierarchical attention fusion networks
for reading comprehension and question answering.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics.
Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018. Neural network acceptability judg-
ments. arXiv preprint arXiv:1805.12471.
Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2018. A broad-coverage challenge corpus
for sentence understanding through inference. In
NAACL.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google‚Äôs neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in neural information
processing systems, pages 3320‚Äì3328.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. QANet: Combining local convolution
with global self-attention for reading comprehen-
sion. In ICLR.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. 2018. Swag: A large-scale adversarial dataset
for grounded commonsense inference. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE
international conference on computer vision , pages
19‚Äì27.
Appendix for ‚ÄúBERT: Pre-training of
Deep Bidirectional Transformers for
Language Understanding‚Äù
We organize the appendix into three sections:
‚Ä¢ Additional implementation details for BERT
are presented in Appendix A;
‚Ä¢ Additional details for our experiments are
presented in Appendix B; and
‚Ä¢ Additional ablation studies are presented in
Appendix C.
We present additional ablation studies for
BERT including:
‚Äì Effect of Number of Training Steps; and
‚Äì Ablation for Different Masking Proce-
dures.
A Additional Details for BERT
A.1 Illustration of the Pre-training Tasks
We provide examples of the pre-training tasks in
the following.
Masked LM and the Masking Procedure As-
suming the unlabeled sentence is my dog is
hairy, and during the random masking procedure
we chose the 4-th token (which corresponding to
hairy), our masking procedure can be further il-
lustrated by
‚Ä¢ 80% of the time: Replace the word with the
[MASK] token, e.g., my dog is hairy ‚Üí
my dog is [MASK]
‚Ä¢ 10% of the time: Replace the word with a
random word, e.g., my dog is hairy ‚Üí my
dog is apple
‚Ä¢ 10% of the time: Keep the word un-
changed, e.g., my dog is hairy ‚Üí my dog
is hairy . The purpose of this is to bias the
representation towards the actual observed
word.
The advantage of this procedure is that the
Transformer encoder does not know which words
it will be asked to predict or which have been re-
placed by random words, so it is forced to keep
a distributional contextual representation of ev-
ery input token. Additionally, because random
replacement only occurs for 1.5% of all tokens
(i.e., 10% of 15%), this does not seem to harm
the model‚Äôs language understanding capability. In
Section C.2, we evaluate the impact this proce-
dure.
Compared to standard langauge model training,
the masked LM only make predictions on 15% of
tokens in each batch, which suggests that more
pre-training steps may be required for the model
BERT (Ours)
Trm Trm Trm
Trm Trm Trm
...
...
Trm Trm Trm
Trm Trm Trm
...
...
OpenAI GPT
Lstm
ELMo
Lstm Lstm
Lstm Lstm Lstm
Lstm Lstm Lstm
Lstm Lstm Lstm
 T1 T2  TN...
...
...
...
...
 E1 E2  EN...
 T1 T2 TN...
 E1 E2  EN...
 T1 T2  TN...
 E1 E2  EN...
Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT
uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-
left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly
conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and
OpenAI GPT are Ô¨Åne-tuning approaches, while ELMo is a feature-based approach.
to converge. In Section C.1 we demonstrate that
MLM does converge marginally slower than a left-
to-right model (which predicts every token), but
the empirical improvements of the MLM model
far outweigh the increased training cost.
Next Sentence Prediction The next sentence
prediction task can be illustrated in the following
examples.
Input = [CLS] the man went to [MASK] store [SEP]
he bought a gallon [MASK] milk [SEP]
Label = IsNext
Input = [CLS] the man [MASK] to the store [SEP]
penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
A.2 Pre-training Procedure
To generate each training input sequence, we sam-
ple two spans of text from the corpus, which we
refer to as ‚Äúsentences‚Äù even though they are typ-
ically much longer than single sentences (but can
be shorter also). The Ô¨Årst sentence receives the A
embedding and the second receives the B embed-
ding. 50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the ‚Äúnext sentence pre-
diction‚Äù task. They are sampled such that the com-
bined length is‚â§ 512 tokens. The LM masking is
applied after WordPiece tokenization with a uni-
form masking rate of 15%, and no special consid-
eration given to partial word pieces.
We train with batch size of 256 sequences (256
sequences * 512 tokens = 128,000 tokens/batch)
for 1,000,000 steps, which is approximately 40
epochs over the 3.3 billion word corpus. We
use Adam with learning rate of 1e-4, Œ≤1 = 0.9,
Œ≤2 = 0.999, L2 weight decay of 0.01, learning
rate warmup over the Ô¨Årst 10,000 steps, and linear
decay of the learning rate. We use a dropout prob-
ability of 0.1 on all layers. We use a gelu acti-
vation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT. The
training loss is the sum of the mean masked LM
likelihood and the mean next sentence prediction
likelihood.
Training of BERT BASE was performed on 4
Cloud TPUs in Pod conÔ¨Åguration (16 TPU chips
total).13 Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.
Longer sequences are disproportionately expen-
sive because attention is quadratic to the sequence
length. To speed up pretraing in our experiments,
we pre-train the model with sequence length of
128 for 90% of the steps. Then, we train the rest
10% of the steps of sequence of 512 to learn the
positional embeddings.
A.3 Fine-tuning Procedure
For Ô¨Åne-tuning, most model hyperparameters are
the same as in pre-training, with the exception of
the batch size, learning rate, and number of train-
ing epochs. The dropout probability was always
kept at 0.1. The optimal hyperparameter values
are task-speciÔ¨Åc, but we found the following range
of possible values to work well across all tasks:
‚Ä¢ Batch size: 16, 32
13https://cloudplatform.googleblog.com/2018/06/Cloud-
TPU-now-offers-preemptible-pricing-and-global-
availability.html
‚Ä¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5
‚Ä¢ Number of epochs: 2, 3, 4
We also observed that large data sets (e.g.,
100k+ labeled training examples) were far less
sensitive to hyperparameter choice than small data
sets. Fine-tuning is typically very fast, so it is rea-
sonable to simply run an exhaustive search over
the above parameters and choose the model that
performs best on the development set.
A.4 Comparison of BERT, ELMo ,and
OpenAI GPT
Here we studies the differences in recent popular
representation learning models including ELMo,
OpenAI GPT and BERT. The comparisons be-
tween the model architectures are shown visually
in Figure 3. Note that in addition to the architec-
ture differences, BERT and OpenAI GPT are Ô¨Åne-
tuning approaches, while ELMo is a feature-based
approach.
The most comparable existing pre-training
method to BERT is OpenAI GPT, which trains a
left-to-right Transformer LM on a large text cor-
pus. In fact, many of the design decisions in BERT
were intentionally made to make it as close to
GPT as possible so that the two methods could be
minimally compared. The core argument of this
work is that the bi-directionality and the two pre-
training tasks presented in Section 3.1 account for
the majority of the empirical improvements, but
we do note that there are several other differences
between how BERT and GPT were trained:
‚Ä¢ GPT is trained on the BooksCorpus (800M
words); BERT is trained on the BooksCor-
pus (800M words) and Wikipedia (2,500M
words).
‚Ä¢ GPT uses a sentence separator ( [SEP]) and
classiÔ¨Åer token ( [CLS]) which are only in-
troduced at Ô¨Åne-tuning time; BERT learns
[SEP], [CLS] and sentence A/B embed-
dings during pre-training.
‚Ä¢ GPT was trained for 1M steps with a batch
size of 32,000 words; BERT was trained for
1M steps with a batch size of 128,000 words.
‚Ä¢ GPT used the same learning rate of 5e-5 for
all Ô¨Åne-tuning experiments; BERT chooses a
task-speciÔ¨Åc Ô¨Åne-tuning learning rate which
performs the best on the development set.
To isolate the effect of these differences, we per-
form ablation experiments in Section 5.1 which
demonstrate that the majority of the improvements
are in fact coming from the two pre-training tasks
and the bidirectionality they enable.
A.5 Illustrations of Fine-tuning on Different
Tasks
The illustration of Ô¨Åne-tuning BERT on different
tasks can be seen in Figure 4. Our task-speciÔ¨Åc
models are formed by incorporating BERT with
one additional output layer, so a minimal num-
ber of parameters need to be learned from scratch.
Among the tasks, (a) and (b) are sequence-level
tasks while (c) and (d) are token-level tasks. In
the Ô¨Ågure, E represents the input embedding, Ti
represents the contextual representation of tokeni,
[CLS] is the special symbol for classiÔ¨Åcation out-
put, and [SEP] is the special symbol to separate
non-consecutive token sequences.
B Detailed Experimental Setup
B.1 Detailed Descriptions for the GLUE
Benchmark Experiments.
Our GLUE results in Table1 are obtained
from https://gluebenchmark.com/
leaderboard and https://blog.
openai.com/language-unsupervised.
The GLUE benchmark includes the following
datasets, the descriptions of which were originally
summarized in Wang et al. (2018a):
MNLI Multi-Genre Natural Language Inference
is a large-scale, crowdsourced entailment classiÔ¨Å-
cation task (Williams et al., 2018). Given a pair of
sentences, the goal is to predict whether the sec-
ond sentence is an entailment, contradiction, or
neutral with respect to the Ô¨Årst one.
QQP Quora Question Pairs is a binary classiÔ¨Å-
cation task where the goal is to determine if two
questions asked on Quora are semantically equiv-
alent (Chen et al., 2018).
QNLI Question Natural Language Inference is
a version of the Stanford Question Answering
Dataset (Rajpurkar et al., 2016) which has been
converted to a binary classiÔ¨Åcation task (Wang
et al., 2018a). The positive examples are (ques-
tion, sentence) pairs which do contain the correct
answer, and the negative examples are (question,
sentence) from the same paragraph which do not
contain the answer.
BERT
E[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô
C
 T1
 T[SEP]...
 TN
 T1‚Äô ...
 TM‚Äô
[CLS] Tok 
1
 [SEP]... Tok 
N
Tok 
1 ... Tok
M
Question Paragraph
BERT
E[CLS] E1  E2  EN
C
 T1
  T2
  TN
Single Sentence 
...
...
BERT
Tok 1  Tok 2  Tok N...[CLS]
E[CLS] E1  E2  EN
C
 T1
  T2
  TN
Single Sentence 
B-PERO O
...
...E[CLS] E1  E[SEP]
Class 
Label
... EN E1‚Äô ... EM‚Äô
C
 T1
 T[SEP]...
 TN
 T1‚Äô ...
 TM‚Äô
Start/End Span
Class 
Label
BERT
Tok 1  Tok 2  Tok N...[CLS] Tok 1[CLS][CLS] Tok 
1
 [SEP]... Tok 
N
Tok 
1 ... Tok
M
Sentence 1
...
Sentence 2
Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
SST-2 The Stanford Sentiment Treebank is a
binary single-sentence classiÔ¨Åcation task consist-
ing of sentences extracted from movie reviews
with human annotations of their sentiment (Socher
et al., 2013).
CoLA The Corpus of Linguistic Acceptability is
a binary single-sentence classiÔ¨Åcation task, where
the goal is to predict whether an English sentence
is linguistically ‚Äúacceptable‚Äù or not (Warstadt
et al., 2018).
STS-B The Semantic Textual Similarity Bench-
mark is a collection of sentence pairs drawn from
news headlines and other sources (Cer et al.,
2017). They were annotated with a score from 1
to 5 denoting how similar the two sentences are in
terms of semantic meaning.
MRPC Microsoft Research Paraphrase Corpus
consists of sentence pairs automatically extracted
from online news sources, with human annotations
for whether the sentences in the pair are semanti-
cally equivalent (Dolan and Brockett, 2005).
RTE Recognizing Textual Entailment is a bi-
nary entailment task similar to MNLI, but with
much less training data (Bentivogli et al., 2009).14
WNLI Winograd NLI is a small natural lan-
guage inference dataset (Levesque et al., 2011).
The GLUE webpage notes that there are issues
with the construction of this dataset, 15 and every
trained system that‚Äôs been submitted to GLUE has
performed worse than the 65.1 baseline accuracy
of predicting the majority class. We therefore ex-
clude this set to be fair to OpenAI GPT. For our
GLUE submission, we always predicted the ma-
14Note that we only report single-task Ô¨Åne-tuning results
in this paper. A multitask Ô¨Åne-tuning approach could poten-
tially push the performance even further. For example, we
did observe substantial improvements on RTE from multi-
task training with MNLI.
15https://gluebenchmark.com/faq
jority class.
C Additional Ablation Studies
C.1 Effect of Number of Training Steps
Figure 5 presents MNLI Dev accuracy after Ô¨Åne-
tuning from a checkpoint that has been pre-trained
fork steps. This allows us to answer the following
questions:
1. Question: Does BERT really need such
a large amount of pre-training (128,000
words/batch * 1,000,000 steps) to achieve
high Ô¨Åne-tuning accuracy?
Answer: Yes, BERT BASE achieves almost
1.0% additional accuracy on MNLI when
trained on 1M steps compared to 500k steps.
2. Question: Does MLM pre-training converge
slower than LTR pre-training, since only 15%
of words are predicted in each batch rather
than every word?
Answer: The MLM model does converge
slightly slower than the LTR model. How-
ever, in terms of absolute accuracy the MLM
model begins to outperform the LTR model
almost immediately.
C.2 Ablation for Different Masking
Procedures
In Section 3.1, we mention that BERT uses a
mixed strategy for masking the target tokens when
pre-training with the masked language model
(MLM) objective. The following is an ablation
study to evaluate the effect of different masking
strategies.
200 400 600 800 1,000
76
78
80
82
84
Pre-training Steps (Thousands)
MNLI Dev Accuracy
BERTBASE (Masked LM)
BERTBASE (Left-to-Right)
Figure 5: Ablation over number of training steps. This
shows the MNLI accuracy after Ô¨Åne-tuning, starting
from model parameters that have been pre-trained for
k steps. The x-axis is the value ofk.
Note that the purpose of the masking strategies
is to reduce the mismatch between pre-training
and Ô¨Åne-tuning, as the [MASK] symbol never ap-
pears during the Ô¨Åne-tuning stage. We report the
Dev results for both MNLI and NER. For NER,
we report both Ô¨Åne-tuning and feature-based ap-
proaches, as we expect the mismatch will be am-
pliÔ¨Åed for the feature-based approach as the model
will not have the chance to adjust the representa-
tions.
Masking Rates Dev Set Results
MASK SAME RND MNLI NER
Fine-tune Fine-tune Feature-based
80% 10% 10% 84.2 95.4 94.9
100% 0% 0% 84.3 94.9 94.0
80% 0% 20% 84.1 95.2 94.6
80% 20% 0% 84.4 95.2 94.7
0% 20% 80% 83.7 94.8 94.6
0% 0% 100% 83.6 94.9 94.6
Table 8: Ablation over different masking strategies.
The results are presented in Table 8. In the table,
MASK means that we replace the target token with
the [MASK] symbol for MLM; S AME means that
we keep the target token as is; R ND means that
we replace the target token with another random
token.
The numbers in the left part of the table repre-
sent the probabilities of the speciÔ¨Åc strategies used
during MLM pre-training (BERT uses 80%, 10%,
10%). The right part of the paper represents the
Dev set results. For the feature-based approach,
we concatenate the last 4 layers of BERT as the
features, which was shown to be the best approach
in Section 5.3.
From the table it can be seen that Ô¨Åne-tuning is
surprisingly robust to different masking strategies.
However, as expected, using only the MASK strat-
egy was problematic when applying the feature-
based approach to NER. Interestingly, using only
the R ND strategy performs much worse than our
strategy as well.


